{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages (0.23.1)\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages (from scikit-learn) (1.18.1)\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages (from scikit-learn) (0.14.1)\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages (from scikit-learn) (1.4.1)\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages (from scikit-learn) (2.1.0)\n"
     ]
    }
   ],
   "source": [
    "# Upgrade sklearn to its latest version\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.22.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "#Basic DS libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Helper Libs\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import joblib\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Vectorizers/Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#Models\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss, accuracy_score, precision_score, recall_score\n",
    "\n",
    "#Gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "#Glove\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "messages_df = pd.read_sql_table(con=engine, table_name='Message')\n",
    "categories_df = pd.read_sql_table(con=engine, table_name='CorpusWide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create categories_list data with a list of the categories for each message\n",
    "categories = pd.read_csv('../data/disaster_categories.csv')\n",
    "categories['categories_list'] = categories['categories'].apply(lambda x: re.sub(r\"[_a-z]+-0[;]?\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\"-1\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\";$\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13  \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9  \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6  \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13  \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12  \n",
       "...                                                  ...     ...        ...  \n",
       "26139                                               None    news         21  \n",
       "26140                                               None    news         22  \n",
       "26141                                               None    news         23  \n",
       "26142                                               None    news         31  \n",
       "26143                                               None    news         36  \n",
       "\n",
       "[26144 rows x 5 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  related  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "...                                                  ...     ...      ...   \n",
       "26139                                               None    news        0   \n",
       "26140                                               None    news        0   \n",
       "26141                                               None    news        1   \n",
       "26142                                               None    news        1   \n",
       "26143                                               None    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0            0      0            0             0                 0  ...   \n",
       "1            0      0            1             0                 0  ...   \n",
       "2            0      0            0             0                 0  ...   \n",
       "3            1      0            1             0                 1  ...   \n",
       "4            0      0            0             0                 0  ...   \n",
       "...        ...    ...          ...           ...               ...  ...   \n",
       "26139        0      0            0             0                 0  ...   \n",
       "26140        0      0            0             0                 0  ...   \n",
       "26141        0      0            0             0                 0  ...   \n",
       "26142        0      0            1             0                 0  ...   \n",
       "26143        0      0            0             0                 0  ...   \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "0                0                     0                0       0      0   \n",
       "1                0                     0                1       0      1   \n",
       "2                0                     0                0       0      0   \n",
       "3                0                     0                0       0      0   \n",
       "4                0                     0                0       0      0   \n",
       "...            ...                   ...              ...     ...    ...   \n",
       "26139            0                     0                0       0      0   \n",
       "26140            0                     0                0       0      0   \n",
       "26141            0                     0                0       0      0   \n",
       "26142            0                     0                0       0      0   \n",
       "26143            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "0         0           0     0              0              0  \n",
       "1         0           0     0              0              0  \n",
       "2         0           0     0              0              0  \n",
       "3         0           0     0              0              0  \n",
       "4         0           0     0              0              0  \n",
       "...     ...         ...   ...            ...            ...  \n",
       "26139     0           0     0              0              0  \n",
       "26140     0           0     0              0              0  \n",
       "26141     0           0     0              0              0  \n",
       "26142     0           0     0              0              0  \n",
       "26143     0           0     0              0              0  \n",
       "\n",
       "[26144 rows x 39 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categories</th>\n",
       "      <th>categories_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;other_aid;weather_related;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_products;o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26243</th>\n",
       "      <td>30261</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26244</th>\n",
       "      <td>30262</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>30263</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26246</th>\n",
       "      <td>30264</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>30265</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                         categories  \\\n",
       "0          2  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "1          7  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "2          8  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "3          9  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "4         12  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "...      ...                                                ...   \n",
       "26243  30261  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26244  30262  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26245  30263  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26246  30264  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26247  30265  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "\n",
       "                                         categories_list  \n",
       "0                                                related  \n",
       "1      related;aid_related;other_aid;weather_related;...  \n",
       "2                                                related  \n",
       "3      related;request;aid_related;medical_products;o...  \n",
       "4                                                related  \n",
       "...                                                  ...  \n",
       "26243                                                     \n",
       "26244                                                     \n",
       "26245                                            related  \n",
       "26246                       related;aid_related;military  \n",
       "26247                                            related  \n",
       "\n",
       "[26248 rows x 3 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Design and Apply Tokenization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Write down tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_list(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A list of tokens extracted from the input text\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_str(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A string with the tokens extracted from the input text concatenated by spaces\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "    #Return tokens list as a string joined by whitespaces\n",
    "    clean_tokens_str = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenize_to_list(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather update cold front cuba could pas haiti'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_to_str(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Generate tokenized messages table for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# messages_df['tokens_list'] = messages_df.message.apply(lambda x: tokenize_to_list(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 20s, sys: 6.67 s, total: 1min 26s\n",
      "Wall time: 1min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "messages_df['tokens_str'] = messages_df.message.apply(lambda x: tokenize_to_str(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "      <th>tokens_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "      <td>weather update cold front cuba could pas haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "      <td>looking someone name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "      <td>un report leogane 80 90 destroyed hospital st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "      <td>say west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "      <td>training demonstrated enhance micronutrient pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "      <td>suitable candidate selected ocha jakarta curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "      <td>proshika operating cox bazar municipality 5 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "      <td>2 000 woman protesting conduct election tearga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "      <td>radical shift thinking came result meeting rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12   \n",
       "...                                                  ...     ...        ...   \n",
       "26139                                               None    news         21   \n",
       "26140                                               None    news         22   \n",
       "26141                                               None    news         23   \n",
       "26142                                               None    news         31   \n",
       "26143                                               None    news         36   \n",
       "\n",
       "                                              tokens_str  \n",
       "0         weather update cold front cuba could pas haiti  \n",
       "1                                              hurricane  \n",
       "2                                   looking someone name  \n",
       "3      un report leogane 80 90 destroyed hospital st ...  \n",
       "4         say west side haiti rest country today tonight  \n",
       "...                                                  ...  \n",
       "26139  training demonstrated enhance micronutrient pr...  \n",
       "26140  suitable candidate selected ocha jakarta curre...  \n",
       "26141  proshika operating cox bazar municipality 5 un...  \n",
       "26142  2 000 woman protesting conduct election tearga...  \n",
       "26143  radical shift thinking came result meeting rec...  \n",
       "\n",
       "[26144 rows x 6 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens = messages_df[['message_id','tokens_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens.to_sql('MessageTokens', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories = messages_tokens.merge(categories_df, on='message_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 - Find n-grams and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ngrams_freqs(messages_array, n=1):\n",
    "#     vec = CountVectorizer(ngram_range=(n, n)).fit(messages_array)\n",
    "#     bag_of_words = vec.transform(messages_array)\n",
    "#     word_count = bag_of_words.sum(axis=0)\n",
    "#     words_freq = [(word, n, word_count[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "#     words_freq_df = pd.DataFrame(data = words_freq, columns = ['ngram','n','count'])\n",
    "#     return words_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str)\n",
    "# bigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=2)\n",
    "# trigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=3)\n",
    "# ngrams_freqs = pd.concat([unigrams_freqs, bigrams_freqs, trigrams_freqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs.to_sql('NGramsFreqs', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Prepare Data for Train/Test Pipeline\n",
    "- Extract X and Y from datasets\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens = pd.read_sql_table(con=engine, table_name='MessageTokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>tokens_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>weather update cold front cuba could pas haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>looking someone name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>un report leogane 80 90 destroyed hospital st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>say west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>training demonstrated enhance micronutrient pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>suitable candidate selected ocha jakarta curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>proshika operating cox bazar municipality 5 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>2 000 woman protesting conduct election tearga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>radical shift thinking came result meeting rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                         tokens_str\n",
       "0               2     weather update cold front cuba could pas haiti\n",
       "1               7                                          hurricane\n",
       "2               8                               looking someone name\n",
       "3               9  un report leogane 80 90 destroyed hospital st ...\n",
       "4              12     say west side haiti rest country today tonight\n",
       "...           ...                                                ...\n",
       "26139       30261  training demonstrated enhance micronutrient pr...\n",
       "26140       30262  suitable candidate selected ocha jakarta curre...\n",
       "26141       30263  proshika operating cox bazar municipality 5 un...\n",
       "26142       30264  2 000 woman protesting conduct election tearga...\n",
       "26143       30265  radical shift thinking came result meeting rec...\n",
       "\n",
       "[26144 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for ML Pipeline\n",
    "X = messages_df.message.values\n",
    "X_tokenized = messages_tokens.tokens_str.values\n",
    "Y_df = categories_df.drop(['message_id', 'message', 'original', 'genre'], axis=1)\n",
    "Y = Y_df.values\n",
    "category_columns = Y_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   20031\n",
       "request                    4453\n",
       "offer                       118\n",
       "aid_related               10822\n",
       "medical_help               2078\n",
       "medical_products           1310\n",
       "search_and_rescue           723\n",
       "security                    471\n",
       "military                    857\n",
       "water                      1666\n",
       "food                       2911\n",
       "shelter                    2303\n",
       "clothing                    403\n",
       "money                       602\n",
       "missing_people              298\n",
       "refugees                    873\n",
       "death                      1190\n",
       "other_aid                  3434\n",
       "infrastructure_related     1701\n",
       "transport                  1196\n",
       "buildings                  1327\n",
       "electricity                 532\n",
       "tools                       159\n",
       "hospitals                   283\n",
       "shops                       120\n",
       "aid_centers                 309\n",
       "other_infrastructure       1147\n",
       "weather_related            7272\n",
       "floods                     2142\n",
       "storm                      2437\n",
       "fire                        282\n",
       "earthquake                 2449\n",
       "cold                        527\n",
       "other_weather              1373\n",
       "direct_report              5055\n",
       "dtype: int64"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  related  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "...                                                  ...     ...      ...   \n",
       "26139                                               None    news        0   \n",
       "26140                                               None    news        0   \n",
       "26141                                               None    news        1   \n",
       "26142                                               None    news        1   \n",
       "26143                                               None    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  ...  \\\n",
       "0            0      0            0             0                 0  ...   \n",
       "1            0      0            1             0                 0  ...   \n",
       "2            0      0            0             0                 0  ...   \n",
       "3            1      0            1             0                 1  ...   \n",
       "4            0      0            0             0                 0  ...   \n",
       "...        ...    ...          ...           ...               ...  ...   \n",
       "26139        0      0            0             0                 0  ...   \n",
       "26140        0      0            0             0                 0  ...   \n",
       "26141        0      0            0             0                 0  ...   \n",
       "26142        0      0            1             0                 0  ...   \n",
       "26143        0      0            0             0                 0  ...   \n",
       "\n",
       "       aid_centers  other_infrastructure  weather_related  floods  storm  \\\n",
       "0                0                     0                0       0      0   \n",
       "1                0                     0                1       0      1   \n",
       "2                0                     0                0       0      0   \n",
       "3                0                     0                0       0      0   \n",
       "4                0                     0                0       0      0   \n",
       "...            ...                   ...              ...     ...    ...   \n",
       "26139            0                     0                0       0      0   \n",
       "26140            0                     0                0       0      0   \n",
       "26141            0                     0                0       0      0   \n",
       "26142            0                     0                0       0      0   \n",
       "26143            0                     0                0       0      0   \n",
       "\n",
       "       fire  earthquake  cold  other_weather  direct_report  \n",
       "0         0           0     0              0              0  \n",
       "1         0           0     0              0              0  \n",
       "2         0           0     0              0              0  \n",
       "3         0           0     0              0              0  \n",
       "4         0           0     0              0              0  \n",
       "...     ...         ...   ...            ...            ...  \n",
       "26139     0           0     0              0              0  \n",
       "26140     0           0     0              0              0  \n",
       "26141     0           0     0              0              0  \n",
       "26142     0           0     0              0              0  \n",
       "26143     0           0     0              0              0  \n",
       "\n",
       "[26144 rows x 39 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Weather update - a cold front from Cuba that could pass over Haiti',\n",
       "       'Is the Hurricane over or is it not over',\n",
       "       'Looking for someone but no name', ...,\n",
       "       \"Proshika, operating in Cox's Bazar municipality and 5 other unions, Ramu and Chokoria, assessment, 5 kg rice, 1,5 kg lentils to 700 families.\",\n",
       "       'Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.',\n",
       "       'A radical shift in thinking came about as a result of this meeting, recognizing that HIV/AIDS is at the core of the humanitarian crisis and identifying the crisis itself as a function of the HIV/AIDS pandemic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weather update cold front cuba could pas haiti', 'hurricane',\n",
       "       'looking someone name', ...,\n",
       "       'proshika operating cox bazar municipality 5 union ramu chokoria assessment 5 kg rice 1 5 kg lentil 700 family',\n",
       "       '2 000 woman protesting conduct election teargassed tried converge local electoral commission office southern oil city port harcourt',\n",
       "       'radical shift thinking came result meeting recognizing hiv aid core humanitarian crisis identifying crisis function hiv aid pandemic'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'water', 'food', 'shelter', 'clothing', 'money', 'missing_people',\n",
       "       'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tokenized, Y, test_size=0.25, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crude charm charismatic dt member thanks video santiago talk twice',\n",
       "       'tidal wave triggered earthquake indonesia sunday swept vast swath coastline including india indonesia malaysia maldives somalia sri lanka thailand',\n",
       "       'earthquake happened 4h34 evening night', ...,\n",
       "       'earthquake day haiti google possibly leaving china',\n",
       "       'matter debated previously non aligned movement foreign minister firmly stated right humanitarian intervention',\n",
       "       'dumbonyc oh rt endtwist coned power station dumbo flooding nopower sandy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19608,) (19608, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15027,  3310,    88,  8115,  1570,   982,   538,   354,   669,\n",
       "        1253,  2161,  1731,   293,   461,   224,   659,   898,  2558,\n",
       "        1276,   879,  1000,   410,   117,   223,    89,   240,   855,\n",
       "        5479,  1597,  1818,   214,  1875,   399,  1024,  3791])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 - Design Transformer to Tokenize sentences before passing on to other transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, output_str=True):\n",
    "        self.output_str = output_str\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([tokenize_to_str(tokens_str, lemmatizer) if self.output_str else \n",
    "                         tokenize_to_list(tokens_str, lemmatizer) for tokens_str in X])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# msgTokenizer = Tokenizer().fit(X, None)\n",
    "# msgTokenizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a Mean/TF-IDF word vector from a locally trained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Train a Word2Vec model using messages text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(object):\n",
    "    \"\"\"An interator that yields messages tokens (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, messages_df, sample_size=-1):\n",
    "        self.messages_df = messages_df\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        messages = self.messages_df\n",
    "        \n",
    "        #Sample dataset if specified\n",
    "        if self.sample_size > 0:\n",
    "            messages = messages.sample(self.sample_size)\n",
    "        \n",
    "        #Yeld tokenized message joined by whitespaces\n",
    "        for token_str in messages.tokens_str:\n",
    "            yield token_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens.tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "sentences = Message(messages_df=messages_tokens)\n",
    "local_w2v_model_full = gensim.models.Word2Vec(sentences=sentences)\n",
    "local_w2v_model_full.save(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_full = gensim.models.Word2Vec.load(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up generated word vocab\n",
    "for i, word in enumerate(w2v_model_full.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look up generated word vectors\n",
    "w2v_model_full.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Implement Vector Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial implementation of aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing defaultdict functionality\n",
    "# test_dict = defaultdict(lambda: 1, [(\"Hello\" , 7), (\"hi\" , 10), (\"there\" , 45),(\"at\" , 23),(\"this\" , 77)])\n",
    "# test_dict['Hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(BaseEstimator):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "        self.word_weights = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# w2v_mean_pip = Pipeline([\n",
    "#     ('features',FeatureUnion([\n",
    "#         ('w2v_mean', MeanEmbeddingVectorizer(w2v_model_full)),\n",
    "#         ('w2v_tfidf', TfidfEmbeddingVectorizer(w2v_model_full))\n",
    "#     ])),    \n",
    "#     ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "# ])\n",
    "\n",
    "# w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "# for category_idx in range(y_pred.shape[1]):\n",
    "#     print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_idx=1\n",
    "# c_report = classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], \n",
    "#                       labels=[0,1], \n",
    "#                       target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1'],\n",
    "#                       output_dict=True)\n",
    "# c_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c_report.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_mean_pip.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum(y_pred[0] == y_test[0])/y_pred[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test = y_test[0]\n",
    "sample_pred = y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_test == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_pred == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(sample_test == 1) == (sample_pred == 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing TPR manually\n",
    "tp_array = (sample_test == 1) & (sample_pred == 1)\n",
    "tp = tp_array.sum()\n",
    "tpr = tp/sample_test.shape[0]\n",
    "print(tp)\n",
    "print(tpr)\n",
    "tp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3/36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing FPR manually\n",
    "fp_array = (sample_test == 0) & (sample_pred == 1)\n",
    "fp = fp_array.sum()\n",
    "fpr = fp/sample_test.shape[0]\n",
    "print(fp)\n",
    "print(fpr)\n",
    "fp_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing FNR manually\n",
    "fn_array = (sample_test == 1) & (sample_pred == 0)\n",
    "fn = fn_array.sum()\n",
    "fnr = fn/sample_test.shape[0]\n",
    "print(fn)\n",
    "print(fnr)\n",
    "fn_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Precision manually\n",
    "print(tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing Recall manually\n",
    "print(tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_test = np.append(np.repeat(1, 4),np.repeat(0,32))\n",
    "example_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_pred = np.repeat(0, 36)\n",
    "example_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(example_test,example_pred))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(example_test,example_pred))\n",
    "print(\"Jaccard Score = \", jaccard_score(example_test,example_pred))\n",
    "print(\"Accuracy = \", accuracy_score(example_test,example_pred))\n",
    "print(\"Recall = \", recall_score(example_test,example_pred))\n",
    "print(\"Precision = \", precision_score(example_test,example_pred))\n",
    "print(\"F1-Score = \", f1_score(example_test,example_pred))\n",
    "print(\"Jaccard Score Micro = \", jaccard_score(example_test,example_pred, average='micro'))\n",
    "print(\"Recall Micro = \", recall_score(example_test,example_pred, average='micro'))\n",
    "print(\"Precision Micro = \", precision_score(example_test,example_pred, average='micro'))\n",
    "print(\"F1-Score Micro = \", f1_score(example_test,example_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "print(mean_absolute_error(y_test, y_pred, multioutput='raw_values'))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "(y_test == y_pred).mean()\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred, multioutput='variance_weighted'))\n",
    "print(explained_variance_score(y_test, y_pred))\n",
    "print(explained_variance_score(y_test, y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Create Estimator to both train and average w2v on corpus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying W2VTransformer - Gensim's sklearn Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "\n",
    "model.fit(common_texts)\n",
    "          \n",
    "model.transform(['graph', 'system'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.head(5).tokens_str.str.split().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df.head(5).tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = W2VTransformer()\n",
    "w2v_model.fit(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word = messages_df.tokens_str[0].split()[1]\n",
    "print(word)\n",
    "w2v_model.transform(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model = gensim.models.Word2Vec(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, word in enumerate(test_model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implement aggregators now with training capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# w2v_mean_pip = Pipeline([\n",
    "#     ('features',FeatureUnion([\n",
    "#         ('w2v_mean', MeanEmbeddingTrainVectorizer(word2vec_model=w2v_model_full)),\n",
    "#         ('w2v_mean_train', MeanEmbeddingTrainVectorizer(num_dims=50)),\n",
    "#         ('w2v_tfidf', TfidfEmbeddingTrainVectorizer(word2vec_model=w2v_model_full)),\n",
    "#         ('w2v_tfidf_train', TfidfEmbeddingTrainVectorizer(num_dims=100)),\n",
    "#     ])),\n",
    "#     ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "# ])\n",
    "\n",
    "# w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "# for category_idx in range(y_pred.shape[1]):\n",
    "#     print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Glove pre-trained model to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove pre-trained vectors can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Code which can be used to convert from Glove default format to Gensim W2V format\n",
    "\n",
    "glove_file = 'glove-pretrained/glove.6B.50d.txt'\n",
    "tmp_file = 'glove-pretrained/glove.6B.50d_word2vec.txt'\n",
    "\n",
    "#_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_50d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.50d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "glove_100d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.100d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "glove_300d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.300d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a Pipeline to train the Word2Vec estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Aggregators with Glove using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('features',FeatureUnion([\n",
    "        ('glove_mean', MeanEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ])),    \n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "# precision = dict()\n",
    "# recall = dict()\n",
    "# average_precision = dict()\n",
    "# for i in range(len(category_columns)):\n",
    "#     precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    #average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "# precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "#     y_score.ravel())\n",
    "# average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "#                                                      average=\"micro\")\n",
    "# print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "#       .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import hamming_loss, zero_one_loss, jaccard_score, accuracy_score, auc, recall_score, precision_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0] == y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "6536*36"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.argmax(y_pred, axis=1).shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "train_foo = [['sci-fi', 'thriller'],['comedy'],['sci-fi', 'thriller'],['comedy']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_label_train = mlb.fit_transform(train_foo)\n",
    "mlb_label_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_df[Y_df['related'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use Doc2Vec to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the number of words in the corpus\n",
    "messages_df['message'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Doc2Vec step by step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_messages(messages, tokens_only=False):\n",
    "    for index, tokens_str in np.ndenumerate(messages):\n",
    "        tokens = tokens_str.split()\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            tags = index\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, tags)\n",
    "\n",
    "train_corpus = list(read_messages(X_train))\n",
    "test_corpus = list(read_messages(X_test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_corpus[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d2v_model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Doc2Vec Estimator/Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, epochs=20):\n",
    "        self.epochs = epochs\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.d2v_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tagged_x = [TaggedDocument(tokens_str.split(), [index]) for index, tokens_str in np.ndenumerate(X)]\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, workers=self.workers, epochs=self.epochs)\n",
    "        self.d2v_model.build_vocab(tagged_x)\n",
    "        self.d2v_model.train(tagged_x, total_examples=self.d2v_model.corpus_count, \n",
    "                             epochs=self.epochs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.asmatrix(np.array([self.d2v_model.infer_vector(tokens_str.split())\n",
    "                                     for tokens_str in X]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "doc2vec_trf = Doc2VecTransformer(epochs=1)\n",
    "doc2vec_trf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test[0].split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inferred_vector = doc2vec_trf.d2v_model.infer_vector(X_test[0].split())\n",
    "print(inferred_vector.shape)\n",
    "print(type(inferred_vector))\n",
    "print(inferred_vector)\n",
    "\n",
    "inferred_vector2 = np.asmatrix(np.array([doc2vec_trf.d2v_model.infer_vector(X_test[0].split())]))\n",
    "print(inferred_vector2.shape)\n",
    "print(type(inferred_vector2))\n",
    "print(inferred_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc2vec_features = doc2vec_trf.transform(X_test)\n",
    "print(doc2vec_features.shape)\n",
    "doc2vec_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Doc2Vec using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Extra Feature - Distance Between Message Word Vector to Categories Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove as our space vectorizer as it is trained on a broad corpus and thus able to better describe texts / words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the embeddings for the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_tokens = np.array([np.array(cat.split('_')) for cat in category_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_300d_w2v_mean = MeanEmbeddingTrainVectorizer(glove_300d_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_300d_w2v_mean.fit(categories_tokens, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tokens_embeddings = glove_300d_w2v_mean.transform(categories_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cat_tokens_embeddings.shape)\n",
    "print(cat_tokens_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_tokens_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msgs_embeddings = glove_300d_w2v_mean.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_msgs_embeddings.shape)\n",
    "print(test_msgs_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_msgs_embeddings[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(test_msgs_embeddings[:10].shape)\n",
    "print(cat_tokens_embeddings.shape)\n",
    "print(cosine_similarity(test_msgs_embeddings[:10],cat_tokens_embeddings).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a CategoriesSimilarity Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the transformer manually using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf = CategoriesSimilarity(categories_tokens, glove_50d_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf.fit(X_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CategoriesSimilarity feature using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cats_sim_pip = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, cat_tokens_embeddings)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "cats_sim_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cats_sim_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"Score = \", cats_sim_pip.score(X_test, y_test))\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. NLP Feature Selection using Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 - All features together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "all_feats = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "all_feats.fit(X_train, y_train)\n",
    "\n",
    "y_pred = all_feats.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = all_feats.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize_to_list(lemmatizer=lemmatizer), ngram_range=(1,3))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('multi_clf', MultiOutputClassifier(RandomForestClassifier(random_state=199), n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipelines for different combinations of features and classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "local_w2v_tfidf_svc = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "local_w2v_tfidf_rf = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_svc = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_svc = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_rf = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_svc = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pipelines = [local_w2v_tfidf_svc, local_w2v_tfidf_rf, glove_tfidf_svc, glove_tfidf_rf, \n",
    "             doc2vec_svc, doc2vec_rf, all_feats_svc, all_feats_rf]\n",
    "\n",
    "grid_dict = {0 : 'Local Word2Vec - TF-IDF - Linear SVC',\n",
    "             1 : 'Local Word2Vec - TF-IDF - Random Forest',\n",
    "             2 : 'Glove 300d - TF-IDF - Linear SVC',\n",
    "             3 : 'Glove 300d - TF-IDF - Random Forest',\n",
    "             4 : 'Doc2Vec - Linear SVC',\n",
    "             5 : 'Doc2Vec - Random Forest',\n",
    "             6 : 'All Features - Linear SVC',\n",
    "             7 : 'All Features - Random Forest',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [\n",
    "#     {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "#      'features__num_dims' : [50,100,300]},\n",
    "#     {'features' : [Doc2VecTransformer()],\n",
    "#      'features__vector_size' : [50,100,300]},\n",
    "#     {'features' : [CategoriesSimilarity(glove_50d_w2v,categories_tokens)],\n",
    "#      'features__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v]},\n",
    "#     {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "#      'features__num_dims' : [50,100,300]},\n",
    "#     {'classifier' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "#      'classifier__estimator__penalty' : ['l1', 'l2'],\n",
    "#      'classifier__estimator__C' : [0.1, 1, 3],\n",
    "#      'classifier__estimator__solver' : ['liblinear']},\n",
    "#     {'classifier' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "#     'classifier__estimator__n_estimators' : [10,50,250],\n",
    "#     'classifier__estimator__max_depth' : [10, 50, 100],\n",
    "#     'classifier__estimator__min_samples_split' : [1, 5, 10]}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic_pip = Pipeline([\n",
    "#     ('features',TfidfEmbeddingTrainVectorizer()),\n",
    "#     ('classifier',MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanEmbeddingTrainVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [\n",
    "#     {'features' : [TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v],\n",
    "#      'features__num_dims' : [50]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# #score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "# jobs = -1\n",
    "# score = 'f1_micro'\n",
    "# def_cv = 3\n",
    "\n",
    "# generic_gs = GridSearchCV(estimator=generic_pip,\n",
    "#             param_grid=param_grid,\n",
    "#             scoring=score,\n",
    "#             cv=def_cv,\n",
    "#             n_jobs=jobs,\n",
    "#             verbose=True)  \n",
    "\n",
    "# # randomized_gs = RandomizedSearchCV(estimator=generic_pip,\n",
    "# #             param_distributions=param_grid,\n",
    "# #             scoring=score,\n",
    "# #             cv=def_cv,\n",
    "# #             n_jobs=jobs,\n",
    "# #             verbose=True,\n",
    "# #             random_state=199,\n",
    "# #             n_iter=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# grids = [generic_gs]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Generic Grid',\n",
    "# }\n",
    "\n",
    "# print('Performing Grid Search...')\n",
    "# # Fit grid search\n",
    "# generic_gs.fit(X_train, y_train)\n",
    "# # Best params\n",
    "# print('Best params: %s' % generic_gs.best_params_)\n",
    "# # Best training data accuracy\n",
    "# print('Best training score: %.3f' % generic_gs.best_score_)\n",
    "# # Predict on test data with best params\n",
    "# test_score = generic_gs.score(X_test, y_test)\n",
    "# # Test data accuracy of model with best params\n",
    "# print('Test set score for best params: %.3f ' % test_score)\n",
    "\n",
    "# # Track Grid Search results\n",
    "# gs_results_df = pd.DataFrame(generic_gs.cv_results_)\n",
    "# gs_results_df['best_model_test_score'] = test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a RandomizedSearchCV for each featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTrainVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, word2vec_model=None, num_dims=100):\n",
    "        if word2vec_model is None:\n",
    "            self.word2vec_model = None\n",
    "            self.num_dims = num_dims\n",
    "            self.workers = multiprocessing.cpu_count() - 1\n",
    "            \n",
    "        else:\n",
    "            self.word2vec_model = word2vec_model\n",
    "            self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "        print(self.num_dims)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers)\n",
    "        \n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingTrainVectorizer(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, word2vec_model=None, num_dims=100):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = num_dims\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.workers_ = multiprocessing.cpu_count() - 1\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers_)\n",
    "        self.num_dims = self.word2vec_model.vector_size\n",
    "            \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights_ = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights_[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CategoriesSimilarity(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, categories_tokens, word2vec_model=None, num_dims=100):\n",
    "        self.categories_tokens = categories_tokens\n",
    "        self.word2vec_model = word2vec_model    \n",
    "        self.num_dims = num_dims\n",
    "        \n",
    "    def compute_mean_embeddings(self, tokens_array):    \n",
    "        mean_embeddings = np.empty([tokens_array.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(tokens_array.shape[0]):\n",
    "            doc_tokens = tokens_array[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings\n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.workers_ = multiprocessing.cpu_count() - 1\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers_)\n",
    "        self.num_dims = self.word2vec_model.vector_size        \n",
    "        self.categories_vectors_ = self.compute_mean_embeddings(self.categories_tokens)\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = self.compute_mean_embeddings(X)\n",
    "        cats_similarities = cosine_similarity(mean_embeddings, self.categories_vectors_)\n",
    "            \n",
    "        return cats_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Pipelines for each featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nlp_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Vectorizers/Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#Models\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss, accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_local_w2v = Pipeline([\n",
    "    ('local_w2v', nlp_estimators.TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "# pip_glove = Pipeline([\n",
    "#     ('glove', nlp_estimators.TfidfEmbeddingTrainVectorizer(glove_50d_w2v)),\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "# ])\n",
    "\n",
    "pip_doc2vec = Pipeline([\n",
    "    ('doc2vec', nlp_estimators.Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "# pip_cats_sim = Pipeline([\n",
    "#     ('cats_sim', nlp_estimators.CategoriesSimilarity(categories_tokens=categories_tokens)),\n",
    "#     ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Grid Params for each featureset / classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_models_simple = [MultiOutputClassifier(RandomForestClassifier(random_state=199,\n",
    "                                                        n_estimators=50,\n",
    "                                                        max_depth=100,\n",
    "                                                        min_samples_split=5)),\n",
    "                                            MultiOutputClassifier(LogisticRegression(random_state=199,\n",
    "                                                                                    solver='liblinear',\n",
    "                                                                                    C=1,\n",
    "                                                                                    penalty='l2'))]\n",
    "\n",
    "params_options_local_w2v_simple = {'local_w2v__num_dims' : [50,100],\n",
    "                                   'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_local_w2v = {'local_w2v__num_dims' : [50,100,300],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_glove = {'glove__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_doc2vec = {'doc2vec__vector_size' : [50,100,300],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_cat_sim = {'cats_sim__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_rf = {'clf' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "                     'clf__estimator__n_estimators' : [50,250],\n",
    "                     'clf__estimator__max_depth' : [50,100],\n",
    "                     'clf__estimator__min_samples_split' : [2, 5]}\n",
    "\n",
    "params_options_lg = {'clf' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "                     'clf__estimator__penalty' : ['l1', 'l2'],\n",
    "                     'clf__estimator__C' : [0.1, 1, 3],\n",
    "                     'clf__estimator__solver' : ['liblinear']}\n",
    "\n",
    "params_options_empty = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "jobs = -1\n",
    "score = 'f1_micro'\n",
    "def_cv = 3\n",
    "verbosity_level=10\n",
    "\n",
    "gs_local_w2v = GridSearchCV(estimator=pip_local_w2v,\n",
    "            param_grid=params_options_local_w2v,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level)  \n",
    "\n",
    "gs_glove = GridSearchCV(estimator=pip_glove,\n",
    "            param_grid=params_options_glove,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "gs_doc2vec = GridSearchCV(estimator=pip_doc2vec,\n",
    "            param_grid=params_options_doc2vec,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level)\n",
    "\n",
    "gs_cats_sim = GridSearchCV(estimator=pip_cats_sim,\n",
    "            param_grid=params_options_cat_sim,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "randomized_gs = RandomizedSearchCV(estimator=pip_local_w2v,\n",
    "            param_distributions=params_options_local_w2v,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level,\n",
    "            random_state=199,\n",
    "            n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "grids = [gs_local_w2v, gs_glove, gs_doc2vec, gs_cats_sim]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Feature: Local W2V',\n",
    "    1 : 'Feature: Glove',\n",
    "    2 : 'Feature: Doc2Vec',\n",
    "    3 : 'Feature: Category Similarity'\n",
    "}\n",
    "\n",
    "# grids = [gs_all_feats_best_params]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Feature: All Features with Best Params',\n",
    "# }\n",
    "\n",
    "# grids = [gs_local_w2v]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Feature: Local W2V',\n",
    "# }\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_score = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "overall_results_df = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    overall_results_df = pd.concat([overall_results_df, gs_results_df])\n",
    "    \n",
    "    # Track best (highest test accuracy) model\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_feature_sets = overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_feature_sets.to_csv('f1micro_score_per_featset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_local_w2v.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_glove.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_doc2vec.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_cats_sim.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv('f1micro_score_per_featset.csv')\n",
    "test[test['rank_test_score'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test All Features together using their best params from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_all_feats = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "        ('glove', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer(vector_size=50)),\n",
    "        ('cats_sim', CategoriesSimilarity(categories_tokens=categories_tokens,word2vec_model=glove_300d_w2v))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_all_feats = {'clf' : params_options_models_simple}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_all_feats_best_params = GridSearchCV(estimator=pip_all_feats,\n",
    "                                  param_grid=params_options_all_feats,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs,\n",
    "            verbose=verbosity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [gs_all_feats_best_params]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Feature: All Features with Best Params',\n",
    "}\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "all_feats_results = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    all_feats_results = pd.concat([all_feats_results, gs_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats_results.to_csv('f1micro_score_all_feats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Decision between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_local_w2v_final = Pipeline([\n",
    "    ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pip_all_feats_final = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "        ('glove', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer(vector_size=50)),\n",
    "        ('cats_sim', CategoriesSimilarity(categories_tokens=categories_tokens,word2vec_model=glove_300d_w2v))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_rf = {'clf' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "                     'clf__estimator__n_estimators' : [25,50,100],\n",
    "                     'clf__estimator__max_depth' : [100],\n",
    "                     'clf__estimator__min_samples_split' : [2, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_local_w2v_final = GridSearchCV(estimator=pip_local_w2v_final,\n",
    "            param_grid=params_options_rf,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "gs_all_feats_final = GridSearchCV(estimator=pip_all_feats_final,\n",
    "                                  param_grid=params_options_rf,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs,\n",
    "            verbose=verbosity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_final = [gs_local_w2v_final]\n",
    "\n",
    "grid_final_dict = {\n",
    "    0 : 'Feature: Local W2V with Best Params',\n",
    "    1 : 'Feature: All Features with Best Params'\n",
    "}\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "final_results = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids_final):\n",
    "    print('\\nEstimator: %s' % grid_final_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_final_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    final_results = pd.concat([final_results, gs_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_results_local_w2v = final_results\n",
    "final_results_local_w2v.to_csv('f1_score_local_w2v_final.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ = [\n",
    "    {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "     'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "     'features__num_dims' : [50,100,300]},\n",
    "    {'features' : [Doc2VecTransformer()],\n",
    "     'features__vector_size' : [50,100,300]},\n",
    "    {'features' : [CategoriesSimilarity(glove_50d_w2v,categories_tokens)],\n",
    "     'features__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v]},\n",
    "    {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "     'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "     'features__num_dims' : [50,100,300]},\n",
    "    {'classifier' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "     'classifier__estimator__penalty' : ['l1', 'l2'],\n",
    "     'classifier__estimator__C' : [0.1, 1, 3],\n",
    "     'classifier__estimator__solver' : ['liblinear']},\n",
    "    {'classifier' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "    'classifier__estimator__n_estimators' : [10,50,250],\n",
    "    'classifier__estimator__max_depth' : [10, 50, 100],\n",
    "    'classifier__estimator__min_samples_split' : [1, 5, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "pip_local_w2v_tfidf_naive = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_local_w2v_tfidf_rf = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_naive = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_lg = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(LogisticRegression(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_doc2vec_naive = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_doc2vec_rf = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_cats_sim_naive = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_cats_sim_rf = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_all_feats_naive = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_all_feats_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_empty = [{}]\n",
    "\n",
    "# grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "#         'clf__min_samples_leaf': param_range,\n",
    "#         'clf__max_depth': param_range,\n",
    "#         'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "# grid_params_svm = [{'clf__kernel': ['linear', 'rbf'], \n",
    "#         'clf__C': param_range}]\n",
    "\n",
    "# Construct grid searches\n",
    "jobs = -1\n",
    "\n",
    "#score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "score = 'f1_micro'\n",
    "\n",
    "def_cv = 3\n",
    "\n",
    "gs_local_w2v_tfidf_naive = GridSearchCV(estimator=pip_local_w2v_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)  \n",
    "\n",
    "gs_local_w2v_tfidf_rf = GridSearchCV(estimator=pip_local_w2v_tfidf_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_glove_tfidf_naive = GridSearchCV(estimator=pip_glove_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_glove_tfidf_rf = GridSearchCV(estimator=pip_glove_tfidf_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_glove_tfidf_lg = GridSearchCV(estimator=pip_glove_tfidf_lg,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "\n",
    "gs_doc2vec_naive = GridSearchCV(estimator=pip_doc2vec_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_doc2vec_rf = GridSearchCV(estimator=pip_doc2vec_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_cats_sim_naive = GridSearchCV(estimator=pip_cats_sim_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_cats_sim_rf = GridSearchCV(estimator=pip_cats_sim_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_all_feats_naive = GridSearchCV(estimator=pip_all_feats_naive,\n",
    "                                  param_grid=grid_params_empty,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs)\n",
    "\n",
    "gs_all_feats_rf = GridSearchCV(estimator=pip_all_feats_rf,\n",
    "                                  param_grid=grid_params_empty,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs)\n",
    "\n",
    "# grids = [gs_local_w2v_tfidf_rf, gs_glove_tfidf_rf, \n",
    "#          gs_doc2vec_rf, gs_cats_sim_rf,\n",
    "#          gs_all_feats_rf]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Local Word2Vec - TF-IDF - Random Forest',\n",
    "#     1 : 'Glove 300d - TF-IDF - Random Forest',\n",
    "#     2 : 'Doc2Vec - Random Forest',\n",
    "#     3 : 'Categories Similarity - Random Forest',\n",
    "#     4 : 'All Features - Random Forest'\n",
    "# }\n",
    "\n",
    "# grids = [gs_local_w2v_tfidf_naive, gs_glove_tfidf_naive, \n",
    "#          gs_doc2vec_naive, gs_cats_sim_naive,\n",
    "#          gs_all_feats_naive]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Local Word2Vec - TF-IDF - Naive Bayes',\n",
    "#     1 : 'Glove 300d - TF-IDF - Naive Bayes',\n",
    "#     2 : 'Doc2Vec - Naive Bayes',\n",
    "#     3 : 'Categories Similarity - Naive Bayes',\n",
    "#     4 : 'All Features - Naive Bayes'\n",
    "# }\n",
    "\n",
    "grids = [gs_glove_tfidf_lg]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Glove 300d - TF-IDF - Logistic Regression',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_score = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "overall_results_df = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    \n",
    "    overall_results_df = pd.concat([overall_results_df, gs_results_df])\n",
    "    \n",
    "    # Track best (highest test accuracy) model\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lg_f1micro_score = overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lg_f1micro_score.to_csv('naive_f1micro_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'multi_clf__estimator__n_estimators': [20,50],\n",
    "        'multi_clf__estimator__max_depth': [50, 100]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train classifier\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(cv, 'classifier.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
