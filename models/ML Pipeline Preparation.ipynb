{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "messages_df = pd.read_sql_table(con=engine, table_name='Message')\n",
    "categories_df = pd.read_sql_table(con=engine, table_name='MessageCategoryWide')\n",
    "X = messages_df.message.values\n",
    "Y = categories_df.drop('message_id', axis=1).values\n",
    "category_columns = categories_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Weather update - a cold front from Cuba that could pass over Haiti',\n",
       "       'Is the Hurricane over or is it not over',\n",
       "       'Looking for someone but no name', ...,\n",
       "       \"Proshika, operating in Cox's Bazar municipality and 5 other unions, Ramu and Chokoria, assessment, 5 kg rice, 1,5 kg lentils to 700 families.\",\n",
       "       'Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.',\n",
       "       'A radical shift in thinking came about as a result of this meeting, recognizing that HIV/AIDS is at the core of the humanitarian crisis and identifying the crisis itself as a function of the HIV/AIDS pandemic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Split dataset\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crudess you are the most charm and charismatic DT member .. thanks for all your videos in Santiago.. and to talk with me twice',\n",
       "       'Tidal waves triggered by an earthquake off Indonesia on Sunday swept over a vast swath of coastlines, including those in India, Indonesia, Malaysia, the Maldives, Somalia, Sri Lanka and Thailand.',\n",
       "       'The earthquake happened at 4h34 in the evening and for the night. ',\n",
       "       ...,\n",
       "       'Earthquake of the day #Haiti or Google possibly leaving China ?',\n",
       "       'When the matter had been debated previously, the Non-Aligned Movement foreign ministers firmly stated that there was no right to humanitarian intervention.',\n",
       "       '@DumboNYC : Oh no RT @endtwist : ConEd power station in #DUMBO is flooding ! #nopower #sandy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19608,) (19608, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write a tokenization function to process your text data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_str(text):\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "    #Return tokens list as a string joined by whitespaces\n",
    "    clean_tokens_str = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather update cold front cuba could pas haiti'"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_str(X[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Generate tokenized messages table for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_df['tokens'] = messages_df.message.apply(lambda x: tokenize_str(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "      <td>weather update cold front cuba could pas haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "      <td>looking someone name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "      <td>un report leogane 80 90 destroyed hospital st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "      <td>say west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "      <td>training demonstrated enhance micronutrient pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "      <td>suitable candidate selected ocha jakarta curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "      <td>proshika operating cox bazar municipality 5 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "      <td>2 000 woman protesting conduct election tearga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "      <td>radical shift thinking came result meeting rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12   \n",
       "...                                                  ...     ...        ...   \n",
       "26139                                               None    news         21   \n",
       "26140                                               None    news         22   \n",
       "26141                                               None    news         23   \n",
       "26142                                               None    news         31   \n",
       "26143                                               None    news         36   \n",
       "\n",
       "                                                  tokens  \n",
       "0         weather update cold front cuba could pas haiti  \n",
       "1                                              hurricane  \n",
       "2                                   looking someone name  \n",
       "3      un report leogane 80 90 destroyed hospital st ...  \n",
       "4         say west side haiti rest country today tonight  \n",
       "...                                                  ...  \n",
       "26139  training demonstrated enhance micronutrient pr...  \n",
       "26140  suitable candidate selected ocha jakarta curre...  \n",
       "26141  proshika operating cox bazar municipality 5 un...  \n",
       "26142  2 000 woman protesting conduct election tearga...  \n",
       "26143  radical shift thinking came result meeting rec...  \n",
       "\n",
       "[26144 rows x 6 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens = messages_df[['message_id','tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens.to_sql('MessageTokens', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Find n-grams and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngrams_freqs(messages_array, n=1):\n",
    "    vec = CountVectorizer(ngram_range=(n, n)).fit(messages_array)\n",
    "    bag_of_words = vec.transform(messages_array)\n",
    "    word_count = bag_of_words.sum(axis=0)\n",
    "    words_freq = [(word, n, word_count[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "    words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "    words_freq_df = pd.DataFrame(data = words_freq, columns = ['ngram','n','count'])\n",
    "    return words_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrams_freqs = get_ngrams_freqs(messages_tokens.tokens)\n",
    "bigrams_freqs = get_ngrams_freqs(messages_tokens.tokens, n=2)\n",
    "trigrams_freqs = get_ngrams_freqs(messages_tokens.tokens, n=3)\n",
    "ngrams_freqs = pd.concat([unigrams_freqs, bigrams_freqs, trigrams_freqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ngram</th>\n",
       "      <th>n</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>water</td>\n",
       "      <td>1</td>\n",
       "      <td>3034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>people</td>\n",
       "      <td>1</td>\n",
       "      <td>2998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>food</td>\n",
       "      <td>1</td>\n",
       "      <td>2892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>help</td>\n",
       "      <td>1</td>\n",
       "      <td>2649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>need</td>\n",
       "      <td>1</td>\n",
       "      <td>2484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289130</th>\n",
       "      <td>crisis identifying crisis</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289131</th>\n",
       "      <td>identifying crisis function</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289132</th>\n",
       "      <td>crisis function hiv</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289133</th>\n",
       "      <td>function hiv aid</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289134</th>\n",
       "      <td>hiv aid pandemic</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>558407 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              ngram  n  count\n",
       "0                             water  1   3034\n",
       "1                            people  1   2998\n",
       "2                              food  1   2892\n",
       "3                              help  1   2649\n",
       "4                              need  1   2484\n",
       "...                             ... ..    ...\n",
       "289130    crisis identifying crisis  3      1\n",
       "289131  identifying crisis function  3      1\n",
       "289132          crisis function hiv  3      1\n",
       "289133             function hiv aid  3      1\n",
       "289134             hiv aid pandemic  3      1\n",
       "\n",
       "[558407 rows x 3 columns]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ngrams_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngrams_freqs.to_sql('NGramsFreqs', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train a Word2Vec model using messages text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(object):\n",
    "    \"\"\"An interator that yields messages tokens (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, messages_df, sample_size=-1):\n",
    "        self.messages_df = messages_df\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        messages = messages_df\n",
    "        \n",
    "        #Sample dataset if specified\n",
    "        if self.sample_size > 0:\n",
    "            messages = messages.sample(self.sample_size)\n",
    "        \n",
    "        #Yeld tokenized message joined by whitespaces\n",
    "        for token_str in messages.tokens:\n",
    "            yield token_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = Message2(messages_df=messages_df)\n",
    "w2v_model_full = gensim.models.Word2Vec(sentences=sentences)\n",
    "w2v_model_full.save(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_full = gensim.models.Word2Vec.load(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "#?w2v_model_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather\n",
      "update\n",
      "cold\n",
      "front\n",
      "cuba\n",
      "could\n",
      "pas\n",
      "haiti\n",
      "hurricane\n",
      "looking\n"
     ]
    }
   ],
   "source": [
    "for i, word in enumerate(w2v_model_full.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.5672106 ,  0.29789364,  0.463643  ,  0.19121738, -0.58913875,\n",
       "        0.86039734,  0.40682167, -0.57124066,  1.2928406 ,  0.45593792,\n",
       "        1.0122268 , -0.3228534 , -1.087786  , -0.35704237,  1.0200481 ,\n",
       "       -1.1935841 , -0.02619644, -0.20060593, -0.10999487,  0.62313074,\n",
       "        0.03346012,  0.59004444,  0.19915423, -0.5932419 ,  0.75793874,\n",
       "       -0.26756352,  0.36034623,  0.32203496, -0.27114403,  0.55780137,\n",
       "       -1.3636817 , -0.35188434,  0.33116597, -0.20347492, -0.14132884,\n",
       "       -0.4912949 , -0.05714604,  0.15034439, -0.01098222,  0.47501513,\n",
       "       -0.35004178,  0.39268813, -0.04542985,  0.761094  , -0.50398266,\n",
       "       -0.6441565 ,  0.64477974, -0.51708215, -0.18178183,  0.3622531 ,\n",
       "       -0.52592355, -1.5337172 ,  0.7676356 ,  0.15075684, -0.231194  ,\n",
       "       -0.16355547, -0.78194165,  0.8512226 , -0.9633786 ,  0.41826957,\n",
       "       -0.9286241 ,  0.4154575 ,  0.22709647,  0.41657946,  0.01742937,\n",
       "        0.2738443 , -1.082624  , -0.57317954,  0.81713694, -0.4791164 ,\n",
       "        0.46778417,  0.10105462, -0.55970675,  0.8767134 , -0.7517109 ,\n",
       "       -0.11670218, -0.21207199, -0.47678208,  0.24365072,  0.14143495,\n",
       "       -0.63220555,  0.9098334 ,  0.3518068 ,  0.6010166 , -0.50933605,\n",
       "        0.27859965, -0.13834418, -0.81977206, -0.9080134 , -1.06892   ,\n",
       "        0.34538126, -1.1616817 , -0.08282463, -0.73922217,  0.38757223,\n",
       "       -0.08250178, -0.23181756, -1.2451925 , -0.31382206,  0.36446193],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_model_full.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Implement Vector Aggregator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([1,2])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26144"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Weather update - a cold front from Cuba that could pass over Haiti'"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26144"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec_model, num_dims):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = num_dims\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model.wv[w] for w in doc_tokens if w in self.word2vec_model.wv]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings\n",
    "    \n",
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec_model, num_dims):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = num_dims\n",
    "        self.word_weights = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model.wv[w]*self.word_weights[w] for w in doc_tokens if w in self.word2vec_model.wv]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 227,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dict = defaultdict(lambda: 1, [(\"Hello\" , 7), (\"hi\" , 10), (\"there\" , 45),(\"at\" , 23),(\"this\" , 77)])\n",
    "test_dict['Hi']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a Pipeline to train the Word2Vec estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('w2v_mean', TfidfEmbeddingVectorizer(w2v_model_full, w2v_model_full.wv.vectors.shape[1])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 12.6 s, sys: 7.14 ms, total: 12.6 s\n",
      "Wall time: 12.6 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('w2v_mean',\n",
       "                 <__main__.TfidfEmbeddingVectorizer object at 0x7f329d07b450>),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=GaussianNB(priors=None,\n",
       "                                                            var_smoothing=1e-09),\n",
       "                                       n_jobs=None))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 229,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "message_id-0       0.28      0.50      0.36      1532\n",
      "message_id-1       0.80      0.55      0.65      4950\n",
      "\n",
      "   micro avg       0.57      0.54      0.55      6482\n",
      "   macro avg       0.54      0.53      0.51      6482\n",
      "weighted avg       0.67      0.54      0.58      6482\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   related-0       0.83      0.94      0.88      5393\n",
      "   related-1       0.22      0.08      0.12      1143\n",
      "\n",
      "    accuracy                           0.79      6536\n",
      "   macro avg       0.52      0.51      0.50      6536\n",
      "weighted avg       0.72      0.79      0.75      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   request-0       1.00      0.14      0.25      6506\n",
      "   request-1       0.01      1.00      0.01        30\n",
      "\n",
      "    accuracy                           0.15      6536\n",
      "   macro avg       0.50      0.57      0.13      6536\n",
      "weighted avg       1.00      0.15      0.25      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     offer-0       0.59      0.92      0.72      3829\n",
      "     offer-1       0.44      0.09      0.15      2707\n",
      "\n",
      "    accuracy                           0.58      6536\n",
      "   macro avg       0.51      0.50      0.43      6536\n",
      "weighted avg       0.53      0.58      0.48      6536\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "aid_related-0       0.92      0.95      0.94      6028\n",
      "aid_related-1       0.07      0.04      0.05       508\n",
      "\n",
      "     accuracy                           0.88      6536\n",
      "    macro avg       0.49      0.50      0.49      6536\n",
      " weighted avg       0.86      0.88      0.87      6536\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "medical_help-0       0.96      0.11      0.19      6208\n",
      "medical_help-1       0.05      0.92      0.10       328\n",
      "\n",
      "      accuracy                           0.15      6536\n",
      "     macro avg       0.51      0.51      0.14      6536\n",
      "  weighted avg       0.92      0.15      0.19      6536\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "medical_products-0       0.97      0.10      0.18      6351\n",
      "medical_products-1       0.03      0.89      0.05       185\n",
      "\n",
      "          accuracy                           0.12      6536\n",
      "         macro avg       0.50      0.49      0.12      6536\n",
      "      weighted avg       0.94      0.12      0.17      6536\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "search_and_rescue-0       0.99      0.11      0.20      6419\n",
      "search_and_rescue-1       0.02      0.96      0.04       117\n",
      "\n",
      "           accuracy                           0.13      6536\n",
      "          macro avg       0.51      0.54      0.12      6536\n",
      "       weighted avg       0.98      0.13      0.20      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  security-0       0.99      0.17      0.28      6348\n",
      "  security-1       0.03      0.93      0.06       188\n",
      "\n",
      "    accuracy                           0.19      6536\n",
      "   macro avg       0.51      0.55      0.17      6536\n",
      "weighted avg       0.96      0.19      0.28      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  military-0       1.00      1.00      1.00      6536\n",
      "  military-1       0.00      0.00      0.00         0\n",
      "\n",
      "   micro avg       1.00      1.00      1.00      6536\n",
      "   macro avg       0.50      0.50      0.50      6536\n",
      "weighted avg       1.00      1.00      1.00      6536\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "child_alone-0       0.94      0.94      0.94      6123\n",
      "child_alone-1       0.06      0.06      0.06       413\n",
      "\n",
      "     accuracy                           0.88      6536\n",
      "    macro avg       0.50      0.50      0.50      6536\n",
      " weighted avg       0.88      0.88      0.88      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     water-0       0.89      0.94      0.91      5786\n",
      "     water-1       0.12      0.07      0.09       750\n",
      "\n",
      "    accuracy                           0.84      6536\n",
      "   macro avg       0.50      0.50      0.50      6536\n",
      "weighted avg       0.80      0.84      0.82      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      food-0       0.92      0.93      0.92      5964\n",
      "      food-1       0.12      0.10      0.11       572\n",
      "\n",
      "    accuracy                           0.86      6536\n",
      "   macro avg       0.52      0.52      0.52      6536\n",
      "weighted avg       0.85      0.86      0.85      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "   shelter-0       0.99      0.15      0.27      6426\n",
      "   shelter-1       0.02      0.89      0.03       110\n",
      "\n",
      "    accuracy                           0.17      6536\n",
      "   macro avg       0.50      0.52      0.15      6536\n",
      "weighted avg       0.97      0.17      0.26      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  clothing-0       0.97      0.11      0.19      6395\n",
      "  clothing-1       0.02      0.84      0.04       141\n",
      "\n",
      "    accuracy                           0.12      6536\n",
      "   macro avg       0.49      0.47      0.12      6536\n",
      "weighted avg       0.95      0.12      0.19      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     money-0       0.99      0.94      0.96      6462\n",
      "     money-1       0.02      0.11      0.03        74\n",
      "\n",
      "    accuracy                           0.93      6536\n",
      "   macro avg       0.50      0.52      0.50      6536\n",
      "weighted avg       0.98      0.93      0.95      6536\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "missing_people-0       0.98      0.10      0.17      6322\n",
      "missing_people-1       0.03      0.94      0.07       214\n",
      "\n",
      "        accuracy                           0.12      6536\n",
      "       macro avg       0.51      0.52      0.12      6536\n",
      "    weighted avg       0.95      0.12      0.17      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  refugees-0       0.96      0.86      0.91      6244\n",
      "  refugees-1       0.08      0.27      0.13       292\n",
      "\n",
      "    accuracy                           0.83      6536\n",
      "   macro avg       0.52      0.56      0.52      6536\n",
      "weighted avg       0.92      0.83      0.87      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     death-0       0.86      0.97      0.91      5660\n",
      "     death-1       0.09      0.02      0.03       876\n",
      "\n",
      "    accuracy                           0.84      6536\n",
      "   macro avg       0.48      0.49      0.47      6536\n",
      "weighted avg       0.76      0.84      0.80      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " other_aid-0       0.95      0.12      0.22      6111\n",
      " other_aid-1       0.07      0.90      0.12       425\n",
      "\n",
      "    accuracy                           0.17      6536\n",
      "   macro avg       0.51      0.51      0.17      6536\n",
      "weighted avg       0.89      0.17      0.21      6536\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          precision    recall  f1-score   support\n",
      "\n",
      "infrastructure_related-0       0.97      0.10      0.18      6219\n",
      "infrastructure_related-1       0.05      0.94      0.10       317\n",
      "\n",
      "                accuracy                           0.14      6536\n",
      "               macro avg       0.51      0.52      0.14      6536\n",
      "            weighted avg       0.93      0.14      0.18      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " transport-0       0.95      0.09      0.17      6209\n",
      " transport-1       0.05      0.91      0.10       327\n",
      "\n",
      "    accuracy                           0.13      6536\n",
      "   macro avg       0.50      0.50      0.13      6536\n",
      "weighted avg       0.91      0.13      0.17      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " buildings-0       0.97      0.11      0.20      6414\n",
      " buildings-1       0.02      0.84      0.03       122\n",
      "\n",
      "    accuracy                           0.13      6536\n",
      "   macro avg       0.50      0.48      0.12      6536\n",
      "weighted avg       0.96      0.13      0.20      6536\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "electricity-0       1.00      0.13      0.23      6494\n",
      "electricity-1       0.01      0.90      0.01        42\n",
      "\n",
      "     accuracy                           0.13      6536\n",
      "    macro avg       0.50      0.52      0.12      6536\n",
      " weighted avg       0.99      0.13      0.23      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     tools-0       1.00      0.14      0.24      6476\n",
      "     tools-1       0.01      0.95      0.02        60\n",
      "\n",
      "    accuracy                           0.14      6536\n",
      "   macro avg       0.50      0.54      0.13      6536\n",
      "weighted avg       0.99      0.14      0.24      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      " hospitals-0       1.00      0.14      0.24      6505\n",
      " hospitals-1       0.00      0.87      0.01        31\n",
      "\n",
      "    accuracy                           0.14      6536\n",
      "   macro avg       0.50      0.50      0.12      6536\n",
      "weighted avg       0.99      0.14      0.24      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     shops-0       0.99      0.14      0.24      6467\n",
      "     shops-1       0.01      0.90      0.02        69\n",
      "\n",
      "    accuracy                           0.14      6536\n",
      "   macro avg       0.50      0.52      0.13      6536\n",
      "weighted avg       0.98      0.14      0.24      6536\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "aid_centers-0       0.96      0.12      0.22      6244\n",
      "aid_centers-1       0.05      0.89      0.09       292\n",
      "\n",
      "     accuracy                           0.16      6536\n",
      "    macro avg       0.50      0.51      0.15      6536\n",
      " weighted avg       0.92      0.16      0.21      6536\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "other_infrastructure-0       0.78      0.08      0.14      4743\n",
      "other_infrastructure-1       0.28      0.94      0.43      1793\n",
      "\n",
      "              accuracy                           0.31      6536\n",
      "             macro avg       0.53      0.51      0.28      6536\n",
      "          weighted avg       0.64      0.31      0.22      6536\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "weather_related-0       0.94      0.10      0.19      5991\n",
      "weather_related-1       0.09      0.93      0.16       545\n",
      "\n",
      "         accuracy                           0.17      6536\n",
      "        macro avg       0.51      0.51      0.17      6536\n",
      "     weighted avg       0.87      0.17      0.18      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    floods-0       0.94      0.08      0.15      5917\n",
      "    floods-1       0.10      0.95      0.18       619\n",
      "\n",
      "    accuracy                           0.17      6536\n",
      "   macro avg       0.52      0.52      0.17      6536\n",
      "weighted avg       0.86      0.17      0.16      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     storm-0       0.99      0.12      0.21      6468\n",
      "     storm-1       0.01      0.93      0.02        68\n",
      "\n",
      "    accuracy                           0.13      6536\n",
      "   macro avg       0.50      0.52      0.12      6536\n",
      "weighted avg       0.98      0.13      0.21      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      fire-0       0.91      0.07      0.14      5962\n",
      "      fire-1       0.09      0.93      0.16       574\n",
      "\n",
      "    accuracy                           0.15      6536\n",
      "   macro avg       0.50      0.50      0.15      6536\n",
      "weighted avg       0.84      0.15      0.14      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "earthquake-0       0.99      0.15      0.26      6408\n",
      "earthquake-1       0.02      0.95      0.04       128\n",
      "\n",
      "    accuracy                           0.17      6536\n",
      "   macro avg       0.51      0.55      0.15      6536\n",
      "weighted avg       0.97      0.17      0.26      6536\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      cold-0       0.97      0.10      0.18      6187\n",
      "      cold-1       0.06      0.95      0.11       349\n",
      "\n",
      "    accuracy                           0.14      6536\n",
      "   macro avg       0.51      0.52      0.14      6536\n",
      "weighted avg       0.92      0.14      0.18      6536\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "other_weather-0       0.81      0.93      0.87      5272\n",
      "other_weather-1       0.21      0.07      0.11      1264\n",
      "\n",
      "       accuracy                           0.77      6536\n",
      "      macro avg       0.51      0.50      0.49      6536\n",
      "   weighted avg       0.69      0.77      0.72      6536\n",
      "\n",
      "CPU times: user 4.39 s, sys: 7.94 ms, total: 4.39 s\n",
      "Wall time: 4.39 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "#print('accuracy %s' % accuracy_score(y_pred, y_test))\n",
    "#print(classification_report(y_test, y_pred,target_names=category_columns))\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize, ngram_range=(1,3))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('multi_clf', MultiOutputClassifier(RandomForestClassifier(random_state=199), n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19662,)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['It works as a fertilizer enhancement, a composting additive, and even as an alternative to household cleaning products, according to EMRO.',\n",
       "       'I need a job to feed my family. I am an electrician and a musician. I perform harmonization and orchestration in Mexico. I speak English and French. Let me know how I can help',\n",
       "       'The report also warns that if the clan fighting extends to sorghum-producing areas in the south during this critical stage of crop harvest, it could further hamper harvesting and exert a negative impact on food security throughout Somalia.',\n",
       "       ...,\n",
       "       'HELP THE EARTHQUAKE VICTIMS IN HAITI http tinyurl.com yk3bspe links to many resources where you can make donations. Broke? donate time',\n",
       "       'The low snowfall pattern and the snowfall differentiation among catchments are clearly reflected in the irrigated wheat production levels.',\n",
       "       \"Having fun at #Lexi 's ! Real food in real house with electrical lighting . Good times . I feel human again #escapingsandy\"],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "WRITEBACKIFCOPY base is read-only",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m-------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31m_RemoteTraceback\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 418, in _process_worker\n    r = call_item()\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/externals/loky/process_executor.py\", line 272, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/_parallel_backends.py\", line 608, in __call__\n    return self.func(*args, **kwargs)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in __call__\n    for func, args, kwargs in self.items]\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/parallel.py\", line 256, in <listcomp>\n    for func, args, kwargs in self.items]\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\", line 612, in predict\n    proba = self.predict_proba(X)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\", line 656, in predict_proba\n    X = self._validate_X_predict(X)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/ensemble/_forest.py\", line 412, in _validate_X_predict\n    return self.estimators_[0]._validate_X_predict(X, check_input=True)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/tree/_classes.py\", line 380, in _validate_X_predict\n    X = check_array(X, dtype=DTYPE, accept_sparse=\"csr\")\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 511, in check_array\n    accept_large_sparse=accept_large_sparse)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/utils/validation.py\", line 327, in _ensure_sparse_format\n    spmatrix = spmatrix.astype(dtype)\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/scipy/sparse/data.py\", line 74, in astype\n    self._deduped_data().astype(dtype, casting=casting, copy=copy),\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/scipy/sparse/data.py\", line 34, in _deduped_data\n    self.sum_duplicates()\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/scipy/sparse/compressed.py\", line 1091, in sum_duplicates\n    self.sort_indices()\n  File \"/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/scipy/sparse/compressed.py\", line 1137, in sort_indices\n    self.indices, self.data)\nValueError: WRITEBACKIFCOPY base is read-only\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mValueError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-92e69b2e40f8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# predict on test data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/utils/metaestimators.py\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0;31m# lambda, but not partial, allows help() to work with update_wrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 116\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mlambda\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    117\u001b[0m         \u001b[0;31m# update the docstring of the returned function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0mupdate_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X, **predict_params)\u001b[0m\n\u001b[1;32m    418\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwith_final\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    419\u001b[0m             \u001b[0mXt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 420\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpredict_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mif_delegate_has_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelegate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_final_estimator'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    195\u001b[0m         y = Parallel(n_jobs=self.n_jobs)(\n\u001b[1;32m    196\u001b[0m             \u001b[0mdelayed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m             for e in self.estimators_)\n\u001b[0m\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1017\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1018\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1019\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    907\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    560\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[1;32m    561\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 562\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    563\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mLokyTimeoutError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    564\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mCancelledError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    434\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mFINISHED\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 435\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    436\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    437\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/tarciso/anaconda3/envs/udacity-ds-p2/lib/python3.7/concurrent/futures/_base.py\u001b[0m in \u001b[0;36m__get_result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    382\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__get_result\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 384\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_exception\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    385\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    386\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: WRITEBACKIFCOPY base is read-only"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['memory', 'steps', 'vect', 'tfidf', 'multi_clf', 'vect__analyzer', 'vect__binary', 'vect__decode_error', 'vect__dtype', 'vect__encoding', 'vect__input', 'vect__lowercase', 'vect__max_df', 'vect__max_features', 'vect__min_df', 'vect__ngram_range', 'vect__preprocessor', 'vect__stop_words', 'vect__strip_accents', 'vect__token_pattern', 'vect__tokenizer', 'vect__vocabulary', 'tfidf__norm', 'tfidf__smooth_idf', 'tfidf__sublinear_tf', 'tfidf__use_idf', 'multi_clf__estimator__bootstrap', 'multi_clf__estimator__class_weight', 'multi_clf__estimator__criterion', 'multi_clf__estimator__max_depth', 'multi_clf__estimator__max_features', 'multi_clf__estimator__max_leaf_nodes', 'multi_clf__estimator__min_impurity_decrease', 'multi_clf__estimator__min_impurity_split', 'multi_clf__estimator__min_samples_leaf', 'multi_clf__estimator__min_samples_split', 'multi_clf__estimator__min_weight_fraction_leaf', 'multi_clf__estimator__n_estimators', 'multi_clf__estimator__n_jobs', 'multi_clf__estimator__oob_score', 'multi_clf__estimator__random_state', 'multi_clf__estimator__verbose', 'multi_clf__estimator__warm_start', 'multi_clf__estimator', 'multi_clf__n_jobs'])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "        'multi_clf__estimator__n_estimators': [20,50],\n",
    "        'multi_clf__estimator__max_depth': [50, 100]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 1 candidates, totalling 3 fits\n",
      "[CV]  ................................................................\n",
      "[CV] ...................... , score=0.22017088800732376, total= 2.7min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=0.22978333841928594, total= 2.4min\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  5.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] ...................... , score=0.22642660970399756, total= 2.5min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  8.4min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  8.4min finished\n"
     ]
    }
   ],
   "source": [
    "# train classifier\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "  related-0       0.55      0.32      0.41      1565\n",
      "  related-1       0.81      0.91      0.86      4941\n",
      "\n",
      "avg / total       0.74      0.77      0.75      6506\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  request-0       0.89      0.98      0.93      5420\n",
      "  request-1       0.81      0.39      0.53      1134\n",
      "\n",
      "avg / total       0.87      0.88      0.86      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    offer-0       1.00      1.00      1.00      6524\n",
      "    offer-1       0.00      0.00      0.00        30\n",
      "\n",
      "avg / total       0.99      1.00      0.99      6554\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "aid_related-0       0.71      0.89      0.79      3900\n",
      "aid_related-1       0.75      0.46      0.57      2654\n",
      "\n",
      "  avg / total       0.73      0.72      0.70      6554\n",
      "\n",
      "                precision    recall  f1-score   support\n",
      "\n",
      "medical_help-0       0.93      1.00      0.96      6038\n",
      "medical_help-1       0.61      0.05      0.10       516\n",
      "\n",
      "   avg / total       0.90      0.92      0.89      6554\n",
      "\n",
      "                    precision    recall  f1-score   support\n",
      "\n",
      "medical_products-0       0.95      1.00      0.98      6233\n",
      "medical_products-1       0.66      0.08      0.15       321\n",
      "\n",
      "       avg / total       0.94      0.95      0.94      6554\n",
      "\n",
      "                     precision    recall  f1-score   support\n",
      "\n",
      "search_and_rescue-0       0.97      1.00      0.99      6380\n",
      "search_and_rescue-1       0.25      0.01      0.01       174\n",
      "\n",
      "        avg / total       0.95      0.97      0.96      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " security-0       0.98      1.00      0.99      6452\n",
      " security-1       0.00      0.00      0.00       102\n",
      "\n",
      "avg / total       0.97      0.98      0.98      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " military-0       0.97      1.00      0.99      6370\n",
      " military-1       0.50      0.02      0.04       184\n",
      "\n",
      "avg / total       0.96      0.97      0.96      6554\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "child_alone-0       1.00      1.00      1.00      6554\n",
      "child_alone-1       0.00      0.00      0.00         0\n",
      "\n",
      "  avg / total       1.00      1.00      1.00      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    water-0       0.95      1.00      0.97      6145\n",
      "    water-1       0.80      0.19      0.31       409\n",
      "\n",
      "avg / total       0.94      0.95      0.93      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     food-0       0.93      0.99      0.96      5843\n",
      "     food-1       0.83      0.41      0.55       711\n",
      "\n",
      "avg / total       0.92      0.93      0.92      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "  shelter-0       0.94      1.00      0.97      5999\n",
      "  shelter-1       0.89      0.29      0.44       555\n",
      "\n",
      "avg / total       0.93      0.94      0.92      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " clothing-0       0.99      1.00      0.99      6472\n",
      " clothing-1       0.83      0.12      0.21        82\n",
      "\n",
      "avg / total       0.99      0.99      0.98      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    money-0       0.98      1.00      0.99      6392\n",
      "    money-1       0.50      0.02      0.04       162\n",
      "\n",
      "avg / total       0.96      0.98      0.96      6554\n",
      "\n",
      "                  precision    recall  f1-score   support\n",
      "\n",
      "missing_people-0       0.99      1.00      1.00      6489\n",
      "missing_people-1       1.00      0.05      0.09        65\n",
      "\n",
      "     avg / total       0.99      0.99      0.99      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      " refugees-0       0.97      1.00      0.98      6359\n",
      " refugees-1       0.22      0.01      0.02       195\n",
      "\n",
      "avg / total       0.95      0.97      0.96      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    death-0       0.96      1.00      0.98      6247\n",
      "    death-1       0.85      0.15      0.25       307\n",
      "\n",
      "avg / total       0.95      0.96      0.94      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "other_aid-0       0.87      1.00      0.93      5685\n",
      "other_aid-1       0.53      0.03      0.06       869\n",
      "\n",
      "avg / total       0.83      0.87      0.81      6554\n",
      "\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "infrastructure_related-0       0.94      1.00      0.97      6127\n",
      "infrastructure_related-1       0.36      0.01      0.02       427\n",
      "\n",
      "             avg / total       0.90      0.93      0.90      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "transport-0       0.96      1.00      0.98      6264\n",
      "transport-1       0.71      0.06      0.11       290\n",
      "\n",
      "avg / total       0.95      0.96      0.94      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "buildings-0       0.95      1.00      0.97      6218\n",
      "buildings-1       0.68      0.07      0.12       336\n",
      "\n",
      "avg / total       0.94      0.95      0.93      6554\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "electricity-0       0.98      1.00      0.99      6421\n",
      "electricity-1       0.33      0.01      0.01       133\n",
      "\n",
      "  avg / total       0.97      0.98      0.97      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    tools-0       0.99      1.00      1.00      6514\n",
      "    tools-1       0.00      0.00      0.00        40\n",
      "\n",
      "avg / total       0.99      0.99      0.99      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "hospitals-0       0.99      1.00      0.99      6476\n",
      "hospitals-1       0.00      0.00      0.00        78\n",
      "\n",
      "avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    shops-0       1.00      1.00      1.00      6524\n",
      "    shops-1       0.00      0.00      0.00        30\n",
      "\n",
      "avg / total       0.99      1.00      0.99      6554\n",
      "\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "aid_centers-0       0.99      1.00      0.99      6473\n",
      "aid_centers-1       0.00      0.00      0.00        81\n",
      "\n",
      "  avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "other_infrastructure-0       0.96      1.00      0.98      6274\n",
      "other_infrastructure-1       0.00      0.00      0.00       280\n",
      "\n",
      "           avg / total       0.92      0.96      0.94      6554\n",
      "\n",
      "                   precision    recall  f1-score   support\n",
      "\n",
      "weather_related-0       0.82      0.97      0.89      4755\n",
      "weather_related-1       0.82      0.43      0.56      1799\n",
      "\n",
      "      avg / total       0.82      0.82      0.80      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "   floods-0       0.93      1.00      0.96      6020\n",
      "   floods-1       0.88      0.17      0.29       534\n",
      "\n",
      "avg / total       0.93      0.93      0.91      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "    storm-0       0.93      0.99      0.96      5939\n",
      "    storm-1       0.75      0.29      0.42       615\n",
      "\n",
      "avg / total       0.91      0.92      0.91      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     fire-0       0.99      1.00      0.99      6488\n",
      "     fire-1       0.00      0.00      0.00        66\n",
      "\n",
      "avg / total       0.98      0.99      0.98      6554\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "earthquake-0       0.96      0.99      0.98      5959\n",
      "earthquake-1       0.90      0.59      0.71       595\n",
      "\n",
      " avg / total       0.96      0.96      0.95      6554\n",
      "\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "     cold-0       0.98      1.00      0.99      6413\n",
      "     cold-1       0.40      0.01      0.03       141\n",
      "\n",
      "avg / total       0.97      0.98      0.97      6554\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "other_weather-0       0.95      1.00      0.98      6238\n",
      "other_weather-1       0.50      0.01      0.02       316\n",
      "\n",
      "    avg / total       0.93      0.95      0.93      6554\n",
      "\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "direct_report-0       0.86      0.98      0.92      5276\n",
      "direct_report-1       0.81      0.34      0.48      1278\n",
      "\n",
      "    avg / total       0.85      0.86      0.83      6554\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/metrics/classification.py:1137: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['initial_model.pkl']"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(cv, 'classifier.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
