{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already up-to-date: scikit-learn in /opt/conda/lib/python3.6/site-packages (0.23.1)\r\n",
      "Requirement already satisfied, skipping upgrade: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (2.1.0)\r\n",
      "Requirement already satisfied, skipping upgrade: joblib>=0.11 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (0.11)\r\n",
      "Requirement already satisfied, skipping upgrade: numpy>=1.13.3 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.18.5)\r\n",
      "Requirement already satisfied, skipping upgrade: scipy>=0.19.1 in /opt/conda/lib/python3.6/site-packages (from scikit-learn) (1.2.1)\r\n"
     ]
    }
   ],
   "source": [
    "# Upgrade sklearn to its latest version\n",
    "!pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.23.1\n"
     ]
    }
   ],
   "source": [
    "import sklearn\n",
    "print(sklearn.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "#Basic DS libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Helper Libs\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import joblib\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Vectorizers/Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#Models\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss, accuracy_score, precision_score, recall_score\n",
    "\n",
    "#Gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "#Glove\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.23.1'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "sklearn.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///DisasterResponse.db')\n",
    "messages_df = pd.read_sql_table(con=engine, table_name='Message')\n",
    "categories_df = pd.read_sql_table(con=engine, table_name='CorpusWide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create categories_list data with a list of the categories for each message\n",
    "categories = pd.read_csv('./disaster_categories.csv')\n",
    "categories['categories_list'] = categories['categories'].apply(lambda x: re.sub(r\"[_a-z]+-0[;]?\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\"-1\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\";$\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>I would like to receive the messages, thank you</td>\n",
       "      <td>Mwen ta renmen jouin messag yo. Merci</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>There's nothing to eat and water, we starving ...</td>\n",
       "      <td>Bon repo pa gen anyen menm grangou swaf</td>\n",
       "      <td>direct</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>I am in Petionville. I need more information r...</td>\n",
       "      <td>M nan pv mvle plis enfomasyon sou 4636</td>\n",
       "      <td>direct</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>I am in Thomassin number 32, in the area named...</td>\n",
       "      <td>Mwen thomassin 32 nan pyron mwen ta renmen jwe...</td>\n",
       "      <td>direct</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>Let's do it together, need food in Delma 75, i...</td>\n",
       "      <td>Ann fel ansanm bezwen manje nan delma 75 nan r...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>More information on the 4636 number in order f...</td>\n",
       "      <td>gen plis enfomasyon sou 4636 la pom w sim ap k...</td>\n",
       "      <td>direct</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>A Comitee in Delmas 19, Rue ( street ) Janvier...</td>\n",
       "      <td>Komite katye delma 19 rue janvier imp charite ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>We need food and water in Klecin 12. We are dy...</td>\n",
       "      <td>Nou bezwen mange avek dlo nan klcin 12 LA LAFI...</td>\n",
       "      <td>direct</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>are you going to call me or do you want me to ...</td>\n",
       "      <td>Eske se rele nap relem oubyen se mwen kap rele...</td>\n",
       "      <td>direct</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>I don't understand how to use this thing 4636.</td>\n",
       "      <td>Mwen pa konprann koman pou m itilize bagay 463...</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>I would like to know if the earthquake is over...</td>\n",
       "      <td>Mwen ta remen connin si trenblem de terre afin...</td>\n",
       "      <td>direct</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>I would like to know if one of the radio ginen...</td>\n",
       "      <td>Mwen ta renmen konnen eske ge jounalis radyo g...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>I'm in Laplaine, I am a victim</td>\n",
       "      <td>Mwen laplan mwen viktim kot yap bay d pou latr...</td>\n",
       "      <td>direct</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>There's a lack of water in Moleya, please info...</td>\n",
       "      <td>Nan moleya pa gen dlo svp f konnen sa pou mwen.</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>Those people who live at Sibert need food they...</td>\n",
       "      <td>nou menm nan sibert nou grangou nou bezwen mange</td>\n",
       "      <td>direct</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>I want to say hello, my message is to let you ...</td>\n",
       "      <td>Mwen salye nou dab mesaj mwen an se jis nou ge...</td>\n",
       "      <td>direct</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>direct</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>People I'm at Delma 2, we don't anything what ...</td>\n",
       "      <td>MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...</td>\n",
       "      <td>direct</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>We are at Gressier we needs assistance right a...</td>\n",
       "      <td>Se gressier nou an difikilte tanpri vin ede nou</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>How can we get water and food in Fontamara 43 ...</td>\n",
       "      <td>Comment nou ka jwen dlo ak manje nan font 43 c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>We need help. Carrefour has been forgotten com...</td>\n",
       "      <td>Nap di secours yo bliye kafou net sant m ap to...</td>\n",
       "      <td>direct</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>30233</td>\n",
       "      <td>The ability to pick dengue from influenza is c...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>30234</td>\n",
       "      <td>A Federation chartered ship arrived from Lae w...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>30235</td>\n",
       "      <td>The result is that in Aceh province many prefa...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26117</th>\n",
       "      <td>30236</td>\n",
       "      <td>Otherwise, the risk is families fleeing again ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>30237</td>\n",
       "      <td>A United Nations team from the Electoral Assis...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26119</th>\n",
       "      <td>30238</td>\n",
       "      <td>Senegal and Guinea-Bissau have agreed to condu...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26120</th>\n",
       "      <td>30239</td>\n",
       "      <td>The President said that her Government always ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>30240</td>\n",
       "      <td>It was decided that all vehicle movement from ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>30241</td>\n",
       "      <td>The tendency to link deforestation with large ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>30242</td>\n",
       "      <td>Polio is a viral disease that attacks the nerv...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>30243</td>\n",
       "      <td>The new constitution declares that 'Somalia is...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>30245</td>\n",
       "      <td>We're providing clean water to people who woul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>30246</td>\n",
       "      <td>Relief items include towels, sanitary napkins,...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26127</th>\n",
       "      <td>30247</td>\n",
       "      <td>In Aceh's Meulaboh town the UN refugee agency ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>30248</td>\n",
       "      <td>WHO is recruiting a sanitary engineer / consul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26129</th>\n",
       "      <td>30249</td>\n",
       "      <td>Following the severe floods which occurred ove...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26130</th>\n",
       "      <td>30250</td>\n",
       "      <td>The closure has stopped 169 inbound flights an...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26131</th>\n",
       "      <td>30251</td>\n",
       "      <td>BANGKOK, 24 January 2012 (NNT) - Prime Ministe...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>30253</td>\n",
       "      <td>Cadmium, a metallic element widely used in bat...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26133</th>\n",
       "      <td>30254</td>\n",
       "      <td>Epidemic surveillance: National Institute of C...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26134</th>\n",
       "      <td>30255</td>\n",
       "      <td>2.1 Due to sporadic skirmishes in eastern D.R....</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>30256</td>\n",
       "      <td>No other army had gone to greater lengths to a...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>30257</td>\n",
       "      <td>The delivery was made in conjunction with the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>30258</td>\n",
       "      <td>However while ECOWAS wanted him to lead a 12-m...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>30259</td>\n",
       "      <td>Hpakant, an area rich with coveted jade stones...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "5              14             Information about the National Palace-   \n",
       "6              15                     Storm at sacred heart of jesus   \n",
       "7              16  Please, we need tents and water. We are in Sil...   \n",
       "8              17    I would like to receive the messages, thank you   \n",
       "9              18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "10             20  There's nothing to eat and water, we starving ...   \n",
       "11             21  I am in Petionville. I need more information r...   \n",
       "12             22  I am in Thomassin number 32, in the area named...   \n",
       "13             24  Let's do it together, need food in Delma 75, i...   \n",
       "14             25  More information on the 4636 number in order f...   \n",
       "15             26  A Comitee in Delmas 19, Rue ( street ) Janvier...   \n",
       "16             27  We need food and water in Klecin 12. We are dy...   \n",
       "17             28  are you going to call me or do you want me to ...   \n",
       "18             30     I don't understand how to use this thing 4636.   \n",
       "19             31  I would like to know if the earthquake is over...   \n",
       "20             32  I would like to know if one of the radio ginen...   \n",
       "21             33                     I'm in Laplaine, I am a victim   \n",
       "22             34  There's a lack of water in Moleya, please info...   \n",
       "23             35  Those people who live at Sibert need food they...   \n",
       "24             36  I want to say hello, my message is to let you ...   \n",
       "25             37                 Can you tell me about this service   \n",
       "26             38  People I'm at Delma 2, we don't anything what ...   \n",
       "27             39  We are at Gressier we needs assistance right a...   \n",
       "28             41  How can we get water and food in Fontamara 43 ...   \n",
       "29             42  We need help. Carrefour has been forgotten com...   \n",
       "...           ...                                                ...   \n",
       "26114       30233  The ability to pick dengue from influenza is c...   \n",
       "26115       30234  A Federation chartered ship arrived from Lae w...   \n",
       "26116       30235  The result is that in Aceh province many prefa...   \n",
       "26117       30236  Otherwise, the risk is families fleeing again ...   \n",
       "26118       30237  A United Nations team from the Electoral Assis...   \n",
       "26119       30238  Senegal and Guinea-Bissau have agreed to condu...   \n",
       "26120       30239  The President said that her Government always ...   \n",
       "26121       30240  It was decided that all vehicle movement from ...   \n",
       "26122       30241  The tendency to link deforestation with large ...   \n",
       "26123       30242  Polio is a viral disease that attacks the nerv...   \n",
       "26124       30243  The new constitution declares that 'Somalia is...   \n",
       "26125       30245  We're providing clean water to people who woul...   \n",
       "26126       30246  Relief items include towels, sanitary napkins,...   \n",
       "26127       30247  In Aceh's Meulaboh town the UN refugee agency ...   \n",
       "26128       30248  WHO is recruiting a sanitary engineer / consul...   \n",
       "26129       30249  Following the severe floods which occurred ove...   \n",
       "26130       30250  The closure has stopped 169 inbound flights an...   \n",
       "26131       30251  BANGKOK, 24 January 2012 (NNT) - Prime Ministe...   \n",
       "26132       30253  Cadmium, a metallic element widely used in bat...   \n",
       "26133       30254  Epidemic surveillance: National Institute of C...   \n",
       "26134       30255  2.1 Due to sporadic skirmishes in eastern D.R....   \n",
       "26135       30256  No other army had gone to greater lengths to a...   \n",
       "26136       30257  The delivery was made in conjunction with the ...   \n",
       "26137       30258  However while ECOWAS wanted him to lead a 12-m...   \n",
       "26138       30259  Hpakant, an area rich with coveted jade stones...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13  \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9  \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6  \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13  \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12  \n",
       "5                    Informtion au nivaux palais nationl  direct          5  \n",
       "6                            Cyclone Coeur sacr de jesus  direct          6  \n",
       "7      Tanpri nou bezwen tant avek dlo nou zon silo m...  direct         12  \n",
       "8                  Mwen ta renmen jouin messag yo. Merci  direct          9  \n",
       "9      Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct         22  \n",
       "10               Bon repo pa gen anyen menm grangou swaf  direct         10  \n",
       "11                M nan pv mvle plis enfomasyon sou 4636  direct         10  \n",
       "12     Mwen thomassin 32 nan pyron mwen ta renmen jwe...  direct         29  \n",
       "13     Ann fel ansanm bezwen manje nan delma 75 nan r...  direct         12  \n",
       "14     gen plis enfomasyon sou 4636 la pom w sim ap k...  direct         21  \n",
       "15     Komite katye delma 19 rue janvier imp charite ...  direct         41  \n",
       "16     Nou bezwen mange avek dlo nan klcin 12 LA LAFI...  direct         26  \n",
       "17     Eske se rele nap relem oubyen se mwen kap rele...  direct         17  \n",
       "18     Mwen pa konprann koman pou m itilize bagay 463...  direct          9  \n",
       "19     Mwen ta remen connin si trenblem de terre afin...  direct         11  \n",
       "20     Mwen ta renmen konnen eske ge jounalis radyo g...  direct         13  \n",
       "21     Mwen laplan mwen viktim kot yap bay d pou latr...  direct          7  \n",
       "22       Nan moleya pa gen dlo svp f konnen sa pou mwen.  direct         12  \n",
       "23      nou menm nan sibert nou grangou nou bezwen mange  direct         11  \n",
       "24     Mwen salye nou dab mesaj mwen an se jis nou ge...  direct         28  \n",
       "25                    Can you tell me about this service  direct          7  \n",
       "26     MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...  direct         20  \n",
       "27       Se gressier nou an difikilte tanpri vin ede nou  direct         13  \n",
       "28     Comment nou ka jwen dlo ak manje nan font 43 c...  direct         12  \n",
       "29     Nap di secours yo bliye kafou net sant m ap to...  direct         19  \n",
       "...                                                  ...     ...        ...  \n",
       "26114                                               None    news         23  \n",
       "26115                                               None    news         16  \n",
       "26116                                               None    news         24  \n",
       "26117                                               None    news         13  \n",
       "26118                                               None    news         45  \n",
       "26119                                               None    news         29  \n",
       "26120                                               None    news         29  \n",
       "26121                                               None    news         36  \n",
       "26122                                               None    news         20  \n",
       "26123                                               None    news         18  \n",
       "26124                                               None    news         31  \n",
       "26125                                               None    news         69  \n",
       "26126                                               None    news         17  \n",
       "26127                                               None    news         33  \n",
       "26128                                               None    news         19  \n",
       "26129                                               None    news         40  \n",
       "26130                                               None    news         11  \n",
       "26131                                               None    news         32  \n",
       "26132                                               None    news         19  \n",
       "26133                                               None    news         55  \n",
       "26134                                               None    news         26  \n",
       "26135                                               None    news         18  \n",
       "26136                                               None    news         25  \n",
       "26137                                               None    news         35  \n",
       "26138                                               None    news         34  \n",
       "26139                                               None    news         21  \n",
       "26140                                               None    news         22  \n",
       "26141                                               None    news         23  \n",
       "26142                                               None    news         31  \n",
       "26143                                               None    news         36  \n",
       "\n",
       "[26144 rows x 5 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>I would like to receive the messages, thank you</td>\n",
       "      <td>Mwen ta renmen jouin messag yo. Merci</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>There's nothing to eat and water, we starving ...</td>\n",
       "      <td>Bon repo pa gen anyen menm grangou swaf</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>I am in Petionville. I need more information r...</td>\n",
       "      <td>M nan pv mvle plis enfomasyon sou 4636</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>I am in Thomassin number 32, in the area named...</td>\n",
       "      <td>Mwen thomassin 32 nan pyron mwen ta renmen jwe...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>Let's do it together, need food in Delma 75, i...</td>\n",
       "      <td>Ann fel ansanm bezwen manje nan delma 75 nan r...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>More information on the 4636 number in order f...</td>\n",
       "      <td>gen plis enfomasyon sou 4636 la pom w sim ap k...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>A Comitee in Delmas 19, Rue ( street ) Janvier...</td>\n",
       "      <td>Komite katye delma 19 rue janvier imp charite ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>We need food and water in Klecin 12. We are dy...</td>\n",
       "      <td>Nou bezwen mange avek dlo nan klcin 12 LA LAFI...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>are you going to call me or do you want me to ...</td>\n",
       "      <td>Eske se rele nap relem oubyen se mwen kap rele...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>I don't understand how to use this thing 4636.</td>\n",
       "      <td>Mwen pa konprann koman pou m itilize bagay 463...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>I would like to know if the earthquake is over...</td>\n",
       "      <td>Mwen ta remen connin si trenblem de terre afin...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>I would like to know if one of the radio ginen...</td>\n",
       "      <td>Mwen ta renmen konnen eske ge jounalis radyo g...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>I'm in Laplaine, I am a victim</td>\n",
       "      <td>Mwen laplan mwen viktim kot yap bay d pou latr...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>There's a lack of water in Moleya, please info...</td>\n",
       "      <td>Nan moleya pa gen dlo svp f konnen sa pou mwen.</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>Those people who live at Sibert need food they...</td>\n",
       "      <td>nou menm nan sibert nou grangou nou bezwen mange</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>I want to say hello, my message is to let you ...</td>\n",
       "      <td>Mwen salye nou dab mesaj mwen an se jis nou ge...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>People I'm at Delma 2, we don't anything what ...</td>\n",
       "      <td>MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>We are at Gressier we needs assistance right a...</td>\n",
       "      <td>Se gressier nou an difikilte tanpri vin ede nou</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>How can we get water and food in Fontamara 43 ...</td>\n",
       "      <td>Comment nou ka jwen dlo ak manje nan font 43 c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>We need help. Carrefour has been forgotten com...</td>\n",
       "      <td>Nap di secours yo bliye kafou net sant m ap to...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>30233</td>\n",
       "      <td>The ability to pick dengue from influenza is c...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>30234</td>\n",
       "      <td>A Federation chartered ship arrived from Lae w...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>30235</td>\n",
       "      <td>The result is that in Aceh province many prefa...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26117</th>\n",
       "      <td>30236</td>\n",
       "      <td>Otherwise, the risk is families fleeing again ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>30237</td>\n",
       "      <td>A United Nations team from the Electoral Assis...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26119</th>\n",
       "      <td>30238</td>\n",
       "      <td>Senegal and Guinea-Bissau have agreed to condu...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26120</th>\n",
       "      <td>30239</td>\n",
       "      <td>The President said that her Government always ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>30240</td>\n",
       "      <td>It was decided that all vehicle movement from ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>30241</td>\n",
       "      <td>The tendency to link deforestation with large ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>30242</td>\n",
       "      <td>Polio is a viral disease that attacks the nerv...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>30243</td>\n",
       "      <td>The new constitution declares that 'Somalia is...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>30245</td>\n",
       "      <td>We're providing clean water to people who woul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>30246</td>\n",
       "      <td>Relief items include towels, sanitary napkins,...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26127</th>\n",
       "      <td>30247</td>\n",
       "      <td>In Aceh's Meulaboh town the UN refugee agency ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>30248</td>\n",
       "      <td>WHO is recruiting a sanitary engineer / consul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26129</th>\n",
       "      <td>30249</td>\n",
       "      <td>Following the severe floods which occurred ove...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26130</th>\n",
       "      <td>30250</td>\n",
       "      <td>The closure has stopped 169 inbound flights an...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26131</th>\n",
       "      <td>30251</td>\n",
       "      <td>BANGKOK, 24 January 2012 (NNT) - Prime Ministe...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>30253</td>\n",
       "      <td>Cadmium, a metallic element widely used in bat...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26133</th>\n",
       "      <td>30254</td>\n",
       "      <td>Epidemic surveillance: National Institute of C...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26134</th>\n",
       "      <td>30255</td>\n",
       "      <td>2.1 Due to sporadic skirmishes in eastern D.R....</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>30256</td>\n",
       "      <td>No other army had gone to greater lengths to a...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>30257</td>\n",
       "      <td>The delivery was made in conjunction with the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>30258</td>\n",
       "      <td>However while ECOWAS wanted him to lead a 12-m...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>30259</td>\n",
       "      <td>Hpakant, an area rich with coveted jade stones...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "5              14             Information about the National Palace-   \n",
       "6              15                     Storm at sacred heart of jesus   \n",
       "7              16  Please, we need tents and water. We are in Sil...   \n",
       "8              17    I would like to receive the messages, thank you   \n",
       "9              18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "10             20  There's nothing to eat and water, we starving ...   \n",
       "11             21  I am in Petionville. I need more information r...   \n",
       "12             22  I am in Thomassin number 32, in the area named...   \n",
       "13             24  Let's do it together, need food in Delma 75, i...   \n",
       "14             25  More information on the 4636 number in order f...   \n",
       "15             26  A Comitee in Delmas 19, Rue ( street ) Janvier...   \n",
       "16             27  We need food and water in Klecin 12. We are dy...   \n",
       "17             28  are you going to call me or do you want me to ...   \n",
       "18             30     I don't understand how to use this thing 4636.   \n",
       "19             31  I would like to know if the earthquake is over...   \n",
       "20             32  I would like to know if one of the radio ginen...   \n",
       "21             33                     I'm in Laplaine, I am a victim   \n",
       "22             34  There's a lack of water in Moleya, please info...   \n",
       "23             35  Those people who live at Sibert need food they...   \n",
       "24             36  I want to say hello, my message is to let you ...   \n",
       "25             37                 Can you tell me about this service   \n",
       "26             38  People I'm at Delma 2, we don't anything what ...   \n",
       "27             39  We are at Gressier we needs assistance right a...   \n",
       "28             41  How can we get water and food in Fontamara 43 ...   \n",
       "29             42  We need help. Carrefour has been forgotten com...   \n",
       "...           ...                                                ...   \n",
       "26114       30233  The ability to pick dengue from influenza is c...   \n",
       "26115       30234  A Federation chartered ship arrived from Lae w...   \n",
       "26116       30235  The result is that in Aceh province many prefa...   \n",
       "26117       30236  Otherwise, the risk is families fleeing again ...   \n",
       "26118       30237  A United Nations team from the Electoral Assis...   \n",
       "26119       30238  Senegal and Guinea-Bissau have agreed to condu...   \n",
       "26120       30239  The President said that her Government always ...   \n",
       "26121       30240  It was decided that all vehicle movement from ...   \n",
       "26122       30241  The tendency to link deforestation with large ...   \n",
       "26123       30242  Polio is a viral disease that attacks the nerv...   \n",
       "26124       30243  The new constitution declares that 'Somalia is...   \n",
       "26125       30245  We're providing clean water to people who woul...   \n",
       "26126       30246  Relief items include towels, sanitary napkins,...   \n",
       "26127       30247  In Aceh's Meulaboh town the UN refugee agency ...   \n",
       "26128       30248  WHO is recruiting a sanitary engineer / consul...   \n",
       "26129       30249  Following the severe floods which occurred ove...   \n",
       "26130       30250  The closure has stopped 169 inbound flights an...   \n",
       "26131       30251  BANGKOK, 24 January 2012 (NNT) - Prime Ministe...   \n",
       "26132       30253  Cadmium, a metallic element widely used in bat...   \n",
       "26133       30254  Epidemic surveillance: National Institute of C...   \n",
       "26134       30255  2.1 Due to sporadic skirmishes in eastern D.R....   \n",
       "26135       30256  No other army had gone to greater lengths to a...   \n",
       "26136       30257  The delivery was made in conjunction with the ...   \n",
       "26137       30258  However while ECOWAS wanted him to lead a 12-m...   \n",
       "26138       30259  Hpakant, an area rich with coveted jade stones...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  related  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "5                    Informtion au nivaux palais nationl  direct        0   \n",
       "6                            Cyclone Coeur sacr de jesus  direct        1   \n",
       "7      Tanpri nou bezwen tant avek dlo nou zon silo m...  direct        1   \n",
       "8                  Mwen ta renmen jouin messag yo. Merci  direct        0   \n",
       "9      Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct        1   \n",
       "10               Bon repo pa gen anyen menm grangou swaf  direct        1   \n",
       "11                M nan pv mvle plis enfomasyon sou 4636  direct        0   \n",
       "12     Mwen thomassin 32 nan pyron mwen ta renmen jwe...  direct        1   \n",
       "13     Ann fel ansanm bezwen manje nan delma 75 nan r...  direct        1   \n",
       "14     gen plis enfomasyon sou 4636 la pom w sim ap k...  direct        1   \n",
       "15     Komite katye delma 19 rue janvier imp charite ...  direct        1   \n",
       "16     Nou bezwen mange avek dlo nan klcin 12 LA LAFI...  direct        1   \n",
       "17     Eske se rele nap relem oubyen se mwen kap rele...  direct        0   \n",
       "18     Mwen pa konprann koman pou m itilize bagay 463...  direct        0   \n",
       "19     Mwen ta remen connin si trenblem de terre afin...  direct        1   \n",
       "20     Mwen ta renmen konnen eske ge jounalis radyo g...  direct        1   \n",
       "21     Mwen laplan mwen viktim kot yap bay d pou latr...  direct        1   \n",
       "22       Nan moleya pa gen dlo svp f konnen sa pou mwen.  direct        1   \n",
       "23      nou menm nan sibert nou grangou nou bezwen mange  direct        1   \n",
       "24     Mwen salye nou dab mesaj mwen an se jis nou ge...  direct        1   \n",
       "25                    Can you tell me about this service  direct        0   \n",
       "26     MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...  direct        1   \n",
       "27       Se gressier nou an difikilte tanpri vin ede nou  direct        1   \n",
       "28     Comment nou ka jwen dlo ak manje nan font 43 c...  direct        1   \n",
       "29     Nap di secours yo bliye kafou net sant m ap to...  direct        1   \n",
       "...                                                  ...     ...      ...   \n",
       "26114                                               None    news        1   \n",
       "26115                                               None    news        1   \n",
       "26116                                               None    news        1   \n",
       "26117                                               None    news        0   \n",
       "26118                                               None    news        0   \n",
       "26119                                               None    news        1   \n",
       "26120                                               None    news        1   \n",
       "26121                                               None    news        1   \n",
       "26122                                               None    news        1   \n",
       "26123                                               None    news        0   \n",
       "26124                                               None    news        1   \n",
       "26125                                               None    news        1   \n",
       "26126                                               None    news        1   \n",
       "26127                                               None    news        1   \n",
       "26128                                               None    news        0   \n",
       "26129                                               None    news        1   \n",
       "26130                                               None    news        1   \n",
       "26131                                               None    news        1   \n",
       "26132                                               None    news        0   \n",
       "26133                                               None    news        1   \n",
       "26134                                               None    news        1   \n",
       "26135                                               None    news        1   \n",
       "26136                                               None    news        1   \n",
       "26137                                               None    news        0   \n",
       "26138                                               None    news        1   \n",
       "26139                                               None    news        0   \n",
       "26140                                               None    news        0   \n",
       "26141                                               None    news        1   \n",
       "26142                                               None    news        1   \n",
       "26143                                               None    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  \\\n",
       "0            0      0            0             0                 0   \n",
       "1            0      0            1             0                 0   \n",
       "2            0      0            0             0                 0   \n",
       "3            1      0            1             0                 1   \n",
       "4            0      0            0             0                 0   \n",
       "5            0      0            0             0                 0   \n",
       "6            0      0            0             0                 0   \n",
       "7            1      0            1             0                 0   \n",
       "8            0      0            0             0                 0   \n",
       "9            1      0            1             1                 1   \n",
       "10           1      0            1             1                 1   \n",
       "11           0      0            0             0                 0   \n",
       "12           1      0            1             0                 0   \n",
       "13           1      0            1             0                 0   \n",
       "14           0      0            0             0                 0   \n",
       "15           1      0            1             0                 1   \n",
       "16           1      0            1             1                 0   \n",
       "17           0      0            0             0                 0   \n",
       "18           0      0            0             0                 0   \n",
       "19           0      0            0             0                 0   \n",
       "20           0      0            0             0                 0   \n",
       "21           1      0            1             0                 0   \n",
       "22           1      0            1             0                 0   \n",
       "23           1      0            1             0                 0   \n",
       "24           0      0            0             0                 0   \n",
       "25           0      0            0             0                 0   \n",
       "26           1      0            1             1                 1   \n",
       "27           1      0            1             1                 0   \n",
       "28           1      0            1             0                 0   \n",
       "29           1      0            1             0                 0   \n",
       "...        ...    ...          ...           ...               ...   \n",
       "26114        0      0            1             1                 0   \n",
       "26115        0      0            1             0                 1   \n",
       "26116        0      0            1             0                 0   \n",
       "26117        0      0            0             0                 0   \n",
       "26118        0      0            0             0                 0   \n",
       "26119        0      0            1             0                 0   \n",
       "26120        0      0            0             0                 0   \n",
       "26121        0      0            1             0                 0   \n",
       "26122        0      0            0             0                 0   \n",
       "26123        0      0            0             0                 0   \n",
       "26124        0      0            0             0                 0   \n",
       "26125        0      0            1             0                 0   \n",
       "26126        0      0            1             0                 1   \n",
       "26127        0      0            1             0                 0   \n",
       "26128        0      0            0             0                 0   \n",
       "26129        0      0            0             0                 0   \n",
       "26130        0      0            0             0                 0   \n",
       "26131        0      0            1             0                 0   \n",
       "26132        0      0            0             0                 0   \n",
       "26133        0      0            1             1                 1   \n",
       "26134        0      0            1             0                 0   \n",
       "26135        1      0            0             0                 0   \n",
       "26136        0      0            0             0                 0   \n",
       "26137        0      0            0             0                 0   \n",
       "26138        0      0            0             0                 0   \n",
       "26139        0      0            0             0                 0   \n",
       "26140        0      0            0             0                 0   \n",
       "26141        0      0            0             0                 0   \n",
       "26142        0      0            1             0                 0   \n",
       "26143        0      0            0             0                 0   \n",
       "\n",
       "           ...        aid_centers  other_infrastructure  weather_related  \\\n",
       "0          ...                  0                     0                0   \n",
       "1          ...                  0                     0                1   \n",
       "2          ...                  0                     0                0   \n",
       "3          ...                  0                     0                0   \n",
       "4          ...                  0                     0                0   \n",
       "5          ...                  0                     0                0   \n",
       "6          ...                  0                     0                1   \n",
       "7          ...                  0                     0                0   \n",
       "8          ...                  0                     0                0   \n",
       "9          ...                  0                     0                0   \n",
       "10         ...                  0                     1                1   \n",
       "11         ...                  0                     0                0   \n",
       "12         ...                  0                     0                0   \n",
       "13         ...                  0                     0                0   \n",
       "14         ...                  0                     0                0   \n",
       "15         ...                  0                     0                0   \n",
       "16         ...                  0                     0                0   \n",
       "17         ...                  0                     0                0   \n",
       "18         ...                  0                     0                0   \n",
       "19         ...                  0                     0                1   \n",
       "20         ...                  0                     0                0   \n",
       "21         ...                  0                     0                0   \n",
       "22         ...                  0                     0                0   \n",
       "23         ...                  0                     0                0   \n",
       "24         ...                  0                     0                0   \n",
       "25         ...                  0                     0                0   \n",
       "26         ...                  0                     0                0   \n",
       "27         ...                  0                     0                1   \n",
       "28         ...                  0                     0                0   \n",
       "29         ...                  0                     0                0   \n",
       "...        ...                ...                   ...              ...   \n",
       "26114      ...                  0                     0                0   \n",
       "26115      ...                  0                     0                0   \n",
       "26116      ...                  0                     0                0   \n",
       "26117      ...                  0                     0                0   \n",
       "26118      ...                  0                     0                0   \n",
       "26119      ...                  0                     0                0   \n",
       "26120      ...                  0                     0                0   \n",
       "26121      ...                  0                     0                0   \n",
       "26122      ...                  0                     0                1   \n",
       "26123      ...                  0                     0                0   \n",
       "26124      ...                  0                     0                0   \n",
       "26125      ...                  0                     0                0   \n",
       "26126      ...                  0                     0                0   \n",
       "26127      ...                  0                     0                0   \n",
       "26128      ...                  0                     0                0   \n",
       "26129      ...                  0                     0                1   \n",
       "26130      ...                  0                     0                0   \n",
       "26131      ...                  0                     0                1   \n",
       "26132      ...                  0                     0                0   \n",
       "26133      ...                  0                     0                0   \n",
       "26134      ...                  0                     0                0   \n",
       "26135      ...                  0                     0                0   \n",
       "26136      ...                  0                     0                0   \n",
       "26137      ...                  0                     0                0   \n",
       "26138      ...                  0                     0                0   \n",
       "26139      ...                  0                     0                0   \n",
       "26140      ...                  0                     0                0   \n",
       "26141      ...                  0                     0                0   \n",
       "26142      ...                  0                     0                0   \n",
       "26143      ...                  0                     0                0   \n",
       "\n",
       "       floods  storm  fire  earthquake  cold  other_weather  direct_report  \n",
       "0           0      0     0           0     0              0              0  \n",
       "1           0      1     0           0     0              0              0  \n",
       "2           0      0     0           0     0              0              0  \n",
       "3           0      0     0           0     0              0              0  \n",
       "4           0      0     0           0     0              0              0  \n",
       "5           0      0     0           0     0              0              0  \n",
       "6           0      1     0           0     0              0              0  \n",
       "7           0      0     0           0     0              0              1  \n",
       "8           0      0     0           0     0              0              0  \n",
       "9           0      0     0           0     0              0              1  \n",
       "10          1      0     0           0     0              0              1  \n",
       "11          0      0     0           0     0              0              0  \n",
       "12          0      0     0           0     0              0              1  \n",
       "13          0      0     0           0     0              0              1  \n",
       "14          0      0     0           0     0              0              0  \n",
       "15          0      0     0           0     0              0              1  \n",
       "16          0      0     0           0     0              0              1  \n",
       "17          0      0     0           0     0              0              0  \n",
       "18          0      0     0           0     0              0              0  \n",
       "19          0      0     0           1     0              0              0  \n",
       "20          0      0     0           0     0              0              0  \n",
       "21          0      0     0           0     0              0              1  \n",
       "22          0      0     0           0     0              0              1  \n",
       "23          0      0     0           0     0              0              0  \n",
       "24          0      0     0           0     0              0              0  \n",
       "25          0      0     0           0     0              0              0  \n",
       "26          0      0     0           0     0              0              1  \n",
       "27          1      0     0           0     0              0              1  \n",
       "28          0      0     0           0     0              0              1  \n",
       "29          0      0     0           0     0              0              1  \n",
       "...       ...    ...   ...         ...   ...            ...            ...  \n",
       "26114       0      0     0           0     0              0              0  \n",
       "26115       0      0     0           0     0              0              0  \n",
       "26116       0      0     0           0     0              0              0  \n",
       "26117       0      0     0           0     0              0              0  \n",
       "26118       0      0     0           0     0              0              0  \n",
       "26119       0      0     0           0     0              0              0  \n",
       "26120       0      0     0           0     0              0              0  \n",
       "26121       0      0     0           0     0              0              0  \n",
       "26122       1      0     1           0     0              1              0  \n",
       "26123       0      0     0           0     0              0              0  \n",
       "26124       0      0     0           0     0              0              0  \n",
       "26125       0      0     0           0     0              0              1  \n",
       "26126       0      0     0           0     0              0              1  \n",
       "26127       0      0     0           0     0              0              0  \n",
       "26128       0      0     0           0     0              0              0  \n",
       "26129       1      0     0           0     0              0              0  \n",
       "26130       0      0     0           0     0              0              0  \n",
       "26131       1      0     0           0     0              0              0  \n",
       "26132       0      0     0           0     0              0              0  \n",
       "26133       0      0     0           0     0              0              0  \n",
       "26134       0      0     0           0     0              0              0  \n",
       "26135       0      0     0           0     0              0              1  \n",
       "26136       0      0     0           0     0              0              0  \n",
       "26137       0      0     0           0     0              0              0  \n",
       "26138       0      0     0           0     0              0              0  \n",
       "26139       0      0     0           0     0              0              0  \n",
       "26140       0      0     0           0     0              0              0  \n",
       "26141       0      0     0           0     0              0              0  \n",
       "26142       0      0     0           0     0              0              0  \n",
       "26143       0      0     0           0     0              0              0  \n",
       "\n",
       "[26144 rows x 39 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categories</th>\n",
       "      <th>categories_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;other_aid;weather_related;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_products;o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related;weather_related;storm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;water;shelter;dire...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_help;medic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_help;medic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;water;direct_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;food;direct_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_products;w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_help;water...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related;weather_related;earthquake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;other_aid;direct_r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;water;direct_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;food</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_help;medic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_help;other...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;water;food;direct_...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;other_aid;direct_r...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26218</th>\n",
       "      <td>30233</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;medical_help</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26219</th>\n",
       "      <td>30234</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;medical_products;other_aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26220</th>\n",
       "      <td>30235</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;shelter</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26221</th>\n",
       "      <td>30236</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26222</th>\n",
       "      <td>30237</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26223</th>\n",
       "      <td>30238</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26224</th>\n",
       "      <td>30239</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26225</th>\n",
       "      <td>30240</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;military;refugees;transport</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26226</th>\n",
       "      <td>30241</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related;weather_related;floods;fire;other_weather</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26227</th>\n",
       "      <td>30242</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26228</th>\n",
       "      <td>30243</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26229</th>\n",
       "      <td>30245</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;search_and_rescue;water;el...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26230</th>\n",
       "      <td>30246</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;medical_products;shelter;d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26231</th>\n",
       "      <td>30247</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;other_aid</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26232</th>\n",
       "      <td>30248</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26233</th>\n",
       "      <td>30249</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related;weather_related;floods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26234</th>\n",
       "      <td>30250</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26235</th>\n",
       "      <td>30251</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;money;other_aid;weather_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26236</th>\n",
       "      <td>30253</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26237</th>\n",
       "      <td>30254</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;medical_help;medical_produ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26238</th>\n",
       "      <td>30255</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;refugees</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26239</th>\n",
       "      <td>30256</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related;request;direct_report</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26240</th>\n",
       "      <td>30257</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26241</th>\n",
       "      <td>30258</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26242</th>\n",
       "      <td>30259</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26243</th>\n",
       "      <td>30261</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26244</th>\n",
       "      <td>30262</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>30263</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26246</th>\n",
       "      <td>30264</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>30265</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26248 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                         categories  \\\n",
       "0          2  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "1          7  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "2          8  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "3          9  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "4         12  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "5         14  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "6         15  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "7         16  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "8         17  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "9         18  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "10        20  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "11        21  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "12        22  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "13        24  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "14        25  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "15        26  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "16        27  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "17        28  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "18        30  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "19        31  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "20        32  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "21        33  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "22        34  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "23        35  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "24        36  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "25        37  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26        38  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "27        39  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "28        41  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "29        42  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "...      ...                                                ...   \n",
       "26218  30233  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26219  30234  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26220  30235  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26221  30236  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26222  30237  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26223  30238  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26224  30239  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26225  30240  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26226  30241  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26227  30242  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26228  30243  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26229  30245  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26230  30246  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26231  30247  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26232  30248  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26233  30249  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26234  30250  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26235  30251  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26236  30253  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26237  30254  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26238  30255  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26239  30256  related-1;request-1;offer-0;aid_related-0;medi...   \n",
       "26240  30257  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26241  30258  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26242  30259  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26243  30261  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26244  30262  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26245  30263  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26246  30264  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26247  30265  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "\n",
       "                                         categories_list  \n",
       "0                                                related  \n",
       "1      related;aid_related;other_aid;weather_related;...  \n",
       "2                                                related  \n",
       "3      related;request;aid_related;medical_products;o...  \n",
       "4                                                related  \n",
       "5                                                         \n",
       "6                          related;weather_related;storm  \n",
       "7      related;request;aid_related;water;shelter;dire...  \n",
       "8                                                         \n",
       "9      related;request;aid_related;medical_help;medic...  \n",
       "10     related;request;aid_related;medical_help;medic...  \n",
       "11                                                        \n",
       "12       related;request;aid_related;water;direct_report  \n",
       "13        related;request;aid_related;food;direct_report  \n",
       "14                                               related  \n",
       "15     related;request;aid_related;medical_products;w...  \n",
       "16     related;request;aid_related;medical_help;water...  \n",
       "17                                                        \n",
       "18                                                        \n",
       "19                    related;weather_related;earthquake  \n",
       "20                                               related  \n",
       "21     related;request;aid_related;other_aid;direct_r...  \n",
       "22       related;request;aid_related;water;direct_report  \n",
       "23                      related;request;aid_related;food  \n",
       "24                                               related  \n",
       "25                                                        \n",
       "26     related;request;aid_related;medical_help;medic...  \n",
       "27     related;request;aid_related;medical_help;other...  \n",
       "28     related;request;aid_related;water;food;direct_...  \n",
       "29     related;request;aid_related;other_aid;direct_r...  \n",
       "...                                                  ...  \n",
       "26218                   related;aid_related;medical_help  \n",
       "26219     related;aid_related;medical_products;other_aid  \n",
       "26220                        related;aid_related;shelter  \n",
       "26221                                                     \n",
       "26222                                                     \n",
       "26223                       related;aid_related;military  \n",
       "26224                                            related  \n",
       "26225    related;aid_related;military;refugees;transport  \n",
       "26226  related;weather_related;floods;fire;other_weather  \n",
       "26227                                                     \n",
       "26228                                            related  \n",
       "26229  related;aid_related;search_and_rescue;water;el...  \n",
       "26230  related;aid_related;medical_products;shelter;d...  \n",
       "26231                      related;aid_related;other_aid  \n",
       "26232                                                     \n",
       "26233                     related;weather_related;floods  \n",
       "26234                                            related  \n",
       "26235  related;aid_related;money;other_aid;weather_re...  \n",
       "26236                                                     \n",
       "26237  related;aid_related;medical_help;medical_produ...  \n",
       "26238                       related;aid_related;refugees  \n",
       "26239                      related;request;direct_report  \n",
       "26240                                            related  \n",
       "26241                                                     \n",
       "26242                                            related  \n",
       "26243                                                     \n",
       "26244                                                     \n",
       "26245                                            related  \n",
       "26246                       related;aid_related;military  \n",
       "26247                                            related  \n",
       "\n",
       "[26248 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Design and Apply Tokenization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Write down tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_list(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A list of tokens extracted from the input text\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_str(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A string with the tokens extracted from the input text concatenated by spaces\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "    #Return tokens list as a string joined by whitespaces\n",
    "    clean_tokens_str = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenize_to_list(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather update cold front cuba could pas haiti'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_to_str(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Generate tokenized messages table for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# messages_df['tokens_list'] = messages_df.message.apply(lambda x: tokenize_to_list(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# messages_df['tokens_str'] = messages_df.message.apply(lambda x: tokenize_to_str(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>I would like to receive the messages, thank you</td>\n",
       "      <td>Mwen ta renmen jouin messag yo. Merci</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>There's nothing to eat and water, we starving ...</td>\n",
       "      <td>Bon repo pa gen anyen menm grangou swaf</td>\n",
       "      <td>direct</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>I am in Petionville. I need more information r...</td>\n",
       "      <td>M nan pv mvle plis enfomasyon sou 4636</td>\n",
       "      <td>direct</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>I am in Thomassin number 32, in the area named...</td>\n",
       "      <td>Mwen thomassin 32 nan pyron mwen ta renmen jwe...</td>\n",
       "      <td>direct</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>Let's do it together, need food in Delma 75, i...</td>\n",
       "      <td>Ann fel ansanm bezwen manje nan delma 75 nan r...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>More information on the 4636 number in order f...</td>\n",
       "      <td>gen plis enfomasyon sou 4636 la pom w sim ap k...</td>\n",
       "      <td>direct</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>A Comitee in Delmas 19, Rue ( street ) Janvier...</td>\n",
       "      <td>Komite katye delma 19 rue janvier imp charite ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>We need food and water in Klecin 12. We are dy...</td>\n",
       "      <td>Nou bezwen mange avek dlo nan klcin 12 LA LAFI...</td>\n",
       "      <td>direct</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>are you going to call me or do you want me to ...</td>\n",
       "      <td>Eske se rele nap relem oubyen se mwen kap rele...</td>\n",
       "      <td>direct</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>I don't understand how to use this thing 4636.</td>\n",
       "      <td>Mwen pa konprann koman pou m itilize bagay 463...</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>I would like to know if the earthquake is over...</td>\n",
       "      <td>Mwen ta remen connin si trenblem de terre afin...</td>\n",
       "      <td>direct</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>I would like to know if one of the radio ginen...</td>\n",
       "      <td>Mwen ta renmen konnen eske ge jounalis radyo g...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>I'm in Laplaine, I am a victim</td>\n",
       "      <td>Mwen laplan mwen viktim kot yap bay d pou latr...</td>\n",
       "      <td>direct</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>There's a lack of water in Moleya, please info...</td>\n",
       "      <td>Nan moleya pa gen dlo svp f konnen sa pou mwen.</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>Those people who live at Sibert need food they...</td>\n",
       "      <td>nou menm nan sibert nou grangou nou bezwen mange</td>\n",
       "      <td>direct</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>I want to say hello, my message is to let you ...</td>\n",
       "      <td>Mwen salye nou dab mesaj mwen an se jis nou ge...</td>\n",
       "      <td>direct</td>\n",
       "      <td>28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>direct</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>People I'm at Delma 2, we don't anything what ...</td>\n",
       "      <td>MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...</td>\n",
       "      <td>direct</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>We are at Gressier we needs assistance right a...</td>\n",
       "      <td>Se gressier nou an difikilte tanpri vin ede nou</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>How can we get water and food in Fontamara 43 ...</td>\n",
       "      <td>Comment nou ka jwen dlo ak manje nan font 43 c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>We need help. Carrefour has been forgotten com...</td>\n",
       "      <td>Nap di secours yo bliye kafou net sant m ap to...</td>\n",
       "      <td>direct</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>30233</td>\n",
       "      <td>The ability to pick dengue from influenza is c...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>30234</td>\n",
       "      <td>A Federation chartered ship arrived from Lae w...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>30235</td>\n",
       "      <td>The result is that in Aceh province many prefa...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26117</th>\n",
       "      <td>30236</td>\n",
       "      <td>Otherwise, the risk is families fleeing again ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>30237</td>\n",
       "      <td>A United Nations team from the Electoral Assis...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>45</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26119</th>\n",
       "      <td>30238</td>\n",
       "      <td>Senegal and Guinea-Bissau have agreed to condu...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26120</th>\n",
       "      <td>30239</td>\n",
       "      <td>The President said that her Government always ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>30240</td>\n",
       "      <td>It was decided that all vehicle movement from ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>30241</td>\n",
       "      <td>The tendency to link deforestation with large ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>30242</td>\n",
       "      <td>Polio is a viral disease that attacks the nerv...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>30243</td>\n",
       "      <td>The new constitution declares that 'Somalia is...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>30245</td>\n",
       "      <td>We're providing clean water to people who woul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>30246</td>\n",
       "      <td>Relief items include towels, sanitary napkins,...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>17</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26127</th>\n",
       "      <td>30247</td>\n",
       "      <td>In Aceh's Meulaboh town the UN refugee agency ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>30248</td>\n",
       "      <td>WHO is recruiting a sanitary engineer / consul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26129</th>\n",
       "      <td>30249</td>\n",
       "      <td>Following the severe floods which occurred ove...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26130</th>\n",
       "      <td>30250</td>\n",
       "      <td>The closure has stopped 169 inbound flights an...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26131</th>\n",
       "      <td>30251</td>\n",
       "      <td>BANGKOK, 24 January 2012 (NNT) - Prime Ministe...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>30253</td>\n",
       "      <td>Cadmium, a metallic element widely used in bat...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26133</th>\n",
       "      <td>30254</td>\n",
       "      <td>Epidemic surveillance: National Institute of C...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26134</th>\n",
       "      <td>30255</td>\n",
       "      <td>2.1 Due to sporadic skirmishes in eastern D.R....</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>30256</td>\n",
       "      <td>No other army had gone to greater lengths to a...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>30257</td>\n",
       "      <td>The delivery was made in conjunction with the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>30258</td>\n",
       "      <td>However while ECOWAS wanted him to lead a 12-m...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>30259</td>\n",
       "      <td>Hpakant, an area rich with coveted jade stones...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>34</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "5              14             Information about the National Palace-   \n",
       "6              15                     Storm at sacred heart of jesus   \n",
       "7              16  Please, we need tents and water. We are in Sil...   \n",
       "8              17    I would like to receive the messages, thank you   \n",
       "9              18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "10             20  There's nothing to eat and water, we starving ...   \n",
       "11             21  I am in Petionville. I need more information r...   \n",
       "12             22  I am in Thomassin number 32, in the area named...   \n",
       "13             24  Let's do it together, need food in Delma 75, i...   \n",
       "14             25  More information on the 4636 number in order f...   \n",
       "15             26  A Comitee in Delmas 19, Rue ( street ) Janvier...   \n",
       "16             27  We need food and water in Klecin 12. We are dy...   \n",
       "17             28  are you going to call me or do you want me to ...   \n",
       "18             30     I don't understand how to use this thing 4636.   \n",
       "19             31  I would like to know if the earthquake is over...   \n",
       "20             32  I would like to know if one of the radio ginen...   \n",
       "21             33                     I'm in Laplaine, I am a victim   \n",
       "22             34  There's a lack of water in Moleya, please info...   \n",
       "23             35  Those people who live at Sibert need food they...   \n",
       "24             36  I want to say hello, my message is to let you ...   \n",
       "25             37                 Can you tell me about this service   \n",
       "26             38  People I'm at Delma 2, we don't anything what ...   \n",
       "27             39  We are at Gressier we needs assistance right a...   \n",
       "28             41  How can we get water and food in Fontamara 43 ...   \n",
       "29             42  We need help. Carrefour has been forgotten com...   \n",
       "...           ...                                                ...   \n",
       "26114       30233  The ability to pick dengue from influenza is c...   \n",
       "26115       30234  A Federation chartered ship arrived from Lae w...   \n",
       "26116       30235  The result is that in Aceh province many prefa...   \n",
       "26117       30236  Otherwise, the risk is families fleeing again ...   \n",
       "26118       30237  A United Nations team from the Electoral Assis...   \n",
       "26119       30238  Senegal and Guinea-Bissau have agreed to condu...   \n",
       "26120       30239  The President said that her Government always ...   \n",
       "26121       30240  It was decided that all vehicle movement from ...   \n",
       "26122       30241  The tendency to link deforestation with large ...   \n",
       "26123       30242  Polio is a viral disease that attacks the nerv...   \n",
       "26124       30243  The new constitution declares that 'Somalia is...   \n",
       "26125       30245  We're providing clean water to people who woul...   \n",
       "26126       30246  Relief items include towels, sanitary napkins,...   \n",
       "26127       30247  In Aceh's Meulaboh town the UN refugee agency ...   \n",
       "26128       30248  WHO is recruiting a sanitary engineer / consul...   \n",
       "26129       30249  Following the severe floods which occurred ove...   \n",
       "26130       30250  The closure has stopped 169 inbound flights an...   \n",
       "26131       30251  BANGKOK, 24 January 2012 (NNT) - Prime Ministe...   \n",
       "26132       30253  Cadmium, a metallic element widely used in bat...   \n",
       "26133       30254  Epidemic surveillance: National Institute of C...   \n",
       "26134       30255  2.1 Due to sporadic skirmishes in eastern D.R....   \n",
       "26135       30256  No other army had gone to greater lengths to a...   \n",
       "26136       30257  The delivery was made in conjunction with the ...   \n",
       "26137       30258  However while ECOWAS wanted him to lead a 12-m...   \n",
       "26138       30259  Hpakant, an area rich with coveted jade stones...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13  \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9  \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6  \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13  \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12  \n",
       "5                    Informtion au nivaux palais nationl  direct          5  \n",
       "6                            Cyclone Coeur sacr de jesus  direct          6  \n",
       "7      Tanpri nou bezwen tant avek dlo nou zon silo m...  direct         12  \n",
       "8                  Mwen ta renmen jouin messag yo. Merci  direct          9  \n",
       "9      Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct         22  \n",
       "10               Bon repo pa gen anyen menm grangou swaf  direct         10  \n",
       "11                M nan pv mvle plis enfomasyon sou 4636  direct         10  \n",
       "12     Mwen thomassin 32 nan pyron mwen ta renmen jwe...  direct         29  \n",
       "13     Ann fel ansanm bezwen manje nan delma 75 nan r...  direct         12  \n",
       "14     gen plis enfomasyon sou 4636 la pom w sim ap k...  direct         21  \n",
       "15     Komite katye delma 19 rue janvier imp charite ...  direct         41  \n",
       "16     Nou bezwen mange avek dlo nan klcin 12 LA LAFI...  direct         26  \n",
       "17     Eske se rele nap relem oubyen se mwen kap rele...  direct         17  \n",
       "18     Mwen pa konprann koman pou m itilize bagay 463...  direct          9  \n",
       "19     Mwen ta remen connin si trenblem de terre afin...  direct         11  \n",
       "20     Mwen ta renmen konnen eske ge jounalis radyo g...  direct         13  \n",
       "21     Mwen laplan mwen viktim kot yap bay d pou latr...  direct          7  \n",
       "22       Nan moleya pa gen dlo svp f konnen sa pou mwen.  direct         12  \n",
       "23      nou menm nan sibert nou grangou nou bezwen mange  direct         11  \n",
       "24     Mwen salye nou dab mesaj mwen an se jis nou ge...  direct         28  \n",
       "25                    Can you tell me about this service  direct          7  \n",
       "26     MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...  direct         20  \n",
       "27       Se gressier nou an difikilte tanpri vin ede nou  direct         13  \n",
       "28     Comment nou ka jwen dlo ak manje nan font 43 c...  direct         12  \n",
       "29     Nap di secours yo bliye kafou net sant m ap to...  direct         19  \n",
       "...                                                  ...     ...        ...  \n",
       "26114                                               None    news         23  \n",
       "26115                                               None    news         16  \n",
       "26116                                               None    news         24  \n",
       "26117                                               None    news         13  \n",
       "26118                                               None    news         45  \n",
       "26119                                               None    news         29  \n",
       "26120                                               None    news         29  \n",
       "26121                                               None    news         36  \n",
       "26122                                               None    news         20  \n",
       "26123                                               None    news         18  \n",
       "26124                                               None    news         31  \n",
       "26125                                               None    news         69  \n",
       "26126                                               None    news         17  \n",
       "26127                                               None    news         33  \n",
       "26128                                               None    news         19  \n",
       "26129                                               None    news         40  \n",
       "26130                                               None    news         11  \n",
       "26131                                               None    news         32  \n",
       "26132                                               None    news         19  \n",
       "26133                                               None    news         55  \n",
       "26134                                               None    news         26  \n",
       "26135                                               None    news         18  \n",
       "26136                                               None    news         25  \n",
       "26137                                               None    news         35  \n",
       "26138                                               None    news         34  \n",
       "26139                                               None    news         21  \n",
       "26140                                               None    news         22  \n",
       "26141                                               None    news         23  \n",
       "26142                                               None    news         31  \n",
       "26143                                               None    news         36  \n",
       "\n",
       "[26144 rows x 5 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_tokens = messages_df[['message_id','tokens_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_tokens.to_sql('MessageTokens', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories = messages_tokens.merge(categories_df, on='message_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 - Find n-grams and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ngrams_freqs(messages_array, n=1):\n",
    "#     vec = CountVectorizer(ngram_range=(n, n)).fit(messages_array)\n",
    "#     bag_of_words = vec.transform(messages_array)\n",
    "#     word_count = bag_of_words.sum(axis=0)\n",
    "#     words_freq = [(word, n, word_count[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "#     words_freq_df = pd.DataFrame(data = words_freq, columns = ['ngram','n','count'])\n",
    "#     return words_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str)\n",
    "# bigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=2)\n",
    "# trigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=3)\n",
    "# ngrams_freqs = pd.concat([unigrams_freqs, bigrams_freqs, trigrams_freqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs.to_sql('NGramsFreqs', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Prepare Data for Train/Test Pipeline\n",
    "- Extract X and Y from datasets\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens = pd.read_sql_table(con=engine, table_name='MessageTokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>tokens_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>weather update cold front cuba could pas haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>looking someone name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>un report leogane 80 90 destroyed hospital st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>say west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>information national palace</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>storm sacred heart jesus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>please need tent water silo thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>would like receive message thank</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>croix de bouquet health issue worker santo 15 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>nothing eat water starving thirsty</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>petionville need information regarding 4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>thomassin number 32 area named pyron would lik...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>let together need food delma 75 didine area</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>information 4636 number order participate see use</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>comitee delmas 19 rue street janvier impasse c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>need food water klecin 12 dying hunger impasse...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>going call want call ou let know</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>understand use thing 4636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>would like know earthquake thanks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>would like know one radio ginen journalist died</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>laplaine victim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>lack water moleya please informed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>people live sibert need food hungry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>want say hello message let know area faustin a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>tell service</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>people delma 2 anything ever please provide u ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>gressier need assistance right away asap come ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>get water food fontamara 43 cite tinante</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>need help carrefour forgotten completely foul ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>30233</td>\n",
       "      <td>ability pick dengue influenza crucial dengue p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>30234</td>\n",
       "      <td>federation chartered ship arrived lae relief s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>30235</td>\n",
       "      <td>result aceh province many prefabricated shelte...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26117</th>\n",
       "      <td>30236</td>\n",
       "      <td>otherwise risk family fleeing country never co...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>30237</td>\n",
       "      <td>united nation team electoral assistance divisi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26119</th>\n",
       "      <td>30238</td>\n",
       "      <td>senegal guinea bissau agreed conduct joint pat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26120</th>\n",
       "      <td>30239</td>\n",
       "      <td>president said government always belief democr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>30240</td>\n",
       "      <td>decided vehicle movement farchana abeche goz b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>30241</td>\n",
       "      <td>tendency link deforestation large flood accord...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>30242</td>\n",
       "      <td>polio viral disease attack nervous system lead...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>30243</td>\n",
       "      <td>new constitution declares somalia federal sove...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>30245</td>\n",
       "      <td>providing clean water people would otherwise f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>30246</td>\n",
       "      <td>relief item include towel sanitary napkin soap...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26127</th>\n",
       "      <td>30247</td>\n",
       "      <td>aceh meulaboh town un refugee agency said begu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>30248</td>\n",
       "      <td>recruiting sanitary engineer consultant 17 feb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26129</th>\n",
       "      <td>30249</td>\n",
       "      <td>following severe flood occurred previous year ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26130</th>\n",
       "      <td>30250</td>\n",
       "      <td>closure stopped 169 inbound flight 108 outboun...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26131</th>\n",
       "      <td>30251</td>\n",
       "      <td>bangkok 24 january 2012 nnt prime minister yin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>30253</td>\n",
       "      <td>cadmium metallic element widely used battery c...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26133</th>\n",
       "      <td>30254</td>\n",
       "      <td>epidemic surveillance national institute commu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26134</th>\n",
       "      <td>30255</td>\n",
       "      <td>2 1 due sporadic skirmish eastern r congo bomb...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>30256</td>\n",
       "      <td>army gone greater length avoid civilian causal...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>30257</td>\n",
       "      <td>delivery made conjunction wfp staffer rented t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>30258</td>\n",
       "      <td>however ecowas wanted lead 12 month transition...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>30259</td>\n",
       "      <td>hpakant area rich coveted jade stone seen rece...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>training demonstrated enhance micronutrient pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>suitable candidate selected ocha jakarta curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>proshika operating cox bazar municipality 5 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>2 000 woman protesting conduct election tearga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>radical shift thinking came result meeting rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                         tokens_str\n",
       "0               2     weather update cold front cuba could pas haiti\n",
       "1               7                                          hurricane\n",
       "2               8                               looking someone name\n",
       "3               9  un report leogane 80 90 destroyed hospital st ...\n",
       "4              12     say west side haiti rest country today tonight\n",
       "5              14                        information national palace\n",
       "6              15                           storm sacred heart jesus\n",
       "7              16                  please need tent water silo thank\n",
       "8              17                   would like receive message thank\n",
       "9              18  croix de bouquet health issue worker santo 15 ...\n",
       "10             20                 nothing eat water starving thirsty\n",
       "11             21        petionville need information regarding 4636\n",
       "12             22  thomassin number 32 area named pyron would lik...\n",
       "13             24        let together need food delma 75 didine area\n",
       "14             25  information 4636 number order participate see use\n",
       "15             26  comitee delmas 19 rue street janvier impasse c...\n",
       "16             27  need food water klecin 12 dying hunger impasse...\n",
       "17             28                   going call want call ou let know\n",
       "18             30                          understand use thing 4636\n",
       "19             31                  would like know earthquake thanks\n",
       "20             32    would like know one radio ginen journalist died\n",
       "21             33                                    laplaine victim\n",
       "22             34                  lack water moleya please informed\n",
       "23             35                people live sibert need food hungry\n",
       "24             36  want say hello message let know area faustin a...\n",
       "25             37                                       tell service\n",
       "26             38  people delma 2 anything ever please provide u ...\n",
       "27             39  gressier need assistance right away asap come ...\n",
       "28             41           get water food fontamara 43 cite tinante\n",
       "29             42  need help carrefour forgotten completely foul ...\n",
       "...           ...                                                ...\n",
       "26114       30233  ability pick dengue influenza crucial dengue p...\n",
       "26115       30234  federation chartered ship arrived lae relief s...\n",
       "26116       30235  result aceh province many prefabricated shelte...\n",
       "26117       30236  otherwise risk family fleeing country never co...\n",
       "26118       30237  united nation team electoral assistance divisi...\n",
       "26119       30238  senegal guinea bissau agreed conduct joint pat...\n",
       "26120       30239  president said government always belief democr...\n",
       "26121       30240  decided vehicle movement farchana abeche goz b...\n",
       "26122       30241  tendency link deforestation large flood accord...\n",
       "26123       30242  polio viral disease attack nervous system lead...\n",
       "26124       30243  new constitution declares somalia federal sove...\n",
       "26125       30245  providing clean water people would otherwise f...\n",
       "26126       30246  relief item include towel sanitary napkin soap...\n",
       "26127       30247  aceh meulaboh town un refugee agency said begu...\n",
       "26128       30248  recruiting sanitary engineer consultant 17 feb...\n",
       "26129       30249  following severe flood occurred previous year ...\n",
       "26130       30250  closure stopped 169 inbound flight 108 outboun...\n",
       "26131       30251  bangkok 24 january 2012 nnt prime minister yin...\n",
       "26132       30253  cadmium metallic element widely used battery c...\n",
       "26133       30254  epidemic surveillance national institute commu...\n",
       "26134       30255  2 1 due sporadic skirmish eastern r congo bomb...\n",
       "26135       30256  army gone greater length avoid civilian causal...\n",
       "26136       30257  delivery made conjunction wfp staffer rented t...\n",
       "26137       30258  however ecowas wanted lead 12 month transition...\n",
       "26138       30259  hpakant area rich coveted jade stone seen rece...\n",
       "26139       30261  training demonstrated enhance micronutrient pr...\n",
       "26140       30262  suitable candidate selected ocha jakarta curre...\n",
       "26141       30263  proshika operating cox bazar municipality 5 un...\n",
       "26142       30264  2 000 woman protesting conduct election tearga...\n",
       "26143       30265  radical shift thinking came result meeting rec...\n",
       "\n",
       "[26144 rows x 2 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for ML Pipeline\n",
    "X = messages_df.message.values\n",
    "X_tokenized = messages_tokens.tokens_str.values\n",
    "Y_df = categories_df.drop(['message_id', 'message', 'original', 'genre'], axis=1)\n",
    "Y = Y_df.values\n",
    "category_columns = Y_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related                   20031\n",
       "request                    4453\n",
       "offer                       118\n",
       "aid_related               10822\n",
       "medical_help               2078\n",
       "medical_products           1310\n",
       "search_and_rescue           723\n",
       "security                    471\n",
       "military                    857\n",
       "water                      1666\n",
       "food                       2911\n",
       "shelter                    2303\n",
       "clothing                    403\n",
       "money                       602\n",
       "missing_people              298\n",
       "refugees                    873\n",
       "death                      1190\n",
       "other_aid                  3434\n",
       "infrastructure_related     1701\n",
       "transport                  1196\n",
       "buildings                  1327\n",
       "electricity                 532\n",
       "tools                       159\n",
       "hospitals                   283\n",
       "shops                       120\n",
       "aid_centers                 309\n",
       "other_infrastructure       1147\n",
       "weather_related            7272\n",
       "floods                     2142\n",
       "storm                      2437\n",
       "fire                        282\n",
       "earthquake                 2449\n",
       "cold                        527\n",
       "other_weather              1373\n",
       "direct_report              5055\n",
       "dtype: int64"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_df.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>related</th>\n",
       "      <th>request</th>\n",
       "      <th>offer</th>\n",
       "      <th>aid_related</th>\n",
       "      <th>medical_help</th>\n",
       "      <th>medical_products</th>\n",
       "      <th>...</th>\n",
       "      <th>aid_centers</th>\n",
       "      <th>other_infrastructure</th>\n",
       "      <th>weather_related</th>\n",
       "      <th>floods</th>\n",
       "      <th>storm</th>\n",
       "      <th>fire</th>\n",
       "      <th>earthquake</th>\n",
       "      <th>cold</th>\n",
       "      <th>other_weather</th>\n",
       "      <th>direct_report</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>14</td>\n",
       "      <td>Information about the National Palace-</td>\n",
       "      <td>Informtion au nivaux palais nationl</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>15</td>\n",
       "      <td>Storm at sacred heart of jesus</td>\n",
       "      <td>Cyclone Coeur sacr de jesus</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>16</td>\n",
       "      <td>Please, we need tents and water. We are in Sil...</td>\n",
       "      <td>Tanpri nou bezwen tant avek dlo nou zon silo m...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>17</td>\n",
       "      <td>I would like to receive the messages, thank you</td>\n",
       "      <td>Mwen ta renmen jouin messag yo. Merci</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18</td>\n",
       "      <td>I am in Croix-des-Bouquets. We have health iss...</td>\n",
       "      <td>Nou kwadebouke, nou gen pwoblem sant m yo nan ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>20</td>\n",
       "      <td>There's nothing to eat and water, we starving ...</td>\n",
       "      <td>Bon repo pa gen anyen menm grangou swaf</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>21</td>\n",
       "      <td>I am in Petionville. I need more information r...</td>\n",
       "      <td>M nan pv mvle plis enfomasyon sou 4636</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>22</td>\n",
       "      <td>I am in Thomassin number 32, in the area named...</td>\n",
       "      <td>Mwen thomassin 32 nan pyron mwen ta renmen jwe...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>24</td>\n",
       "      <td>Let's do it together, need food in Delma 75, i...</td>\n",
       "      <td>Ann fel ansanm bezwen manje nan delma 75 nan r...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>25</td>\n",
       "      <td>More information on the 4636 number in order f...</td>\n",
       "      <td>gen plis enfomasyon sou 4636 la pom w sim ap k...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>26</td>\n",
       "      <td>A Comitee in Delmas 19, Rue ( street ) Janvier...</td>\n",
       "      <td>Komite katye delma 19 rue janvier imp charite ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>27</td>\n",
       "      <td>We need food and water in Klecin 12. We are dy...</td>\n",
       "      <td>Nou bezwen mange avek dlo nan klcin 12 LA LAFI...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>28</td>\n",
       "      <td>are you going to call me or do you want me to ...</td>\n",
       "      <td>Eske se rele nap relem oubyen se mwen kap rele...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>30</td>\n",
       "      <td>I don't understand how to use this thing 4636.</td>\n",
       "      <td>Mwen pa konprann koman pou m itilize bagay 463...</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>31</td>\n",
       "      <td>I would like to know if the earthquake is over...</td>\n",
       "      <td>Mwen ta remen connin si trenblem de terre afin...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>32</td>\n",
       "      <td>I would like to know if one of the radio ginen...</td>\n",
       "      <td>Mwen ta renmen konnen eske ge jounalis radyo g...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>33</td>\n",
       "      <td>I'm in Laplaine, I am a victim</td>\n",
       "      <td>Mwen laplan mwen viktim kot yap bay d pou latr...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>34</td>\n",
       "      <td>There's a lack of water in Moleya, please info...</td>\n",
       "      <td>Nan moleya pa gen dlo svp f konnen sa pou mwen.</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>35</td>\n",
       "      <td>Those people who live at Sibert need food they...</td>\n",
       "      <td>nou menm nan sibert nou grangou nou bezwen mange</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>36</td>\n",
       "      <td>I want to say hello, my message is to let you ...</td>\n",
       "      <td>Mwen salye nou dab mesaj mwen an se jis nou ge...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>37</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>Can you tell me about this service</td>\n",
       "      <td>direct</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>38</td>\n",
       "      <td>People I'm at Delma 2, we don't anything what ...</td>\n",
       "      <td>MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>39</td>\n",
       "      <td>We are at Gressier we needs assistance right a...</td>\n",
       "      <td>Se gressier nou an difikilte tanpri vin ede nou</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>41</td>\n",
       "      <td>How can we get water and food in Fontamara 43 ...</td>\n",
       "      <td>Comment nou ka jwen dlo ak manje nan font 43 c...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>42</td>\n",
       "      <td>We need help. Carrefour has been forgotten com...</td>\n",
       "      <td>Nap di secours yo bliye kafou net sant m ap to...</td>\n",
       "      <td>direct</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26114</th>\n",
       "      <td>30233</td>\n",
       "      <td>The ability to pick dengue from influenza is c...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26115</th>\n",
       "      <td>30234</td>\n",
       "      <td>A Federation chartered ship arrived from Lae w...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26116</th>\n",
       "      <td>30235</td>\n",
       "      <td>The result is that in Aceh province many prefa...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26117</th>\n",
       "      <td>30236</td>\n",
       "      <td>Otherwise, the risk is families fleeing again ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26118</th>\n",
       "      <td>30237</td>\n",
       "      <td>A United Nations team from the Electoral Assis...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26119</th>\n",
       "      <td>30238</td>\n",
       "      <td>Senegal and Guinea-Bissau have agreed to condu...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26120</th>\n",
       "      <td>30239</td>\n",
       "      <td>The President said that her Government always ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26121</th>\n",
       "      <td>30240</td>\n",
       "      <td>It was decided that all vehicle movement from ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26122</th>\n",
       "      <td>30241</td>\n",
       "      <td>The tendency to link deforestation with large ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26123</th>\n",
       "      <td>30242</td>\n",
       "      <td>Polio is a viral disease that attacks the nerv...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26124</th>\n",
       "      <td>30243</td>\n",
       "      <td>The new constitution declares that 'Somalia is...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26125</th>\n",
       "      <td>30245</td>\n",
       "      <td>We're providing clean water to people who woul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26126</th>\n",
       "      <td>30246</td>\n",
       "      <td>Relief items include towels, sanitary napkins,...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26127</th>\n",
       "      <td>30247</td>\n",
       "      <td>In Aceh's Meulaboh town the UN refugee agency ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26128</th>\n",
       "      <td>30248</td>\n",
       "      <td>WHO is recruiting a sanitary engineer / consul...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26129</th>\n",
       "      <td>30249</td>\n",
       "      <td>Following the severe floods which occurred ove...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26130</th>\n",
       "      <td>30250</td>\n",
       "      <td>The closure has stopped 169 inbound flights an...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26131</th>\n",
       "      <td>30251</td>\n",
       "      <td>BANGKOK, 24 January 2012 (NNT) - Prime Ministe...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26132</th>\n",
       "      <td>30253</td>\n",
       "      <td>Cadmium, a metallic element widely used in bat...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26133</th>\n",
       "      <td>30254</td>\n",
       "      <td>Epidemic surveillance: National Institute of C...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26134</th>\n",
       "      <td>30255</td>\n",
       "      <td>2.1 Due to sporadic skirmishes in eastern D.R....</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26135</th>\n",
       "      <td>30256</td>\n",
       "      <td>No other army had gone to greater lengths to a...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26136</th>\n",
       "      <td>30257</td>\n",
       "      <td>The delivery was made in conjunction with the ...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26137</th>\n",
       "      <td>30258</td>\n",
       "      <td>However while ECOWAS wanted him to lead a 12-m...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26138</th>\n",
       "      <td>30259</td>\n",
       "      <td>Hpakant, an area rich with coveted jade stones...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows Ã— 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "5              14             Information about the National Palace-   \n",
       "6              15                     Storm at sacred heart of jesus   \n",
       "7              16  Please, we need tents and water. We are in Sil...   \n",
       "8              17    I would like to receive the messages, thank you   \n",
       "9              18  I am in Croix-des-Bouquets. We have health iss...   \n",
       "10             20  There's nothing to eat and water, we starving ...   \n",
       "11             21  I am in Petionville. I need more information r...   \n",
       "12             22  I am in Thomassin number 32, in the area named...   \n",
       "13             24  Let's do it together, need food in Delma 75, i...   \n",
       "14             25  More information on the 4636 number in order f...   \n",
       "15             26  A Comitee in Delmas 19, Rue ( street ) Janvier...   \n",
       "16             27  We need food and water in Klecin 12. We are dy...   \n",
       "17             28  are you going to call me or do you want me to ...   \n",
       "18             30     I don't understand how to use this thing 4636.   \n",
       "19             31  I would like to know if the earthquake is over...   \n",
       "20             32  I would like to know if one of the radio ginen...   \n",
       "21             33                     I'm in Laplaine, I am a victim   \n",
       "22             34  There's a lack of water in Moleya, please info...   \n",
       "23             35  Those people who live at Sibert need food they...   \n",
       "24             36  I want to say hello, my message is to let you ...   \n",
       "25             37                 Can you tell me about this service   \n",
       "26             38  People I'm at Delma 2, we don't anything what ...   \n",
       "27             39  We are at Gressier we needs assistance right a...   \n",
       "28             41  How can we get water and food in Fontamara 43 ...   \n",
       "29             42  We need help. Carrefour has been forgotten com...   \n",
       "...           ...                                                ...   \n",
       "26114       30233  The ability to pick dengue from influenza is c...   \n",
       "26115       30234  A Federation chartered ship arrived from Lae w...   \n",
       "26116       30235  The result is that in Aceh province many prefa...   \n",
       "26117       30236  Otherwise, the risk is families fleeing again ...   \n",
       "26118       30237  A United Nations team from the Electoral Assis...   \n",
       "26119       30238  Senegal and Guinea-Bissau have agreed to condu...   \n",
       "26120       30239  The President said that her Government always ...   \n",
       "26121       30240  It was decided that all vehicle movement from ...   \n",
       "26122       30241  The tendency to link deforestation with large ...   \n",
       "26123       30242  Polio is a viral disease that attacks the nerv...   \n",
       "26124       30243  The new constitution declares that 'Somalia is...   \n",
       "26125       30245  We're providing clean water to people who woul...   \n",
       "26126       30246  Relief items include towels, sanitary napkins,...   \n",
       "26127       30247  In Aceh's Meulaboh town the UN refugee agency ...   \n",
       "26128       30248  WHO is recruiting a sanitary engineer / consul...   \n",
       "26129       30249  Following the severe floods which occurred ove...   \n",
       "26130       30250  The closure has stopped 169 inbound flights an...   \n",
       "26131       30251  BANGKOK, 24 January 2012 (NNT) - Prime Ministe...   \n",
       "26132       30253  Cadmium, a metallic element widely used in bat...   \n",
       "26133       30254  Epidemic surveillance: National Institute of C...   \n",
       "26134       30255  2.1 Due to sporadic skirmishes in eastern D.R....   \n",
       "26135       30256  No other army had gone to greater lengths to a...   \n",
       "26136       30257  The delivery was made in conjunction with the ...   \n",
       "26137       30258  However while ECOWAS wanted him to lead a 12-m...   \n",
       "26138       30259  Hpakant, an area rich with coveted jade stones...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  related  \\\n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct        1   \n",
       "1                     Cyclone nan fini osinon li pa fini  direct        1   \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct        1   \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct        1   \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct        1   \n",
       "5                    Informtion au nivaux palais nationl  direct        0   \n",
       "6                            Cyclone Coeur sacr de jesus  direct        1   \n",
       "7      Tanpri nou bezwen tant avek dlo nou zon silo m...  direct        1   \n",
       "8                  Mwen ta renmen jouin messag yo. Merci  direct        0   \n",
       "9      Nou kwadebouke, nou gen pwoblem sant m yo nan ...  direct        1   \n",
       "10               Bon repo pa gen anyen menm grangou swaf  direct        1   \n",
       "11                M nan pv mvle plis enfomasyon sou 4636  direct        0   \n",
       "12     Mwen thomassin 32 nan pyron mwen ta renmen jwe...  direct        1   \n",
       "13     Ann fel ansanm bezwen manje nan delma 75 nan r...  direct        1   \n",
       "14     gen plis enfomasyon sou 4636 la pom w sim ap k...  direct        1   \n",
       "15     Komite katye delma 19 rue janvier imp charite ...  direct        1   \n",
       "16     Nou bezwen mange avek dlo nan klcin 12 LA LAFI...  direct        1   \n",
       "17     Eske se rele nap relem oubyen se mwen kap rele...  direct        0   \n",
       "18     Mwen pa konprann koman pou m itilize bagay 463...  direct        0   \n",
       "19     Mwen ta remen connin si trenblem de terre afin...  direct        1   \n",
       "20     Mwen ta renmen konnen eske ge jounalis radyo g...  direct        1   \n",
       "21     Mwen laplan mwen viktim kot yap bay d pou latr...  direct        1   \n",
       "22       Nan moleya pa gen dlo svp f konnen sa pou mwen.  direct        1   \n",
       "23      nou menm nan sibert nou grangou nou bezwen mange  direct        1   \n",
       "24     Mwen salye nou dab mesaj mwen an se jis nou ge...  direct        1   \n",
       "25                    Can you tell me about this service  direct        0   \n",
       "26     MEZANMI MWEN MWEN NAN DELMA 2 NOU TR MAN MAL N...  direct        1   \n",
       "27       Se gressier nou an difikilte tanpri vin ede nou  direct        1   \n",
       "28     Comment nou ka jwen dlo ak manje nan font 43 c...  direct        1   \n",
       "29     Nap di secours yo bliye kafou net sant m ap to...  direct        1   \n",
       "...                                                  ...     ...      ...   \n",
       "26114                                               None    news        1   \n",
       "26115                                               None    news        1   \n",
       "26116                                               None    news        1   \n",
       "26117                                               None    news        0   \n",
       "26118                                               None    news        0   \n",
       "26119                                               None    news        1   \n",
       "26120                                               None    news        1   \n",
       "26121                                               None    news        1   \n",
       "26122                                               None    news        1   \n",
       "26123                                               None    news        0   \n",
       "26124                                               None    news        1   \n",
       "26125                                               None    news        1   \n",
       "26126                                               None    news        1   \n",
       "26127                                               None    news        1   \n",
       "26128                                               None    news        0   \n",
       "26129                                               None    news        1   \n",
       "26130                                               None    news        1   \n",
       "26131                                               None    news        1   \n",
       "26132                                               None    news        0   \n",
       "26133                                               None    news        1   \n",
       "26134                                               None    news        1   \n",
       "26135                                               None    news        1   \n",
       "26136                                               None    news        1   \n",
       "26137                                               None    news        0   \n",
       "26138                                               None    news        1   \n",
       "26139                                               None    news        0   \n",
       "26140                                               None    news        0   \n",
       "26141                                               None    news        1   \n",
       "26142                                               None    news        1   \n",
       "26143                                               None    news        1   \n",
       "\n",
       "       request  offer  aid_related  medical_help  medical_products  \\\n",
       "0            0      0            0             0                 0   \n",
       "1            0      0            1             0                 0   \n",
       "2            0      0            0             0                 0   \n",
       "3            1      0            1             0                 1   \n",
       "4            0      0            0             0                 0   \n",
       "5            0      0            0             0                 0   \n",
       "6            0      0            0             0                 0   \n",
       "7            1      0            1             0                 0   \n",
       "8            0      0            0             0                 0   \n",
       "9            1      0            1             1                 1   \n",
       "10           1      0            1             1                 1   \n",
       "11           0      0            0             0                 0   \n",
       "12           1      0            1             0                 0   \n",
       "13           1      0            1             0                 0   \n",
       "14           0      0            0             0                 0   \n",
       "15           1      0            1             0                 1   \n",
       "16           1      0            1             1                 0   \n",
       "17           0      0            0             0                 0   \n",
       "18           0      0            0             0                 0   \n",
       "19           0      0            0             0                 0   \n",
       "20           0      0            0             0                 0   \n",
       "21           1      0            1             0                 0   \n",
       "22           1      0            1             0                 0   \n",
       "23           1      0            1             0                 0   \n",
       "24           0      0            0             0                 0   \n",
       "25           0      0            0             0                 0   \n",
       "26           1      0            1             1                 1   \n",
       "27           1      0            1             1                 0   \n",
       "28           1      0            1             0                 0   \n",
       "29           1      0            1             0                 0   \n",
       "...        ...    ...          ...           ...               ...   \n",
       "26114        0      0            1             1                 0   \n",
       "26115        0      0            1             0                 1   \n",
       "26116        0      0            1             0                 0   \n",
       "26117        0      0            0             0                 0   \n",
       "26118        0      0            0             0                 0   \n",
       "26119        0      0            1             0                 0   \n",
       "26120        0      0            0             0                 0   \n",
       "26121        0      0            1             0                 0   \n",
       "26122        0      0            0             0                 0   \n",
       "26123        0      0            0             0                 0   \n",
       "26124        0      0            0             0                 0   \n",
       "26125        0      0            1             0                 0   \n",
       "26126        0      0            1             0                 1   \n",
       "26127        0      0            1             0                 0   \n",
       "26128        0      0            0             0                 0   \n",
       "26129        0      0            0             0                 0   \n",
       "26130        0      0            0             0                 0   \n",
       "26131        0      0            1             0                 0   \n",
       "26132        0      0            0             0                 0   \n",
       "26133        0      0            1             1                 1   \n",
       "26134        0      0            1             0                 0   \n",
       "26135        1      0            0             0                 0   \n",
       "26136        0      0            0             0                 0   \n",
       "26137        0      0            0             0                 0   \n",
       "26138        0      0            0             0                 0   \n",
       "26139        0      0            0             0                 0   \n",
       "26140        0      0            0             0                 0   \n",
       "26141        0      0            0             0                 0   \n",
       "26142        0      0            1             0                 0   \n",
       "26143        0      0            0             0                 0   \n",
       "\n",
       "           ...        aid_centers  other_infrastructure  weather_related  \\\n",
       "0          ...                  0                     0                0   \n",
       "1          ...                  0                     0                1   \n",
       "2          ...                  0                     0                0   \n",
       "3          ...                  0                     0                0   \n",
       "4          ...                  0                     0                0   \n",
       "5          ...                  0                     0                0   \n",
       "6          ...                  0                     0                1   \n",
       "7          ...                  0                     0                0   \n",
       "8          ...                  0                     0                0   \n",
       "9          ...                  0                     0                0   \n",
       "10         ...                  0                     1                1   \n",
       "11         ...                  0                     0                0   \n",
       "12         ...                  0                     0                0   \n",
       "13         ...                  0                     0                0   \n",
       "14         ...                  0                     0                0   \n",
       "15         ...                  0                     0                0   \n",
       "16         ...                  0                     0                0   \n",
       "17         ...                  0                     0                0   \n",
       "18         ...                  0                     0                0   \n",
       "19         ...                  0                     0                1   \n",
       "20         ...                  0                     0                0   \n",
       "21         ...                  0                     0                0   \n",
       "22         ...                  0                     0                0   \n",
       "23         ...                  0                     0                0   \n",
       "24         ...                  0                     0                0   \n",
       "25         ...                  0                     0                0   \n",
       "26         ...                  0                     0                0   \n",
       "27         ...                  0                     0                1   \n",
       "28         ...                  0                     0                0   \n",
       "29         ...                  0                     0                0   \n",
       "...        ...                ...                   ...              ...   \n",
       "26114      ...                  0                     0                0   \n",
       "26115      ...                  0                     0                0   \n",
       "26116      ...                  0                     0                0   \n",
       "26117      ...                  0                     0                0   \n",
       "26118      ...                  0                     0                0   \n",
       "26119      ...                  0                     0                0   \n",
       "26120      ...                  0                     0                0   \n",
       "26121      ...                  0                     0                0   \n",
       "26122      ...                  0                     0                1   \n",
       "26123      ...                  0                     0                0   \n",
       "26124      ...                  0                     0                0   \n",
       "26125      ...                  0                     0                0   \n",
       "26126      ...                  0                     0                0   \n",
       "26127      ...                  0                     0                0   \n",
       "26128      ...                  0                     0                0   \n",
       "26129      ...                  0                     0                1   \n",
       "26130      ...                  0                     0                0   \n",
       "26131      ...                  0                     0                1   \n",
       "26132      ...                  0                     0                0   \n",
       "26133      ...                  0                     0                0   \n",
       "26134      ...                  0                     0                0   \n",
       "26135      ...                  0                     0                0   \n",
       "26136      ...                  0                     0                0   \n",
       "26137      ...                  0                     0                0   \n",
       "26138      ...                  0                     0                0   \n",
       "26139      ...                  0                     0                0   \n",
       "26140      ...                  0                     0                0   \n",
       "26141      ...                  0                     0                0   \n",
       "26142      ...                  0                     0                0   \n",
       "26143      ...                  0                     0                0   \n",
       "\n",
       "       floods  storm  fire  earthquake  cold  other_weather  direct_report  \n",
       "0           0      0     0           0     0              0              0  \n",
       "1           0      1     0           0     0              0              0  \n",
       "2           0      0     0           0     0              0              0  \n",
       "3           0      0     0           0     0              0              0  \n",
       "4           0      0     0           0     0              0              0  \n",
       "5           0      0     0           0     0              0              0  \n",
       "6           0      1     0           0     0              0              0  \n",
       "7           0      0     0           0     0              0              1  \n",
       "8           0      0     0           0     0              0              0  \n",
       "9           0      0     0           0     0              0              1  \n",
       "10          1      0     0           0     0              0              1  \n",
       "11          0      0     0           0     0              0              0  \n",
       "12          0      0     0           0     0              0              1  \n",
       "13          0      0     0           0     0              0              1  \n",
       "14          0      0     0           0     0              0              0  \n",
       "15          0      0     0           0     0              0              1  \n",
       "16          0      0     0           0     0              0              1  \n",
       "17          0      0     0           0     0              0              0  \n",
       "18          0      0     0           0     0              0              0  \n",
       "19          0      0     0           1     0              0              0  \n",
       "20          0      0     0           0     0              0              0  \n",
       "21          0      0     0           0     0              0              1  \n",
       "22          0      0     0           0     0              0              1  \n",
       "23          0      0     0           0     0              0              0  \n",
       "24          0      0     0           0     0              0              0  \n",
       "25          0      0     0           0     0              0              0  \n",
       "26          0      0     0           0     0              0              1  \n",
       "27          1      0     0           0     0              0              1  \n",
       "28          0      0     0           0     0              0              1  \n",
       "29          0      0     0           0     0              0              1  \n",
       "...       ...    ...   ...         ...   ...            ...            ...  \n",
       "26114       0      0     0           0     0              0              0  \n",
       "26115       0      0     0           0     0              0              0  \n",
       "26116       0      0     0           0     0              0              0  \n",
       "26117       0      0     0           0     0              0              0  \n",
       "26118       0      0     0           0     0              0              0  \n",
       "26119       0      0     0           0     0              0              0  \n",
       "26120       0      0     0           0     0              0              0  \n",
       "26121       0      0     0           0     0              0              0  \n",
       "26122       1      0     1           0     0              1              0  \n",
       "26123       0      0     0           0     0              0              0  \n",
       "26124       0      0     0           0     0              0              0  \n",
       "26125       0      0     0           0     0              0              1  \n",
       "26126       0      0     0           0     0              0              1  \n",
       "26127       0      0     0           0     0              0              0  \n",
       "26128       0      0     0           0     0              0              0  \n",
       "26129       1      0     0           0     0              0              0  \n",
       "26130       0      0     0           0     0              0              0  \n",
       "26131       1      0     0           0     0              0              0  \n",
       "26132       0      0     0           0     0              0              0  \n",
       "26133       0      0     0           0     0              0              0  \n",
       "26134       0      0     0           0     0              0              0  \n",
       "26135       0      0     0           0     0              0              1  \n",
       "26136       0      0     0           0     0              0              0  \n",
       "26137       0      0     0           0     0              0              0  \n",
       "26138       0      0     0           0     0              0              0  \n",
       "26139       0      0     0           0     0              0              0  \n",
       "26140       0      0     0           0     0              0              0  \n",
       "26141       0      0     0           0     0              0              0  \n",
       "26142       0      0     0           0     0              0              0  \n",
       "26143       0      0     0           0     0              0              0  \n",
       "\n",
       "[26144 rows x 39 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Weather update - a cold front from Cuba that could pass over Haiti',\n",
       "       'Is the Hurricane over or is it not over',\n",
       "       'Looking for someone but no name', ...,\n",
       "       \"Proshika, operating in Cox's Bazar municipality and 5 other unions, Ramu and Chokoria, assessment, 5 kg rice, 1,5 kg lentils to 700 families.\",\n",
       "       'Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.',\n",
       "       'A radical shift in thinking came about as a result of this meeting, recognizing that HIV/AIDS is at the core of the humanitarian crisis and identifying the crisis itself as a function of the HIV/AIDS pandemic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weather update cold front cuba could pas haiti', 'hurricane',\n",
       "       'looking someone name', ...,\n",
       "       'proshika operating cox bazar municipality 5 union ramu chokoria assessment 5 kg rice 1 5 kg lentil 700 family',\n",
       "       '2 000 woman protesting conduct election teargassed tried converge local electoral commission office southern oil city port harcourt',\n",
       "       'radical shift thinking came result meeting recognizing hiv aid core humanitarian crisis identifying crisis function hiv aid pandemic'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'water', 'food', 'shelter', 'clothing', 'money', 'missing_people',\n",
       "       'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_tokenized, Y, test_size=0.25, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crude charm charismatic dt member thanks video santiago talk twice',\n",
       "       'tidal wave triggered earthquake indonesia sunday swept vast swath coastline including india indonesia malaysia maldives somalia sri lanka thailand',\n",
       "       'earthquake happened 4h34 evening night', ...,\n",
       "       'earthquake day haiti google possibly leaving china',\n",
       "       'matter debated previously non aligned movement foreign minister firmly stated right humanitarian intervention',\n",
       "       'dumbonyc oh rt endtwist coned power station dumbo flooding nopower sandy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19608,) (19608, 35)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([15027,  3310,    88,  8115,  1570,   982,   538,   354,   669,\n",
       "        1253,  2161,  1731,   293,   461,   224,   659,   898,  2558,\n",
       "        1276,   879,  1000,   410,   117,   223,    89,   240,   855,\n",
       "        5479,  1597,  1818,   214,  1875,   399,  1024,  3791])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.sum(axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 - Design Transformer to Tokenize sentences before passing on to other transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, output_str=True):\n",
    "        self.output_str = output_str\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([tokenize_to_str(tokens_str, lemmatizer) if self.output_str else \n",
    "                         tokenize_to_list(tokens_str, lemmatizer) for tokens_str in X])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# msgTokenizer = Tokenizer().fit(X, None)\n",
    "# msgTokenizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a Mean/TF-IDF word vector from a locally trained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Train a Word2Vec model using messages text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Message(object):\n",
    "    \"\"\"An interator that yields messages tokens (lists of str).\"\"\"\n",
    "    \n",
    "    def __init__(self, messages_df, sample_size=-1):\n",
    "        self.messages_df = messages_df\n",
    "        self.sample_size = sample_size\n",
    "\n",
    "    def __iter__(self):\n",
    "        messages = self.messages_df\n",
    "        \n",
    "        #Sample dataset if specified\n",
    "        if self.sample_size > 0:\n",
    "            messages = messages.sample(self.sample_size)\n",
    "        \n",
    "        #Yeld tokenized message joined by whitespaces\n",
    "        for token_str in messages.tokens_str:\n",
    "            yield token_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0           weather update cold front cuba could pas haiti\n",
       "1                                                hurricane\n",
       "2                                     looking someone name\n",
       "3        un report leogane 80 90 destroyed hospital st ...\n",
       "4           say west side haiti rest country today tonight\n",
       "5                              information national palace\n",
       "6                                 storm sacred heart jesus\n",
       "7                        please need tent water silo thank\n",
       "8                         would like receive message thank\n",
       "9        croix de bouquet health issue worker santo 15 ...\n",
       "10                      nothing eat water starving thirsty\n",
       "11             petionville need information regarding 4636\n",
       "12       thomassin number 32 area named pyron would lik...\n",
       "13             let together need food delma 75 didine area\n",
       "14       information 4636 number order participate see use\n",
       "15       comitee delmas 19 rue street janvier impasse c...\n",
       "16       need food water klecin 12 dying hunger impasse...\n",
       "17                        going call want call ou let know\n",
       "18                               understand use thing 4636\n",
       "19                       would like know earthquake thanks\n",
       "20         would like know one radio ginen journalist died\n",
       "21                                         laplaine victim\n",
       "22                       lack water moleya please informed\n",
       "23                     people live sibert need food hungry\n",
       "24       want say hello message let know area faustin a...\n",
       "25                                            tell service\n",
       "26       people delma 2 anything ever please provide u ...\n",
       "27       gressier need assistance right away asap come ...\n",
       "28                get water food fontamara 43 cite tinante\n",
       "29       need help carrefour forgotten completely foul ...\n",
       "                               ...                        \n",
       "26114    ability pick dengue influenza crucial dengue p...\n",
       "26115    federation chartered ship arrived lae relief s...\n",
       "26116    result aceh province many prefabricated shelte...\n",
       "26117    otherwise risk family fleeing country never co...\n",
       "26118    united nation team electoral assistance divisi...\n",
       "26119    senegal guinea bissau agreed conduct joint pat...\n",
       "26120    president said government always belief democr...\n",
       "26121    decided vehicle movement farchana abeche goz b...\n",
       "26122    tendency link deforestation large flood accord...\n",
       "26123    polio viral disease attack nervous system lead...\n",
       "26124    new constitution declares somalia federal sove...\n",
       "26125    providing clean water people would otherwise f...\n",
       "26126    relief item include towel sanitary napkin soap...\n",
       "26127    aceh meulaboh town un refugee agency said begu...\n",
       "26128    recruiting sanitary engineer consultant 17 feb...\n",
       "26129    following severe flood occurred previous year ...\n",
       "26130    closure stopped 169 inbound flight 108 outboun...\n",
       "26131    bangkok 24 january 2012 nnt prime minister yin...\n",
       "26132    cadmium metallic element widely used battery c...\n",
       "26133    epidemic surveillance national institute commu...\n",
       "26134    2 1 due sporadic skirmish eastern r congo bomb...\n",
       "26135    army gone greater length avoid civilian causal...\n",
       "26136    delivery made conjunction wfp staffer rented t...\n",
       "26137    however ecowas wanted lead 12 month transition...\n",
       "26138    hpakant area rich coveted jade stone seen rece...\n",
       "26139    training demonstrated enhance micronutrient pr...\n",
       "26140    suitable candidate selected ocha jakarta curre...\n",
       "26141    proshika operating cox bazar municipality 5 un...\n",
       "26142    2 000 woman protesting conduct election tearga...\n",
       "26143    radical shift thinking came result meeting rec...\n",
       "Name: tokens_str, Length: 26144, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_tokens.tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 7.35 s, sys: 96.3 ms, total: 7.45 s\n",
      "Wall time: 7.54 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "sentences = Message(messages_df=messages_tokens)\n",
    "local_w2v_model_full = gensim.models.Word2Vec(sentences=sentences)\n",
    "local_w2v_model_full.save(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model_full = gensim.models.Word2Vec.load(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weather\n",
      "update\n",
      "cold\n",
      "front\n",
      "cuba\n",
      "could\n",
      "pas\n",
      "haiti\n",
      "hurricane\n",
      "looking\n"
     ]
    }
   ],
   "source": [
    "# look up generated word vocab\n",
    "for i, word in enumerate(w2v_model_full.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.19078784, -0.18491617,  0.03814096,  0.3418014 , -0.9759837 ,\n",
       "       -1.4897901 , -0.43548572, -0.20545296,  1.2964498 ,  0.52204883,\n",
       "        0.06461112,  0.13435331, -0.4328741 , -0.9239483 , -0.22408496,\n",
       "       -0.5571155 ,  0.43278468,  0.55698615, -0.1026466 , -0.49350274,\n",
       "       -0.24672078,  0.888357  ,  0.73535264, -0.44993743,  1.2125608 ,\n",
       "       -0.8714119 ,  0.3708165 ,  0.04596617, -0.19340983,  0.09381265,\n",
       "        0.07652403,  0.19241536,  0.47503707,  0.0617723 , -0.6348238 ,\n",
       "       -0.02342288,  0.6671087 ,  0.07306863,  0.9675678 ,  0.8765735 ,\n",
       "       -1.0461487 ,  0.47872558, -0.3998758 , -1.3513135 ,  0.24322149,\n",
       "       -0.13318177, -0.12466659,  0.15902896, -0.9197659 ,  0.4422364 ,\n",
       "       -0.74258924,  1.3287657 , -0.58001095,  0.53123295, -0.72101235,\n",
       "       -0.5474531 , -0.5145969 ,  0.56529915, -0.33326197, -0.00676469,\n",
       "        0.54244834, -0.5966822 , -0.98309493,  0.21653189,  0.8753762 ,\n",
       "        0.0968083 , -1.0476868 ,  0.5039579 ,  0.47279334,  0.3710782 ,\n",
       "       -1.021042  , -0.21056084, -1.1906172 ,  0.1946597 ,  0.33586565,\n",
       "       -0.8271495 ,  0.2684122 ,  0.2300787 , -0.07543404,  0.12288249,\n",
       "        0.33049172,  0.06688719,  0.24332708, -0.04084858,  0.24667868,\n",
       "       -0.7469941 , -0.06070077,  0.08819111,  0.9173733 ,  0.7032793 ,\n",
       "        0.35485289, -0.62935156, -1.3749442 ,  0.46498075, -0.9388541 ,\n",
       "        0.40542057, -0.9682613 , -0.00914589, -0.7731063 , -1.1555648 ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# look up generated word vectors\n",
    "w2v_model_full.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Implement Vector Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial implementation of aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(BaseEstimator):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing defaultdict functionality\n",
    "# test_dict = defaultdict(lambda: 1, [(\"Hello\" , 7), (\"hi\" , 10), (\"there\" , 45),(\"at\" , 23),(\"this\" , 77)])\n",
    "# test_dict['Hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(BaseEstimator):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "        self.word_weights = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# w2v_mean_pip = Pipeline([\n",
    "#     ('features',FeatureUnion([\n",
    "#         ('w2v_mean', MeanEmbeddingVectorizer(w2v_model_full)),\n",
    "#         ('w2v_tfidf', TfidfEmbeddingVectorizer(w2v_model_full))\n",
    "#     ])),    \n",
    "#     ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "# ])\n",
    "\n",
    "# w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "# for category_idx in range(y_pred.shape[1]):\n",
    "#     print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_idx=1\n",
    "# c_report = classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], \n",
    "#                       labels=[0,1], \n",
    "#                       target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1'],\n",
    "#                       output_dict=True)\n",
    "# c_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c_report.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_mean_pip.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum(y_pred[0] == y_test[0])/y_pred[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_test = y_test[0]\n",
    "sample_pred = y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_test == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_pred == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sample_test == 1) == (sample_pred == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing TPR manually\n",
    "tp_array = (sample_test == 1) & (sample_pred == 1)\n",
    "tp = tp_array.sum()\n",
    "tpr = tp/sample_test.shape[0]\n",
    "print(tp)\n",
    "print(tpr)\n",
    "tp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing FPR manually\n",
    "fp_array = (sample_test == 0) & (sample_pred == 1)\n",
    "fp = fp_array.sum()\n",
    "fpr = fp/sample_test.shape[0]\n",
    "print(fp)\n",
    "print(fpr)\n",
    "fp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing FNR manually\n",
    "fn_array = (sample_test == 1) & (sample_pred == 0)\n",
    "fn = fn_array.sum()\n",
    "fnr = fn/sample_test.shape[0]\n",
    "print(fn)\n",
    "print(fnr)\n",
    "fn_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing Precision manually\n",
    "print(tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing Recall manually\n",
    "print(tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example_test = np.append(np.repeat(1, 4),np.repeat(0,32))\n",
    "example_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example_pred = np.repeat(0, 36)\n",
    "example_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(example_test,example_pred))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(example_test,example_pred))\n",
    "print(\"Jaccard Score = \", jaccard_score(example_test,example_pred))\n",
    "print(\"Accuracy = \", accuracy_score(example_test,example_pred))\n",
    "print(\"Recall = \", recall_score(example_test,example_pred))\n",
    "print(\"Precision = \", precision_score(example_test,example_pred))\n",
    "print(\"F1-Score = \", f1_score(example_test,example_pred))\n",
    "print(\"Jaccard Score Micro = \", jaccard_score(example_test,example_pred, average='micro'))\n",
    "print(\"Recall Micro = \", recall_score(example_test,example_pred, average='micro'))\n",
    "print(\"Precision Micro = \", precision_score(example_test,example_pred, average='micro'))\n",
    "print(\"F1-Score Micro = \", f1_score(example_test,example_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "print(mean_absolute_error(y_test, y_pred, multioutput='raw_values'))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "(y_test == y_pred).mean()\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred, multioutput='variance_weighted'))\n",
    "print(explained_variance_score(y_test, y_pred))\n",
    "print(explained_variance_score(y_test, y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Create Estimator to both train and average w2v on corpus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying W2VTransformer - Gensim's sklearn Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "\n",
    "model.fit(common_texts)\n",
    "          \n",
    "model.transform(['graph', 'system'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages_df.head(5).tokens_str.str.split().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages_df.head(5).tokens_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v_model = W2VTransformer()\n",
    "w2v_model.fit(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = messages_df.tokens_str[0].split()[1]\n",
    "print(word)\n",
    "w2v_model.transform(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model = gensim.models.Word2Vec(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, word in enumerate(test_model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implement aggregators now with training capability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# from sklearn.naive_bayes import GaussianNB\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# w2v_mean_pip = Pipeline([\n",
    "#     ('features',FeatureUnion([\n",
    "#         ('w2v_mean', MeanEmbeddingTrainVectorizer(word2vec_model=w2v_model_full)),\n",
    "#         ('w2v_mean_train', MeanEmbeddingTrainVectorizer(num_dims=50)),\n",
    "#         ('w2v_tfidf', TfidfEmbeddingTrainVectorizer(word2vec_model=w2v_model_full)),\n",
    "#         ('w2v_tfidf_train', TfidfEmbeddingTrainVectorizer(num_dims=100)),\n",
    "#     ])),\n",
    "#     ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "# ])\n",
    "\n",
    "# w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "# for category_idx in range(y_pred.shape[1]):\n",
    "#     print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Glove pre-trained model to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove pre-trained vectors can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Code which can be used to convert from Glove default format to Gensim W2V format\n",
    "\n",
    "glove_file = 'glove-pretrained/glove.6B.50d.txt'\n",
    "tmp_file = 'glove-pretrained/glove.6B.50d_word2vec.txt'\n",
    "\n",
    "#_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.1 s, sys: 450 ms, total: 28.5 s\n",
      "Wall time: 29.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_50d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.50d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 52.3 s, sys: 675 ms, total: 52.9 s\n",
      "Wall time: 53.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "glove_100d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.100d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 28s, sys: 1.88 s, total: 2min 30s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "glove_300d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.300d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a Pipeline to train the Word2Vec estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Aggregators with Glove using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('features',FeatureUnion([\n",
    "        ('glove_mean', MeanEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ])),    \n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "# precision = dict()\n",
    "# recall = dict()\n",
    "# average_precision = dict()\n",
    "# for i in range(len(category_columns)):\n",
    "#     precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    #average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "# precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "#     y_score.ravel())\n",
    "# average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "#                                                      average=\"micro\")\n",
    "# print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "#       .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import hamming_loss, zero_one_loss, jaccard_score, accuracy_score, auc, recall_score, precision_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0] == y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6536*36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.argmax(y_pred, axis=1).shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "train_foo = [['sci-fi', 'thriller'],['comedy'],['sci-fi', 'thriller'],['comedy']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_label_train = mlb.fit_transform(train_foo)\n",
    "mlb_label_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_df[Y_df['related'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use Doc2Vec to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the number of words in the corpus\n",
    "messages_df['message'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Doc2Vec step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def read_messages(messages, tokens_only=False):\n",
    "    for index, tokens_str in np.ndenumerate(messages):\n",
    "        tokens = tokens_str.split()\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            tags = index\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, tags)\n",
    "\n",
    "train_corpus = list(read_messages(X_train))\n",
    "test_corpus = list(read_messages(X_test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2v_model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Doc2Vec Estimator/Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, epochs=20):\n",
    "        self.epochs = epochs\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.d2v_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tagged_x = [TaggedDocument(tokens_str.split(), [index]) for index, tokens_str in np.ndenumerate(X)]\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, workers=self.workers, epochs=self.epochs)\n",
    "        self.d2v_model.build_vocab(tagged_x)\n",
    "        self.d2v_model.train(tagged_x, total_examples=self.d2v_model.corpus_count, \n",
    "                             epochs=self.epochs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.asmatrix(np.array([self.d2v_model.infer_vector(tokens_str.split())\n",
    "                                     for tokens_str in X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "doc2vec_trf = Doc2VecTransformer(epochs=1)\n",
    "doc2vec_trf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inferred_vector = doc2vec_trf.d2v_model.infer_vector(X_test[0].split())\n",
    "print(inferred_vector.shape)\n",
    "print(type(inferred_vector))\n",
    "print(inferred_vector)\n",
    "\n",
    "inferred_vector2 = np.asmatrix(np.array([doc2vec_trf.d2v_model.infer_vector(X_test[0].split())]))\n",
    "print(inferred_vector2.shape)\n",
    "print(type(inferred_vector2))\n",
    "print(inferred_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec_features = doc2vec_trf.transform(X_test)\n",
    "print(doc2vec_features.shape)\n",
    "doc2vec_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Doc2Vec using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Extra Feature - Distance Between Message Word Vector to Categories Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove as our space vectorizer as it is trained on a broad corpus and thus able to better describe texts / words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the embeddings for the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'water', 'food', 'shelter', 'clothing', 'money', 'missing_people',\n",
       "       'refugees', 'death', 'other_aid', 'infrastructure_related', 'transport',\n",
       "       'buildings', 'electricity', 'tools', 'hospitals', 'shops',\n",
       "       'aid_centers', 'other_infrastructure', 'weather_related', 'floods',\n",
       "       'storm', 'fire', 'earthquake', 'cold', 'other_weather',\n",
       "       'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_tokens = np.array([np.array(cat.split('_')) for cat in category_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['related'], dtype='<U7'), array(['request'], dtype='<U7'),\n",
       "       array(['offer'], dtype='<U5'),\n",
       "       array(['aid', 'related'], dtype='<U7'),\n",
       "       array(['medical', 'help'], dtype='<U7'),\n",
       "       array(['medical', 'products'], dtype='<U8'),\n",
       "       array(['search', 'and', 'rescue'], dtype='<U6'),\n",
       "       array(['security'], dtype='<U8'), array(['military'], dtype='<U8'),\n",
       "       array(['water'], dtype='<U5'), array(['food'], dtype='<U4'),\n",
       "       array(['shelter'], dtype='<U7'), array(['clothing'], dtype='<U8'),\n",
       "       array(['money'], dtype='<U5'),\n",
       "       array(['missing', 'people'], dtype='<U7'),\n",
       "       array(['refugees'], dtype='<U8'), array(['death'], dtype='<U5'),\n",
       "       array(['other', 'aid'], dtype='<U5'),\n",
       "       array(['infrastructure', 'related'], dtype='<U14'),\n",
       "       array(['transport'], dtype='<U9'),\n",
       "       array(['buildings'], dtype='<U9'),\n",
       "       array(['electricity'], dtype='<U11'),\n",
       "       array(['tools'], dtype='<U5'), array(['hospitals'], dtype='<U9'),\n",
       "       array(['shops'], dtype='<U5'),\n",
       "       array(['aid', 'centers'], dtype='<U7'),\n",
       "       array(['other', 'infrastructure'], dtype='<U14'),\n",
       "       array(['weather', 'related'], dtype='<U7'),\n",
       "       array(['floods'], dtype='<U6'), array(['storm'], dtype='<U5'),\n",
       "       array(['fire'], dtype='<U4'), array(['earthquake'], dtype='<U10'),\n",
       "       array(['cold'], dtype='<U4'),\n",
       "       array(['other', 'weather'], dtype='<U7'),\n",
       "       array(['direct', 'report'], dtype='<U6')], dtype=object)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_300d_w2v_mean = MeanEmbeddingTrainVectorizer(glove_300d_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_300d_w2v_mean.fit(categories_tokens, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_tokens_embeddings = glove_300d_w2v_mean.transform(categories_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(cat_tokens_embeddings.shape)\n",
    "print(cat_tokens_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cat_tokens_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_msgs_embeddings = glove_300d_w2v_mean.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(test_msgs_embeddings.shape)\n",
    "print(test_msgs_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_msgs_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(test_msgs_embeddings[:10].shape)\n",
    "print(cat_tokens_embeddings.shape)\n",
    "print(cosine_similarity(test_msgs_embeddings[:10],cat_tokens_embeddings).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a CategoriesSimilarity Transformer "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the transformer manually using test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf = CategoriesSimilarity(categories_tokens, glove_50d_w2v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf.fit(X_test, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cats_sim_transf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CategoriesSimilarity feature using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cats_sim_pip = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, cat_tokens_embeddings)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "cats_sim_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cats_sim_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"Score = \", cats_sim_pip.score(X_test, y_test))\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. NLP Feature Selection using Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 - All features together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "all_feats = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "all_feats.fit(X_train, y_train)\n",
    "\n",
    "y_pred = all_feats.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score = all_feats.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize_to_list(lemmatizer=lemmatizer), ngram_range=(1,3))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('multi_clf', MultiOutputClassifier(RandomForestClassifier(random_state=199), n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipelines for different combinations of features and classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm\n",
    "\n",
    "local_w2v_tfidf_svc = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "local_w2v_tfidf_rf = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_svc = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_svc = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_rf = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_svc = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pipelines = [local_w2v_tfidf_svc, local_w2v_tfidf_rf, glove_tfidf_svc, glove_tfidf_rf, \n",
    "             doc2vec_svc, doc2vec_rf, all_feats_svc, all_feats_rf]\n",
    "\n",
    "grid_dict = {0 : 'Local Word2Vec - TF-IDF - Linear SVC',\n",
    "             1 : 'Local Word2Vec - TF-IDF - Random Forest',\n",
    "             2 : 'Glove 300d - TF-IDF - Linear SVC',\n",
    "             3 : 'Glove 300d - TF-IDF - Random Forest',\n",
    "             4 : 'Doc2Vec - Linear SVC',\n",
    "             5 : 'Doc2Vec - Random Forest',\n",
    "             6 : 'All Features - Linear SVC',\n",
    "             7 : 'All Features - Random Forest',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [\n",
    "#     {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "#      'features__num_dims' : [50,100,300]},\n",
    "#     {'features' : [Doc2VecTransformer()],\n",
    "#      'features__vector_size' : [50,100,300]},\n",
    "#     {'features' : [CategoriesSimilarity(glove_50d_w2v,categories_tokens)],\n",
    "#      'features__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v]},\n",
    "#     {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "#      'features__num_dims' : [50,100,300]},\n",
    "#     {'classifier' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "#      'classifier__estimator__penalty' : ['l1', 'l2'],\n",
    "#      'classifier__estimator__C' : [0.1, 1, 3],\n",
    "#      'classifier__estimator__solver' : ['liblinear']},\n",
    "#     {'classifier' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "#     'classifier__estimator__n_estimators' : [10,50,250],\n",
    "#     'classifier__estimator__max_depth' : [10, 50, 100],\n",
    "#     'classifier__estimator__min_samples_split' : [1, 5, 10]}\n",
    "# ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generic_pip = Pipeline([\n",
    "#     ('features',TfidfEmbeddingTrainVectorizer()),\n",
    "#     ('classifier',MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MeanEmbeddingTrainVectorizer().get_params().keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = [\n",
    "#     {'features' : [TfidfEmbeddingTrainVectorizer()],\n",
    "#      'features__word2vec_model' : [None,glove_50d_w2v],\n",
    "#      'features__num_dims' : [50]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# #score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "# jobs = -1\n",
    "# score = 'f1_micro'\n",
    "# def_cv = 3\n",
    "\n",
    "# generic_gs = GridSearchCV(estimator=generic_pip,\n",
    "#             param_grid=param_grid,\n",
    "#             scoring=score,\n",
    "#             cv=def_cv,\n",
    "#             n_jobs=jobs,\n",
    "#             verbose=True)  \n",
    "\n",
    "# # randomized_gs = RandomizedSearchCV(estimator=generic_pip,\n",
    "# #             param_distributions=param_grid,\n",
    "# #             scoring=score,\n",
    "# #             cv=def_cv,\n",
    "# #             n_jobs=jobs,\n",
    "# #             verbose=True,\n",
    "# #             random_state=199,\n",
    "# #             n_iter=2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# grids = [generic_gs]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Generic Grid',\n",
    "# }\n",
    "\n",
    "# print('Performing Grid Search...')\n",
    "# # Fit grid search\n",
    "# generic_gs.fit(X_train, y_train)\n",
    "# # Best params\n",
    "# print('Best params: %s' % generic_gs.best_params_)\n",
    "# # Best training data accuracy\n",
    "# print('Best training score: %.3f' % generic_gs.best_score_)\n",
    "# # Predict on test data with best params\n",
    "# test_score = generic_gs.score(X_test, y_test)\n",
    "# # Test data accuracy of model with best params\n",
    "# print('Test set score for best params: %.3f ' % test_score)\n",
    "\n",
    "# # Track Grid Search results\n",
    "# gs_results_df = pd.DataFrame(generic_gs.cv_results_)\n",
    "# gs_results_df['best_model_test_score'] = test_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run a RandomizedSearchCV for each featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTrainVectorizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, word2vec_model=None, num_dims=100):\n",
    "        if word2vec_model is None:\n",
    "            self.word2vec_model = None\n",
    "            self.num_dims = num_dims\n",
    "            self.workers = multiprocessing.cpu_count() - 1\n",
    "            \n",
    "        else:\n",
    "            self.word2vec_model = word2vec_model\n",
    "            self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "        print(self.num_dims)\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers)\n",
    "        \n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingTrainVectorizer(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, word2vec_model=None, num_dims=100):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = num_dims\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.workers_ = multiprocessing.cpu_count() - 1\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers_)\n",
    "        self.num_dims = self.word2vec_model.vector_size\n",
    "            \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights_ = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights_[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CategoriesSimilarity(BaseEstimator):\n",
    "    \n",
    "    def __init__(self, categories_tokens, word2vec_model=None, num_dims=100):\n",
    "        self.categories_tokens = categories_tokens\n",
    "        self.word2vec_model = word2vec_model    \n",
    "        self.num_dims = num_dims\n",
    "        \n",
    "    def compute_mean_embeddings(self, tokens_array):    \n",
    "        mean_embeddings = np.empty([tokens_array.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(tokens_array.shape[0]):\n",
    "            doc_tokens = tokens_array[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings\n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.workers_ = multiprocessing.cpu_count() - 1\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X, size=self.num_dims, \n",
    "                                                         workers=self.workers_)\n",
    "        self.num_dims = self.word2vec_model.vector_size        \n",
    "        self.categories_vectors_ = self.compute_mean_embeddings(self.categories_tokens)\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = self.compute_mean_embeddings(X)\n",
    "        cats_similarities = cosine_similarity(mean_embeddings, self.categories_vectors_)\n",
    "            \n",
    "        return cats_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Pipelines for each featureset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_local_w2v = Pipeline([\n",
    "    ('local_w2v', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pip_glove = Pipeline([\n",
    "    ('glove', TfidfEmbeddingTrainVectorizer(glove_50d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pip_doc2vec = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pip_cats_sim = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(categories_tokens=categories_tokens)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Define Grid Params for each featureset / classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_models_simple = [MultiOutputClassifier(RandomForestClassifier(random_state=199,\n",
    "                                                        n_estimators=50,\n",
    "                                                        max_depth=100,\n",
    "                                                        min_samples_split=5)),\n",
    "                                            MultiOutputClassifier(LogisticRegression(random_state=199,\n",
    "                                                                                    solver='liblinear',\n",
    "                                                                                    C=1,\n",
    "                                                                                    penalty='l2'))]\n",
    "\n",
    "params_options_local_w2v_simple = {'local_w2v__num_dims' : [50,100],\n",
    "                                   'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_local_w2v = {'local_w2v__num_dims' : [50,100,300],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_glove = {'glove__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_doc2vec = {'doc2vec__vector_size' : [50,100,300],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_cat_sim = {'cats_sim__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "                            'clf' : params_options_models_simple}\n",
    "\n",
    "params_options_rf = {'clf' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "                     'clf__estimator__n_estimators' : [50,250],\n",
    "                     'clf__estimator__max_depth' : [50,100],\n",
    "                     'clf__estimator__min_samples_split' : [2, 5]}\n",
    "\n",
    "params_options_lg = {'clf' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "                     'clf__estimator__penalty' : ['l1', 'l2'],\n",
    "                     'clf__estimator__C' : [0.1, 1, 3],\n",
    "                     'clf__estimator__solver' : ['liblinear']}\n",
    "\n",
    "params_options_empty = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "#score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "jobs = -1\n",
    "score = 'f1_micro'\n",
    "def_cv = 3\n",
    "verbosity_level=10\n",
    "\n",
    "gs_local_w2v = GridSearchCV(estimator=pip_local_w2v,\n",
    "            param_grid=params_options_local_w2v,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level)  \n",
    "\n",
    "gs_glove = GridSearchCV(estimator=pip_glove,\n",
    "            param_grid=params_options_glove,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "gs_doc2vec = GridSearchCV(estimator=pip_doc2vec,\n",
    "            param_grid=params_options_doc2vec,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level)\n",
    "\n",
    "gs_cats_sim = GridSearchCV(estimator=pip_cats_sim,\n",
    "            param_grid=params_options_cat_sim,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "randomized_gs = RandomizedSearchCV(estimator=pip_local_w2v,\n",
    "            param_distributions=params_options_local_w2v,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level,\n",
    "            random_state=199,\n",
    "            n_iter=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Feature: Local W2V\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50, score=0.467, total= 3.0min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50, score=0.464, total= 3.0min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  6.0min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=50, score=0.463, total= 3.1min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  9.1min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100, score=0.472, total= 3.9min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 13.0min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100, score=0.471, total= 3.9min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 16.9min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=100, score=0.468, total= 3.9min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 20.7min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300, score=0.477, total= 6.0min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed: 26.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300, score=0.473, total= 6.0min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 32.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), local_w2v__num_dims=300, score=0.473, total= 6.0min\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 38.8min remaining:    0.0s\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50, score=0.370, total=  39.1s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50, score=0.365, total=  38.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=50, score=0.367, total=  38.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100, score=0.370, total=  41.4s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100, score=0.365, total=  40.7s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=100, score=0.367, total=  41.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300, score=0.370, total=  52.5s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300, score=0.365, total=  52.0s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), local_w2v__num_dims=300, score=0.367, total=  52.5s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 45.4min finished\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf': MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), 'local_w2v__num_dims': 300}\n",
      "Best training score: 0.474\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set score for best params: 0.475 \n",
      "\n",
      "Estimator: Feature: Glove\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.454, total= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  3.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.449, total= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  5.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.454, total= 2.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  8.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.459, total= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 12.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.457, total= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 16.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.457, total= 3.6min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 20.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.465, total= 5.9min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed: 26.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.467, total= 6.0min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 33.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.464, total= 6.2min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 39.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.437, total=  21.2s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.431, total=  21.3s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, score=0.433, total=  21.7s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.444, total=  30.6s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.437, total=  31.0s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, score=0.440, total=  30.7s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.448, total=  58.7s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.442, total=  59.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940> \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), glove__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, score=0.443, total=  59.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 47.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf': MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), 'glove__word2vec_model': <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>}\n",
      "Best training score: 0.465\n",
      "Test set score for best params: 0.467 \n",
      "\n",
      "Estimator: Feature: Doc2Vec\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:47: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50, score=0.387, total= 5.2min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  5.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50, score=0.382, total= 5.2min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed: 10.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=50, score=0.385, total= 5.3min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed: 15.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100, score=0.385, total= 9.2min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed: 24.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100, score=0.380, total= 9.3min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed: 34.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=100, score=0.384, total= 9.2min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed: 43.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300, score=0.387, total=24.1min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed: 67.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300, score=0.380, total=23.5min\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 90.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), doc2vec__vector_size=300, score=0.385, total=23.9min\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 114.8min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50, score=0.370, total=  21.9s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50, score=0.365, total=  22.6s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=50, score=0.367, total=  21.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100, score=0.370, total=  24.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100, score=0.365, total=  24.9s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=100, score=0.367, total=  24.8s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300, score=0.370, total=  36.6s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300, score=0.365, total=  36.5s\n",
      "[CV] clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300 \n",
      "[CV]  clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), doc2vec__vector_size=300, score=0.367, total=  36.4s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 119.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'clf': MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), 'doc2vec__vector_size': 50}\n",
      "Best training score: 0.385\n",
      "Test set score for best params: 0.383 \n",
      "\n",
      "Estimator: Feature: Category Similarity\n",
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:47: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.430, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   1 out of   1 | elapsed:  2.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.425, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   2 out of   2 | elapsed:  4.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.428, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   3 out of   3 | elapsed:  6.4min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.376, total=   9.2s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   4 out of   4 | elapsed:  6.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.372, total=   9.2s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   5 out of   5 | elapsed:  7.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f60155cc198>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.374, total=   9.4s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   6 out of   6 | elapsed:  7.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.430, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   7 out of   7 | elapsed:  9.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.425, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   8 out of   8 | elapsed: 11.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.432, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done   9 out of   9 | elapsed: 13.7min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.375, total=   9.0s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.369, total=   9.0s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5ffdf03cc0>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.371, total=   9.3s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.436, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.433, total= 2.0min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199)), score=0.431, total= 2.2min\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.372, total=   9.8s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.369, total=   9.8s\n",
      "[CV] cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')) \n",
      "[CV]  cats_sim__word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, clf=MultiOutputClassifier(estimator=LogisticRegression(C=1, random_state=199,\n",
      "                                                   solver='liblinear')), score=0.372, total=   9.6s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Done  18 out of  18 | elapsed: 22.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: {'cats_sim__word2vec_model': <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fec1d4940>, 'clf': MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
      "                                                       min_samples_split=5,\n",
      "                                                       n_estimators=50,\n",
      "                                                       random_state=199))}\n",
      "Best training score: 0.433\n",
      "Test set score for best params: 0.435 \n",
      "\n",
      "Classifier with best test set accuracy: Feature: Local W2V\n",
      "CPU times: user 4h 25min 5s, sys: 30.7 s, total: 4h 25min 35s\n",
      "Wall time: 4h 26min 43s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:47: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "grids = [gs_local_w2v, gs_glove, gs_doc2vec, gs_cats_sim]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Feature: Local W2V',\n",
    "    1 : 'Feature: Glove',\n",
    "    2 : 'Feature: Doc2Vec',\n",
    "    3 : 'Feature: Category Similarity'\n",
    "}\n",
    "\n",
    "# grids = [gs_all_feats_best_params]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Feature: All Features with Best Params',\n",
    "# }\n",
    "\n",
    "# grids = [gs_local_w2v]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Feature: Local W2V',\n",
    "# }\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_score = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "overall_results_df = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    overall_results_df = pd.concat([overall_results_df, gs_results_df])\n",
    "    \n",
    "    # Track best (highest test accuracy) model\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_model_test_score</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_cats_sim__word2vec_model</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_doc2vec__vector_size</th>\n",
       "      <th>param_glove__word2vec_model</th>\n",
       "      <th>param_local_w2v__num_dims</th>\n",
       "      <th>param_set_order</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>167.244666</td>\n",
       "      <td>13.906328</td>\n",
       "      <td>0.464909</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.467375</td>\n",
       "      <td>0.464434</td>\n",
       "      <td>0.462917</td>\n",
       "      <td>2.242656</td>\n",
       "      <td>0.354722</td>\n",
       "      <td>0.001851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>220.064234</td>\n",
       "      <td>13.668766</td>\n",
       "      <td>0.470284</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>1</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.471858</td>\n",
       "      <td>0.471068</td>\n",
       "      <td>0.467926</td>\n",
       "      <td>0.865947</td>\n",
       "      <td>0.301182</td>\n",
       "      <td>0.001698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>346.429916</td>\n",
       "      <td>14.566423</td>\n",
       "      <td>0.474161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.476849</td>\n",
       "      <td>0.472814</td>\n",
       "      <td>0.472820</td>\n",
       "      <td>0.647241</td>\n",
       "      <td>0.282001</td>\n",
       "      <td>0.001901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>26.878744</td>\n",
       "      <td>12.008283</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>50</td>\n",
       "      <td>3</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.206916</td>\n",
       "      <td>0.118321</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>29.203583</td>\n",
       "      <td>12.084064</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100</td>\n",
       "      <td>4</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.305883</td>\n",
       "      <td>0.120588</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>39.943756</td>\n",
       "      <td>12.376639</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300</td>\n",
       "      <td>5</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.131247</td>\n",
       "      <td>0.165002</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>154.236127</td>\n",
       "      <td>6.253713</td>\n",
       "      <td>0.452303</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.453860</td>\n",
       "      <td>0.449492</td>\n",
       "      <td>0.453557</td>\n",
       "      <td>0.849040</td>\n",
       "      <td>0.011945</td>\n",
       "      <td>0.001992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>208.462374</td>\n",
       "      <td>6.279925</td>\n",
       "      <td>0.457773</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.458837</td>\n",
       "      <td>0.457468</td>\n",
       "      <td>0.457015</td>\n",
       "      <td>1.141801</td>\n",
       "      <td>0.196657</td>\n",
       "      <td>0.000775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>355.494968</td>\n",
       "      <td>6.937809</td>\n",
       "      <td>0.465437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.464921</td>\n",
       "      <td>0.466906</td>\n",
       "      <td>0.464483</td>\n",
       "      <td>6.199437</td>\n",
       "      <td>0.262951</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>17.073034</td>\n",
       "      <td>4.364227</td>\n",
       "      <td>0.433407</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.437000</td>\n",
       "      <td>0.430560</td>\n",
       "      <td>0.432662</td>\n",
       "      <td>0.167471</td>\n",
       "      <td>0.049439</td>\n",
       "      <td>0.002682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>26.393060</td>\n",
       "      <td>4.375241</td>\n",
       "      <td>0.440067</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.443586</td>\n",
       "      <td>0.437071</td>\n",
       "      <td>0.439543</td>\n",
       "      <td>0.131923</td>\n",
       "      <td>0.059781</td>\n",
       "      <td>0.002685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>54.707542</td>\n",
       "      <td>4.562382</td>\n",
       "      <td>0.444468</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.447776</td>\n",
       "      <td>0.442216</td>\n",
       "      <td>0.443411</td>\n",
       "      <td>0.438462</td>\n",
       "      <td>0.006416</td>\n",
       "      <td>0.002390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>306.331991</td>\n",
       "      <td>6.948815</td>\n",
       "      <td>0.384753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387285</td>\n",
       "      <td>0.381709</td>\n",
       "      <td>0.385264</td>\n",
       "      <td>2.333601</td>\n",
       "      <td>0.018058</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>545.128783</td>\n",
       "      <td>7.422454</td>\n",
       "      <td>0.383175</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.385267</td>\n",
       "      <td>0.380273</td>\n",
       "      <td>0.383985</td>\n",
       "      <td>1.857798</td>\n",
       "      <td>0.043398</td>\n",
       "      <td>0.002118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>1420.836541</td>\n",
       "      <td>9.360239</td>\n",
       "      <td>0.383913</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.386760</td>\n",
       "      <td>0.379938</td>\n",
       "      <td>0.385041</td>\n",
       "      <td>14.789757</td>\n",
       "      <td>0.194229</td>\n",
       "      <td>0.002897</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>17.362835</td>\n",
       "      <td>4.721548</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>50</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.315864</td>\n",
       "      <td>0.035105</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>19.802789</td>\n",
       "      <td>5.048601</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>100</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.176957</td>\n",
       "      <td>0.142177</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>30.496341</td>\n",
       "      <td>5.988760</td>\n",
       "      <td>0.367483</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>300</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.370103</td>\n",
       "      <td>0.365089</td>\n",
       "      <td>0.367257</td>\n",
       "      <td>0.111015</td>\n",
       "      <td>0.047804</td>\n",
       "      <td>0.002053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>115.007597</td>\n",
       "      <td>4.093426</td>\n",
       "      <td>0.427722</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>3</td>\n",
       "      <td>0.430089</td>\n",
       "      <td>0.425414</td>\n",
       "      <td>0.427664</td>\n",
       "      <td>0.483611</td>\n",
       "      <td>0.046578</td>\n",
       "      <td>0.001909</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>7.102805</td>\n",
       "      <td>2.177822</td>\n",
       "      <td>0.373830</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>4</td>\n",
       "      <td>0.375757</td>\n",
       "      <td>0.371749</td>\n",
       "      <td>0.373984</td>\n",
       "      <td>0.077842</td>\n",
       "      <td>0.029864</td>\n",
       "      <td>0.001640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>115.228944</td>\n",
       "      <td>4.064365</td>\n",
       "      <td>0.429128</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>2</td>\n",
       "      <td>0.430324</td>\n",
       "      <td>0.425311</td>\n",
       "      <td>0.431748</td>\n",
       "      <td>0.950140</td>\n",
       "      <td>0.022948</td>\n",
       "      <td>0.002761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>6.956295</td>\n",
       "      <td>2.170544</td>\n",
       "      <td>0.371877</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>5</td>\n",
       "      <td>0.374749</td>\n",
       "      <td>0.369453</td>\n",
       "      <td>0.371428</td>\n",
       "      <td>0.116625</td>\n",
       "      <td>0.012897</td>\n",
       "      <td>0.002185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>118.590605</td>\n",
       "      <td>4.333922</td>\n",
       "      <td>0.433137</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436032</td>\n",
       "      <td>0.432530</td>\n",
       "      <td>0.430850</td>\n",
       "      <td>6.491331</td>\n",
       "      <td>0.236132</td>\n",
       "      <td>0.002159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>7.338744</td>\n",
       "      <td>2.427869</td>\n",
       "      <td>0.370794</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>6</td>\n",
       "      <td>0.372187</td>\n",
       "      <td>0.368669</td>\n",
       "      <td>0.371527</td>\n",
       "      <td>0.165633</td>\n",
       "      <td>0.133500</td>\n",
       "      <td>0.001527</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   best_model_test_score                       grid_id  mean_fit_time  \\\n",
       "0               0.475325            Feature: Local W2V     167.244666   \n",
       "1               0.475325            Feature: Local W2V     220.064234   \n",
       "2               0.475325            Feature: Local W2V     346.429916   \n",
       "3               0.475325            Feature: Local W2V      26.878744   \n",
       "4               0.475325            Feature: Local W2V      29.203583   \n",
       "5               0.475325            Feature: Local W2V      39.943756   \n",
       "0               0.467183                Feature: Glove     154.236127   \n",
       "1               0.467183                Feature: Glove     208.462374   \n",
       "2               0.467183                Feature: Glove     355.494968   \n",
       "3               0.467183                Feature: Glove      17.073034   \n",
       "4               0.467183                Feature: Glove      26.393060   \n",
       "5               0.467183                Feature: Glove      54.707542   \n",
       "0               0.383461              Feature: Doc2Vec     306.331991   \n",
       "1               0.383461              Feature: Doc2Vec     545.128783   \n",
       "2               0.383461              Feature: Doc2Vec    1420.836541   \n",
       "3               0.383461              Feature: Doc2Vec      17.362835   \n",
       "4               0.383461              Feature: Doc2Vec      19.802789   \n",
       "5               0.383461              Feature: Doc2Vec      30.496341   \n",
       "0               0.434835  Feature: Category Similarity     115.007597   \n",
       "1               0.434835  Feature: Category Similarity       7.102805   \n",
       "2               0.434835  Feature: Category Similarity     115.228944   \n",
       "3               0.434835  Feature: Category Similarity       6.956295   \n",
       "4               0.434835  Feature: Category Similarity     118.590605   \n",
       "5               0.434835  Feature: Category Similarity       7.338744   \n",
       "\n",
       "   mean_score_time  mean_test_score  \\\n",
       "0        13.906328         0.464909   \n",
       "1        13.668766         0.470284   \n",
       "2        14.566423         0.474161   \n",
       "3        12.008283         0.367483   \n",
       "4        12.084064         0.367483   \n",
       "5        12.376639         0.367483   \n",
       "0         6.253713         0.452303   \n",
       "1         6.279925         0.457773   \n",
       "2         6.937809         0.465437   \n",
       "3         4.364227         0.433407   \n",
       "4         4.375241         0.440067   \n",
       "5         4.562382         0.444468   \n",
       "0         6.948815         0.384753   \n",
       "1         7.422454         0.383175   \n",
       "2         9.360239         0.383913   \n",
       "3         4.721548         0.367483   \n",
       "4         5.048601         0.367483   \n",
       "5         5.988760         0.367483   \n",
       "0         4.093426         0.427722   \n",
       "1         2.177822         0.373830   \n",
       "2         4.064365         0.429128   \n",
       "3         2.170544         0.371877   \n",
       "4         4.333922         0.433137   \n",
       "5         2.427869         0.370794   \n",
       "\n",
       "                      param_cats_sim__word2vec_model  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "0  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "1  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "2  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "3  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "4  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "5  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "1  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "2  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "3  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "4  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "5  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "0  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "1  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "2  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "3  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "4  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "5  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "0  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "1  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "2  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "3  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "4  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "5  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "0  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "1  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "2  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "3  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "4  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "5  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "\n",
       "  param_doc2vec__vector_size  \\\n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "5                        NaN   \n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "5                        NaN   \n",
       "0                         50   \n",
       "1                        100   \n",
       "2                        300   \n",
       "3                         50   \n",
       "4                        100   \n",
       "5                        300   \n",
       "0                        NaN   \n",
       "1                        NaN   \n",
       "2                        NaN   \n",
       "3                        NaN   \n",
       "4                        NaN   \n",
       "5                        NaN   \n",
       "\n",
       "                         param_glove__word2vec_model  \\\n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "0  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "1  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "2  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "3  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "4  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "5  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "0                                                NaN   \n",
       "1                                                NaN   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4                                                NaN   \n",
       "5                                                NaN   \n",
       "\n",
       "  param_local_w2v__num_dims  param_set_order  \\\n",
       "0                        50                0   \n",
       "1                       100                1   \n",
       "2                       300                2   \n",
       "3                        50                3   \n",
       "4                       100                4   \n",
       "5                       300                5   \n",
       "0                       NaN                0   \n",
       "1                       NaN                1   \n",
       "2                       NaN                2   \n",
       "3                       NaN                3   \n",
       "4                       NaN                4   \n",
       "5                       NaN                5   \n",
       "0                       NaN                0   \n",
       "1                       NaN                1   \n",
       "2                       NaN                2   \n",
       "3                       NaN                3   \n",
       "4                       NaN                4   \n",
       "5                       NaN                5   \n",
       "0                       NaN                0   \n",
       "1                       NaN                1   \n",
       "2                       NaN                2   \n",
       "3                       NaN                3   \n",
       "4                       NaN                4   \n",
       "5                       NaN                5   \n",
       "\n",
       "                                              params  rank_test_score  \\\n",
       "0  {'clf': MultiOutputClassifier(estimator=Random...                3   \n",
       "1  {'clf': MultiOutputClassifier(estimator=Random...                2   \n",
       "2  {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "3  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "4  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "5  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "0  {'clf': MultiOutputClassifier(estimator=Random...                3   \n",
       "1  {'clf': MultiOutputClassifier(estimator=Random...                2   \n",
       "2  {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "3  {'clf': MultiOutputClassifier(estimator=Logist...                6   \n",
       "4  {'clf': MultiOutputClassifier(estimator=Logist...                5   \n",
       "5  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "0  {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "1  {'clf': MultiOutputClassifier(estimator=Random...                3   \n",
       "2  {'clf': MultiOutputClassifier(estimator=Random...                2   \n",
       "3  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "4  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "5  {'clf': MultiOutputClassifier(estimator=Logist...                4   \n",
       "0  {'cats_sim__word2vec_model': <gensim.models.ke...                3   \n",
       "1  {'cats_sim__word2vec_model': <gensim.models.ke...                4   \n",
       "2  {'cats_sim__word2vec_model': <gensim.models.ke...                2   \n",
       "3  {'cats_sim__word2vec_model': <gensim.models.ke...                5   \n",
       "4  {'cats_sim__word2vec_model': <gensim.models.ke...                1   \n",
       "5  {'cats_sim__word2vec_model': <gensim.models.ke...                6   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  std_fit_time  \\\n",
       "0           0.467375           0.464434           0.462917      2.242656   \n",
       "1           0.471858           0.471068           0.467926      0.865947   \n",
       "2           0.476849           0.472814           0.472820      0.647241   \n",
       "3           0.370103           0.365089           0.367257      0.206916   \n",
       "4           0.370103           0.365089           0.367257      0.305883   \n",
       "5           0.370103           0.365089           0.367257      0.131247   \n",
       "0           0.453860           0.449492           0.453557      0.849040   \n",
       "1           0.458837           0.457468           0.457015      1.141801   \n",
       "2           0.464921           0.466906           0.464483      6.199437   \n",
       "3           0.437000           0.430560           0.432662      0.167471   \n",
       "4           0.443586           0.437071           0.439543      0.131923   \n",
       "5           0.447776           0.442216           0.443411      0.438462   \n",
       "0           0.387285           0.381709           0.385264      2.333601   \n",
       "1           0.385267           0.380273           0.383985      1.857798   \n",
       "2           0.386760           0.379938           0.385041     14.789757   \n",
       "3           0.370103           0.365089           0.367257      0.315864   \n",
       "4           0.370103           0.365089           0.367257      0.176957   \n",
       "5           0.370103           0.365089           0.367257      0.111015   \n",
       "0           0.430089           0.425414           0.427664      0.483611   \n",
       "1           0.375757           0.371749           0.373984      0.077842   \n",
       "2           0.430324           0.425311           0.431748      0.950140   \n",
       "3           0.374749           0.369453           0.371428      0.116625   \n",
       "4           0.436032           0.432530           0.430850      6.491331   \n",
       "5           0.372187           0.368669           0.371527      0.165633   \n",
       "\n",
       "   std_score_time  std_test_score  \n",
       "0        0.354722        0.001851  \n",
       "1        0.301182        0.001698  \n",
       "2        0.282001        0.001901  \n",
       "3        0.118321        0.002053  \n",
       "4        0.120588        0.002053  \n",
       "5        0.165002        0.002053  \n",
       "0        0.011945        0.001992  \n",
       "1        0.196657        0.000775  \n",
       "2        0.262951        0.001054  \n",
       "3        0.049439        0.002682  \n",
       "4        0.059781        0.002685  \n",
       "5        0.006416        0.002390  \n",
       "0        0.018058        0.002305  \n",
       "1        0.043398        0.002118  \n",
       "2        0.194229        0.002897  \n",
       "3        0.035105        0.002053  \n",
       "4        0.142177        0.002053  \n",
       "5        0.047804        0.002053  \n",
       "0        0.046578        0.001909  \n",
       "1        0.029864        0.001640  \n",
       "2        0.022948        0.002761  \n",
       "3        0.012897        0.002185  \n",
       "4        0.236132        0.002159  \n",
       "5        0.133500        0.001527  "
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_feature_sets = overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_feature_sets.to_csv('f1micro_score_per_featset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('local_w2v',\n",
       "                 TfidfEmbeddingTrainVectorizer(num_dims=300,\n",
       "                                               word2vec_model=<gensim.models.word2vec.Word2Vec object at 0x7f5fc6f53048>)),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
       "                                                                        min_samples_split=5,\n",
       "                                                                        n_estimators=50,\n",
       "                                                                        random_state=199)))])"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_local_w2v.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('glove',\n",
       "                 TfidfEmbeddingTrainVectorizer(num_dims=300,\n",
       "                                               word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fba491f28>)),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
       "                                                                        min_samples_split=5,\n",
       "                                                                        n_estimators=50,\n",
       "                                                                        random_state=199)))])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_glove.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('doc2vec', Doc2VecTransformer(vector_size=50)),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
       "                                                                        min_samples_split=5,\n",
       "                                                                        n_estimators=50,\n",
       "                                                                        random_state=199)))])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_doc2vec.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('cats_sim',\n",
       "                 CategoriesSimilarity(categories_tokens=array([array(['related'], dtype='<U7'), array(['request'], dtype='<U7'),\n",
       "       array(['offer'], dtype='<U5'),\n",
       "       array(['aid', 'related'], dtype='<U7'),\n",
       "       array(['medical', 'help'], dtype='<U7'),\n",
       "       array(['medical', 'products'], dtype='<U8'),\n",
       "       array(['search', 'and', 'rescue'], dtype='<U6'),\n",
       "       array(['security'], dtyp...\n",
       "       array(['cold'], dtype='<U4'),\n",
       "       array(['other', 'weather'], dtype='<U7'),\n",
       "       array(['direct', 'report'], dtype='<U6')], dtype=object),\n",
       "                                      num_dims=300,\n",
       "                                      word2vec_model=<gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f5fb8629f60>)),\n",
       "                ('clf',\n",
       "                 MultiOutputClassifier(estimator=RandomForestClassifier(max_depth=100,\n",
       "                                                                        min_samples_split=5,\n",
       "                                                                        n_estimators=50,\n",
       "                                                                        random_state=199)))])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs_cats_sim.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>best_model_test_score</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>param_cats_sim__word2vec_model</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>param_doc2vec__vector_size</th>\n",
       "      <th>param_glove__word2vec_model</th>\n",
       "      <th>param_local_w2v__num_dims</th>\n",
       "      <th>param_set_order</th>\n",
       "      <th>params</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>std_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.475325</td>\n",
       "      <td>Feature: Local W2V</td>\n",
       "      <td>346.429916</td>\n",
       "      <td>14.566423</td>\n",
       "      <td>0.474161</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.476849</td>\n",
       "      <td>0.472814</td>\n",
       "      <td>0.472820</td>\n",
       "      <td>0.647241</td>\n",
       "      <td>0.282001</td>\n",
       "      <td>0.001901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.467183</td>\n",
       "      <td>Feature: Glove</td>\n",
       "      <td>355.494968</td>\n",
       "      <td>6.937809</td>\n",
       "      <td>0.465437</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.464921</td>\n",
       "      <td>0.466906</td>\n",
       "      <td>0.464483</td>\n",
       "      <td>6.199437</td>\n",
       "      <td>0.262951</td>\n",
       "      <td>0.001054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.383461</td>\n",
       "      <td>Feature: Doc2Vec</td>\n",
       "      <td>306.331991</td>\n",
       "      <td>6.948815</td>\n",
       "      <td>0.384753</td>\n",
       "      <td>NaN</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>50.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.387285</td>\n",
       "      <td>0.381709</td>\n",
       "      <td>0.385264</td>\n",
       "      <td>2.333601</td>\n",
       "      <td>0.018058</td>\n",
       "      <td>0.002305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.434835</td>\n",
       "      <td>Feature: Category Similarity</td>\n",
       "      <td>118.590605</td>\n",
       "      <td>4.333922</td>\n",
       "      <td>0.433137</td>\n",
       "      <td>&lt;gensim.models.keyedvectors.Word2VecKeyedVecto...</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>{'cats_sim__word2vec_model': &lt;gensim.models.ke...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.436032</td>\n",
       "      <td>0.432530</td>\n",
       "      <td>0.430850</td>\n",
       "      <td>6.491331</td>\n",
       "      <td>0.236132</td>\n",
       "      <td>0.002159</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    best_model_test_score                       grid_id  mean_fit_time  \\\n",
       "2                0.475325            Feature: Local W2V     346.429916   \n",
       "8                0.467183                Feature: Glove     355.494968   \n",
       "12               0.383461              Feature: Doc2Vec     306.331991   \n",
       "22               0.434835  Feature: Category Similarity     118.590605   \n",
       "\n",
       "    mean_score_time  mean_test_score  \\\n",
       "2         14.566423         0.474161   \n",
       "8          6.937809         0.465437   \n",
       "12         6.948815         0.384753   \n",
       "22         4.333922         0.433137   \n",
       "\n",
       "                       param_cats_sim__word2vec_model  \\\n",
       "2                                                 NaN   \n",
       "8                                                 NaN   \n",
       "12                                                NaN   \n",
       "22  <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "\n",
       "                                            param_clf  \\\n",
       "2   MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "8   MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "12  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "22  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "\n",
       "    param_doc2vec__vector_size  \\\n",
       "2                          NaN   \n",
       "8                          NaN   \n",
       "12                        50.0   \n",
       "22                         NaN   \n",
       "\n",
       "                          param_glove__word2vec_model  \\\n",
       "2                                                 NaN   \n",
       "8   <gensim.models.keyedvectors.Word2VecKeyedVecto...   \n",
       "12                                                NaN   \n",
       "22                                                NaN   \n",
       "\n",
       "    param_local_w2v__num_dims  param_set_order  \\\n",
       "2                       300.0                2   \n",
       "8                         NaN                2   \n",
       "12                        NaN                0   \n",
       "22                        NaN                4   \n",
       "\n",
       "                                               params  rank_test_score  \\\n",
       "2   {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "8   {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "12  {'clf': MultiOutputClassifier(estimator=Random...                1   \n",
       "22  {'cats_sim__word2vec_model': <gensim.models.ke...                1   \n",
       "\n",
       "    split0_test_score  split1_test_score  split2_test_score  std_fit_time  \\\n",
       "2            0.476849           0.472814           0.472820      0.647241   \n",
       "8            0.464921           0.466906           0.464483      6.199437   \n",
       "12           0.387285           0.381709           0.385264      2.333601   \n",
       "22           0.436032           0.432530           0.430850      6.491331   \n",
       "\n",
       "    std_score_time  std_test_score  \n",
       "2         0.282001        0.001901  \n",
       "8         0.262951        0.001054  \n",
       "12        0.018058        0.002305  \n",
       "22        0.236132        0.002159  "
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = pd.read_csv('f1micro_score_per_featset.csv')\n",
    "test[test['rank_test_score'] == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test All Features together using their best params from GridSearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_all_feats = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "        ('glove', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer(vector_size=50)),\n",
    "        ('cats_sim', CategoriesSimilarity(categories_tokens=categories_tokens,word2vec_model=glove_300d_w2v))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_all_feats = {'clf' : params_options_models_simple}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_all_feats_best_params = GridSearchCV(estimator=pip_all_feats,\n",
    "                                  param_grid=params_options_all_feats,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs,\n",
    "            verbose=verbosity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids = [gs_all_feats_best_params]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Feature: All Features with Best Params',\n",
    "}\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "all_feats_results = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    all_feats_results = pd.concat([all_feats_results, gs_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>param_clf</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>best_model_test_score</th>\n",
       "      <th>param_set_order</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>660.303529</td>\n",
       "      <td>4.151322</td>\n",
       "      <td>28.056237</td>\n",
       "      <td>0.124641</td>\n",
       "      <td>MultiOutputClassifier(estimator=RandomForestCl...</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Random...</td>\n",
       "      <td>0.471034</td>\n",
       "      <td>0.469648</td>\n",
       "      <td>0.469826</td>\n",
       "      <td>0.470169</td>\n",
       "      <td>0.000616</td>\n",
       "      <td>1</td>\n",
       "      <td>Feature: All Features with Best Params</td>\n",
       "      <td>0.473316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>172.703791</td>\n",
       "      <td>1.489163</td>\n",
       "      <td>24.720033</td>\n",
       "      <td>0.489687</td>\n",
       "      <td>MultiOutputClassifier(estimator=LogisticRegres...</td>\n",
       "      <td>{'clf': MultiOutputClassifier(estimator=Logist...</td>\n",
       "      <td>0.450838</td>\n",
       "      <td>0.444178</td>\n",
       "      <td>0.446887</td>\n",
       "      <td>0.447301</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>2</td>\n",
       "      <td>Feature: All Features with Best Params</td>\n",
       "      <td>0.473316</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time  \\\n",
       "0     660.303529      4.151322        28.056237        0.124641   \n",
       "1     172.703791      1.489163        24.720033        0.489687   \n",
       "\n",
       "                                           param_clf  \\\n",
       "0  MultiOutputClassifier(estimator=RandomForestCl...   \n",
       "1  MultiOutputClassifier(estimator=LogisticRegres...   \n",
       "\n",
       "                                              params  split0_test_score  \\\n",
       "0  {'clf': MultiOutputClassifier(estimator=Random...           0.471034   \n",
       "1  {'clf': MultiOutputClassifier(estimator=Logist...           0.450838   \n",
       "\n",
       "   split1_test_score  split2_test_score  mean_test_score  std_test_score  \\\n",
       "0           0.469648           0.469826         0.470169        0.000616   \n",
       "1           0.444178           0.446887         0.447301        0.002735   \n",
       "\n",
       "   rank_test_score                                 grid_id  \\\n",
       "0                1  Feature: All Features with Best Params   \n",
       "1                2  Feature: All Features with Best Params   \n",
       "\n",
       "   best_model_test_score  param_set_order  \n",
       "0               0.473316                0  \n",
       "1               0.473316                1  "
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_feats_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feats_results.to_csv('f1micro_score_all_feats.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Final Decision between models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip_local_w2v_final = Pipeline([\n",
    "    ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])\n",
    "\n",
    "pip_all_feats_final = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v', TfidfEmbeddingTrainVectorizer(num_dims=300)),\n",
    "        ('glove', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer(vector_size=50)),\n",
    "        ('cats_sim', CategoriesSimilarity(categories_tokens=categories_tokens,word2vec_model=glove_300d_w2v))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier()))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_options_rf = {'clf' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "                     'clf__estimator__n_estimators' : [50,100],\n",
    "                     'clf__estimator__max_depth' : [50,100],\n",
    "                     'clf__estimator__min_samples_split' : [2, 5]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs_local_w2v_final = GridSearchCV(estimator=pip_local_w2v_final,\n",
    "            param_grid=params_options_rf,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs,\n",
    "            verbose=verbosity_level) \n",
    "\n",
    "gs_all_feats_final = GridSearchCV(estimator=pip_all_feats_final,\n",
    "                                  param_grid=params_options_rf,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs,\n",
    "            verbose=verbosity_level)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Feature: All Features with Best Params\n",
      "Fitting 3 folds for each of 8 candidates, totalling 24 fits\n",
      "[CV] clf=MultiOutputClassifier(estimator=RandomForestClassifier(random_state=199)), clf__estimator__max_depth=50, clf__estimator__min_samples_split=2, clf__estimator__n_estimators=50 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__contains__` (Method will be removed in 4.0.0, use self.wv.__contains__() instead).\n",
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:29: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-138-4199e5de1261>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nEstimator: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgrid_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0;31m# Fit grid search\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m     \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     \u001b[0;31m# Best params\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Best params: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_params_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1186\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1187\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1188\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    713\u001b[0m                                \u001b[0;32mfor\u001b[0m \u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    714\u001b[0m                                in product(candidate_params,\n\u001b[0;32m--> 715\u001b[0;31m                                           cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    716\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    717\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[0;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[1;32m    529\u001b[0m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    530\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 531\u001b[0;31m             \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    532\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    333\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m'passthrough'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m                 \u001b[0mfit_params_last_step\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_params_steps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_final_estimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mXt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params_last_step\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, Y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mself\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    350\u001b[0m         \"\"\"\n\u001b[0;32m--> 351\u001b[0;31m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    352\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mestimator\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimators_\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    353\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m    174\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m                 **fit_params_validated)\n\u001b[0;32m--> 176\u001b[0;31m             for i in range(y.shape[1]))\n\u001b[0m\u001b[1;32m    177\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/multioutput.py\u001b[0m in \u001b[0;36m_fit_estimator\u001b[0;34m(estimator, X, y, sample_weight, **fit_params)\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    390\u001b[0m                     \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m                     n_samples_bootstrap=n_samples_bootstrap)\n\u001b[0;32m--> 392\u001b[0;31m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[1;32m    393\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m             \u001b[0;31m# Collect newly grown trees\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    778\u001b[0m             \u001b[0;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 779\u001b[0;31m             \u001b[0;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    781\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    623\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    624\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 588\u001b[0;31m         \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    590\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    109\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    330\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    331\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 332\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    333\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 894\u001b[0;31m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    373\u001b[0m                                            min_impurity_split)\n\u001b[1;32m    374\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         \u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_idx_sorted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    376\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    377\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_outputs_\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mis_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "grids_final = [gs_local_w2v_final]\n",
    "\n",
    "grid_final_dict = {\n",
    "    0 : 'Feature: Local W2V with Best Params',\n",
    "    1 : 'Feature: All Features with Best Params'\n",
    "}\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "final_results = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids_final):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    gs_results_df['param_set_order'] = np.arange(len(gs_results_df))\n",
    "    \n",
    "    final_results = pd.concat([final_results, gs_results_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_ = [\n",
    "    {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "     'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "     'features__num_dims' : [50,100,300]},\n",
    "    {'features' : [Doc2VecTransformer()],\n",
    "     'features__vector_size' : [50,100,300]},\n",
    "    {'features' : [CategoriesSimilarity(glove_50d_w2v,categories_tokens)],\n",
    "     'features__word2vec_model' : [glove_50d_w2v,glove_100d_w2v,glove_300d_w2v]},\n",
    "    {'features' : [MeanEmbeddingTrainVectorizer(),TfidfEmbeddingTrainVectorizer()],\n",
    "     'features__word2vec_model' : [None,glove_50d_w2v,glove_100d_w2v,glove_300d_w2v],\n",
    "     'features__num_dims' : [50,100,300]},\n",
    "    {'classifier' : [MultiOutputClassifier(LogisticRegression(random_state=199))],\n",
    "     'classifier__estimator__penalty' : ['l1', 'l2'],\n",
    "     'classifier__estimator__C' : [0.1, 1, 3],\n",
    "     'classifier__estimator__solver' : ['liblinear']},\n",
    "    {'classifier' : [MultiOutputClassifier(RandomForestClassifier(random_state=199))],\n",
    "    'classifier__estimator__n_estimators' : [10,50,250],\n",
    "    'classifier__estimator__max_depth' : [10, 50, 100],\n",
    "    'classifier__estimator__min_samples_split' : [1, 5, 10]}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "pip_local_w2v_tfidf_naive = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_local_w2v_tfidf_rf = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_naive = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_lg = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(LogisticRegression(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_doc2vec_naive = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_doc2vec_rf = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_cats_sim_naive = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_cats_sim_rf = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pip_all_feats_naive = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_all_feats_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params_empty = [{}]\n",
    "\n",
    "# grid_params_rf = [{'clf__criterion': ['gini', 'entropy'],\n",
    "#         'clf__min_samples_leaf': param_range,\n",
    "#         'clf__max_depth': param_range,\n",
    "#         'clf__min_samples_split': param_range[1:]}]\n",
    "\n",
    "# grid_params_svm = [{'clf__kernel': ['linear', 'rbf'], \n",
    "#         'clf__C': param_range}]\n",
    "\n",
    "# Construct grid searches\n",
    "jobs = -1\n",
    "\n",
    "#score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "score = 'f1_micro'\n",
    "\n",
    "def_cv = 3\n",
    "\n",
    "gs_local_w2v_tfidf_naive = GridSearchCV(estimator=pip_local_w2v_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)  \n",
    "\n",
    "gs_local_w2v_tfidf_rf = GridSearchCV(estimator=pip_local_w2v_tfidf_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_glove_tfidf_naive = GridSearchCV(estimator=pip_glove_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_glove_tfidf_rf = GridSearchCV(estimator=pip_glove_tfidf_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_glove_tfidf_lg = GridSearchCV(estimator=pip_glove_tfidf_lg,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "\n",
    "gs_doc2vec_naive = GridSearchCV(estimator=pip_doc2vec_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_doc2vec_rf = GridSearchCV(estimator=pip_doc2vec_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs)\n",
    "\n",
    "gs_cats_sim_naive = GridSearchCV(estimator=pip_cats_sim_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_cats_sim_rf = GridSearchCV(estimator=pip_cats_sim_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=def_cv,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_all_feats_naive = GridSearchCV(estimator=pip_all_feats_naive,\n",
    "                                  param_grid=grid_params_empty,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs)\n",
    "\n",
    "gs_all_feats_rf = GridSearchCV(estimator=pip_all_feats_rf,\n",
    "                                  param_grid=grid_params_empty,\n",
    "                                 scoring=score,\n",
    "                                 cv=def_cv,\n",
    "                                 n_jobs=jobs)\n",
    "\n",
    "# grids = [gs_local_w2v_tfidf_rf, gs_glove_tfidf_rf, \n",
    "#          gs_doc2vec_rf, gs_cats_sim_rf,\n",
    "#          gs_all_feats_rf]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Local Word2Vec - TF-IDF - Random Forest',\n",
    "#     1 : 'Glove 300d - TF-IDF - Random Forest',\n",
    "#     2 : 'Doc2Vec - Random Forest',\n",
    "#     3 : 'Categories Similarity - Random Forest',\n",
    "#     4 : 'All Features - Random Forest'\n",
    "# }\n",
    "\n",
    "# grids = [gs_local_w2v_tfidf_naive, gs_glove_tfidf_naive, \n",
    "#          gs_doc2vec_naive, gs_cats_sim_naive,\n",
    "#          gs_all_feats_naive]\n",
    "\n",
    "# grid_dict = {\n",
    "#     0 : 'Local Word2Vec - TF-IDF - Naive Bayes',\n",
    "#     1 : 'Glove 300d - TF-IDF - Naive Bayes',\n",
    "#     2 : 'Doc2Vec - Naive Bayes',\n",
    "#     3 : 'Categories Similarity - Naive Bayes',\n",
    "#     4 : 'All Features - Naive Bayes'\n",
    "# }\n",
    "\n",
    "grids = [gs_glove_tfidf_lg]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Glove 300d - TF-IDF - Logistic Regression',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Glove 300d - TF-IDF - Logistic Regression\n",
      "Best params: {}\n",
      "Best training score: 0.444\n",
      "Test set score for best params: 0.447 \n",
      "\n",
      "Classifier with best test set accuracy: Glove 300d - TF-IDF - Logistic Regression\n",
      "CPU times: user 6min 28s, sys: 4.3 s, total: 6min 32s\n",
      "Wall time: 6min 34s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split0_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split1_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('split2_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('mean_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/utils/deprecation.py:122: FutureWarning: You are accessing a training score ('std_train_score'), which will not be available by default any more in 0.21. If you need training scores, please set return_train_score=True\n",
      "  warnings.warn(*warn_args, **warn_kwargs)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_score = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "overall_results_df = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training score: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    \n",
    "    overall_results_df = pd.concat([overall_results_df, gs_results_df])\n",
    "    \n",
    "    # Track best (highest test accuracy) model\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>split2_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>split0_train_score</th>\n",
       "      <th>split1_train_score</th>\n",
       "      <th>split2_train_score</th>\n",
       "      <th>mean_train_score</th>\n",
       "      <th>std_train_score</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>best_model_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>24.193751</td>\n",
       "      <td>1.243320</td>\n",
       "      <td>9.702975</td>\n",
       "      <td>0.216793</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.216412</td>\n",
       "      <td>0.216126</td>\n",
       "      <td>0.212304</td>\n",
       "      <td>0.214947</td>\n",
       "      <td>0.001873</td>\n",
       "      <td>1</td>\n",
       "      <td>0.218802</td>\n",
       "      <td>0.211861</td>\n",
       "      <td>0.217092</td>\n",
       "      <td>0.215918</td>\n",
       "      <td>0.002952</td>\n",
       "      <td>Local Word2Vec - TF-IDF - Naive Bayes</td>\n",
       "      <td>0.209587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.761650</td>\n",
       "      <td>0.157636</td>\n",
       "      <td>4.028346</td>\n",
       "      <td>0.026570</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.227605</td>\n",
       "      <td>0.232018</td>\n",
       "      <td>0.221844</td>\n",
       "      <td>0.227156</td>\n",
       "      <td>0.004165</td>\n",
       "      <td>1</td>\n",
       "      <td>0.229607</td>\n",
       "      <td>0.229266</td>\n",
       "      <td>0.227849</td>\n",
       "      <td>0.228907</td>\n",
       "      <td>0.000761</td>\n",
       "      <td>Glove 300d - TF-IDF - Naive Bayes</td>\n",
       "      <td>0.225530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.382411</td>\n",
       "      <td>0.169216</td>\n",
       "      <td>2.269197</td>\n",
       "      <td>0.052283</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.374412</td>\n",
       "      <td>0.368413</td>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.371065</td>\n",
       "      <td>0.002498</td>\n",
       "      <td>1</td>\n",
       "      <td>0.371911</td>\n",
       "      <td>0.373925</td>\n",
       "      <td>0.372439</td>\n",
       "      <td>0.372759</td>\n",
       "      <td>0.000852</td>\n",
       "      <td>Doc2Vec - Naive Bayes</td>\n",
       "      <td>0.368967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4.361952</td>\n",
       "      <td>0.024142</td>\n",
       "      <td>2.160518</td>\n",
       "      <td>0.002270</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.218708</td>\n",
       "      <td>0.223602</td>\n",
       "      <td>0.218829</td>\n",
       "      <td>0.220380</td>\n",
       "      <td>0.002279</td>\n",
       "      <td>1</td>\n",
       "      <td>0.221607</td>\n",
       "      <td>0.220599</td>\n",
       "      <td>0.221507</td>\n",
       "      <td>0.221238</td>\n",
       "      <td>0.000453</td>\n",
       "      <td>Categories Similarity - Naive Bayes</td>\n",
       "      <td>0.219315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>46.058670</td>\n",
       "      <td>2.215212</td>\n",
       "      <td>18.078570</td>\n",
       "      <td>0.056807</td>\n",
       "      <td>{}</td>\n",
       "      <td>0.223004</td>\n",
       "      <td>0.225169</td>\n",
       "      <td>0.217339</td>\n",
       "      <td>0.221837</td>\n",
       "      <td>0.003301</td>\n",
       "      <td>1</td>\n",
       "      <td>0.224831</td>\n",
       "      <td>0.222482</td>\n",
       "      <td>0.222858</td>\n",
       "      <td>0.223390</td>\n",
       "      <td>0.001030</td>\n",
       "      <td>All Features - Naive Bayes</td>\n",
       "      <td>0.219720</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time params  \\\n",
       "0      24.193751      1.243320         9.702975        0.216793     {}   \n",
       "0       8.761650      0.157636         4.028346        0.026570     {}   \n",
       "0       5.382411      0.169216         2.269197        0.052283     {}   \n",
       "0       4.361952      0.024142         2.160518        0.002270     {}   \n",
       "0      46.058670      2.215212        18.078570        0.056807     {}   \n",
       "\n",
       "   split0_test_score  split1_test_score  split2_test_score  mean_test_score  \\\n",
       "0           0.216412           0.216126           0.212304         0.214947   \n",
       "0           0.227605           0.232018           0.221844         0.227156   \n",
       "0           0.374412           0.368413           0.370370         0.371065   \n",
       "0           0.218708           0.223602           0.218829         0.220380   \n",
       "0           0.223004           0.225169           0.217339         0.221837   \n",
       "\n",
       "   std_test_score  rank_test_score  split0_train_score  split1_train_score  \\\n",
       "0        0.001873                1            0.218802            0.211861   \n",
       "0        0.004165                1            0.229607            0.229266   \n",
       "0        0.002498                1            0.371911            0.373925   \n",
       "0        0.002279                1            0.221607            0.220599   \n",
       "0        0.003301                1            0.224831            0.222482   \n",
       "\n",
       "   split2_train_score  mean_train_score  std_train_score  \\\n",
       "0            0.217092          0.215918         0.002952   \n",
       "0            0.227849          0.228907         0.000761   \n",
       "0            0.372439          0.372759         0.000852   \n",
       "0            0.221507          0.221238         0.000453   \n",
       "0            0.222858          0.223390         0.001030   \n",
       "\n",
       "                                 grid_id  best_model_test_score  \n",
       "0  Local Word2Vec - TF-IDF - Naive Bayes               0.209587  \n",
       "0      Glove 300d - TF-IDF - Naive Bayes               0.225530  \n",
       "0                  Doc2Vec - Naive Bayes               0.368967  \n",
       "0    Categories Similarity - Naive Bayes               0.219315  \n",
       "0             All Features - Naive Bayes               0.219720  "
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lg_f1micro_score = overall_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_lg_f1micro_score.to_csv('naive_f1micro_score.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "        'multi_clf__estimator__n_estimators': [20,50],\n",
    "        'multi_clf__estimator__max_depth': [50, 100]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(cv, 'classifier.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
