{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML Pipeline Preparation\n",
    "Follow the instructions below to help you create your ML pipeline.\n",
    "### 1. Import libraries and load data from database.\n",
    "- Import Python libraries\n",
    "- Load dataset from database with [`read_sql_table`](https://pandas.pydata.org/pandas-docs/stable/generated/pandas.read_sql_table.html)\n",
    "- Define feature and target variables X and Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/tarciso/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "\n",
    "#Basic DS libs\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#Helper Libs\n",
    "import re\n",
    "from sqlalchemy import create_engine\n",
    "import joblib\n",
    "\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "#Pipeline\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Vectorizers/Transformers\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
    "\n",
    "#Models\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#Evaluation Metrics\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import hamming_loss, zero_one_loss, jaccard_score, accuracy_score, precision_score, recall_score\n",
    "\n",
    "#Gensim\n",
    "from gensim.test.utils import datapath\n",
    "from gensim import utils\n",
    "import gensim.models\n",
    "\n",
    "#Glove\n",
    "from gensim.test.utils import datapath, get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "\n",
    "#Doc2Vec\n",
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "from sklearn.base import BaseEstimator\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data from database\n",
    "engine = create_engine('sqlite:///../data/DisasterResponse.db')\n",
    "messages_df = pd.read_sql_table(con=engine, table_name='Message')\n",
    "categories_df = pd.read_sql_table(con=engine, table_name='MessageCategoryWide')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create categories_list data with a list of the categories for each message\n",
    "categories = pd.read_csv('../data/disaster_categories.csv')\n",
    "categories['categories_list'] = categories['categories'].apply(lambda x: re.sub(r\"[_a-z]+-0[;]?\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\"-1\", \"\", x)) \\\n",
    "                                    .apply(lambda x: re.sub(r\";$\", \"\", x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>categories</th>\n",
       "      <th>categories_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;other_aid;weather_related;...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>related-1;request-1;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;request;aid_related;medical_products;o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26243</th>\n",
       "      <td>30261</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26244</th>\n",
       "      <td>30262</td>\n",
       "      <td>related-0;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26245</th>\n",
       "      <td>30263</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26246</th>\n",
       "      <td>30264</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-1;medi...</td>\n",
       "      <td>related;aid_related;military</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26247</th>\n",
       "      <td>30265</td>\n",
       "      <td>related-1;request-0;offer-0;aid_related-0;medi...</td>\n",
       "      <td>related</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26248 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                         categories  \\\n",
       "0          2  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "1          7  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "2          8  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "3          9  related-1;request-1;offer-0;aid_related-1;medi...   \n",
       "4         12  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "...      ...                                                ...   \n",
       "26243  30261  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26244  30262  related-0;request-0;offer-0;aid_related-0;medi...   \n",
       "26245  30263  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "26246  30264  related-1;request-0;offer-0;aid_related-1;medi...   \n",
       "26247  30265  related-1;request-0;offer-0;aid_related-0;medi...   \n",
       "\n",
       "                                         categories_list  \n",
       "0                                                related  \n",
       "1      related;aid_related;other_aid;weather_related;...  \n",
       "2                                                related  \n",
       "3      related;request;aid_related;medical_products;o...  \n",
       "4                                                related  \n",
       "...                                                  ...  \n",
       "26243                                                     \n",
       "26244                                                     \n",
       "26245                                            related  \n",
       "26246                       related;aid_related;military  \n",
       "26247                                            related  \n",
       "\n",
       "[26248 rows x 3 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2 - Design and Apply Tokenization Strategies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 - Write down tokenization functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_list(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A list of tokens extracted from the input text\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "\n",
    "    return clean_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_to_str(text, lemmatizer):\n",
    "    '''\n",
    "    INPUT\n",
    "    text - text string to be tokenized\n",
    "    lemmatizer - lemmatizer object to be used to process text tokens\n",
    "    \n",
    "    OUTPUT\n",
    "    A string with the tokens extracted from the input text concatenated by spaces\n",
    "    \n",
    "    This function receives raw text as input a pre-processes it for NLP analysis, removing punctuation and\n",
    "    special characters, normalizing case and removing extra spaces, as well as removing stop words and \n",
    "    applying lemmatization\n",
    "    '''\n",
    "    tokens = nltk.tokenize.word_tokenize(re.sub(r\"[^a-zA-Z0-9]\", \" \", text.lower().strip()))\n",
    "    clean_tokens = [lemmatizer.lemmatize(tok) for tok in tokens if tok not in stopwords.words(\"english\")]\n",
    "    #Return tokens list as a string joined by whitespaces\n",
    "    clean_tokens_str = ' '.join(clean_tokens)\n",
    "\n",
    "    return clean_tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['weather', 'update', 'cold', 'front', 'cuba', 'could', 'pas', 'haiti']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "tokenize_to_list(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'weather update cold front cuba could pas haiti'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_to_str(messages_df.message[0],lemmatizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 - Generate tokenized messages table for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# lemmatizer = WordNetLemmatizer()\n",
    "# messages_df['tokens_list'] = messages_df.message.apply(lambda x: tokenize_to_list(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# messages_df['tokens_str'] = messages_df.message.apply(lambda x: tokenize_to_str(x, lemmatizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>message</th>\n",
       "      <th>original</th>\n",
       "      <th>genre</th>\n",
       "      <th>num_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>Weather update - a cold front from Cuba that c...</td>\n",
       "      <td>Un front froid se retrouve sur Cuba ce matin. ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>Is the Hurricane over or is it not over</td>\n",
       "      <td>Cyclone nan fini osinon li pa fini</td>\n",
       "      <td>direct</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>Looking for someone but no name</td>\n",
       "      <td>Patnm, di Maryani relem pou li banm nouvel li ...</td>\n",
       "      <td>direct</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>UN reports Leogane 80-90 destroyed. Only Hospi...</td>\n",
       "      <td>direct</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>says: west side of Haiti, rest of the country ...</td>\n",
       "      <td>facade ouest d Haiti et le reste du pays aujou...</td>\n",
       "      <td>direct</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>The training demonstrated how to enhance micro...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>A suitable candidate has been selected and OCH...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>Proshika, operating in Cox's Bazar municipalit...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>Some 2,000 women protesting against the conduc...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>A radical shift in thinking came about as a re...</td>\n",
       "      <td>None</td>\n",
       "      <td>news</td>\n",
       "      <td>36</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                            message  \\\n",
       "0               2  Weather update - a cold front from Cuba that c...   \n",
       "1               7            Is the Hurricane over or is it not over   \n",
       "2               8                    Looking for someone but no name   \n",
       "3               9  UN reports Leogane 80-90 destroyed. Only Hospi...   \n",
       "4              12  says: west side of Haiti, rest of the country ...   \n",
       "...           ...                                                ...   \n",
       "26139       30261  The training demonstrated how to enhance micro...   \n",
       "26140       30262  A suitable candidate has been selected and OCH...   \n",
       "26141       30263  Proshika, operating in Cox's Bazar municipalit...   \n",
       "26142       30264  Some 2,000 women protesting against the conduc...   \n",
       "26143       30265  A radical shift in thinking came about as a re...   \n",
       "\n",
       "                                                original   genre  num_words  \n",
       "0      Un front froid se retrouve sur Cuba ce matin. ...  direct         13  \n",
       "1                     Cyclone nan fini osinon li pa fini  direct          9  \n",
       "2      Patnm, di Maryani relem pou li banm nouvel li ...  direct          6  \n",
       "3      UN reports Leogane 80-90 destroyed. Only Hospi...  direct         13  \n",
       "4      facade ouest d Haiti et le reste du pays aujou...  direct         12  \n",
       "...                                                  ...     ...        ...  \n",
       "26139                                               None    news         21  \n",
       "26140                                               None    news         22  \n",
       "26141                                               None    news         23  \n",
       "26142                                               None    news         31  \n",
       "26143                                               None    news         36  \n",
       "\n",
       "[26144 rows x 5 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_tokens = messages_df[['message_id','tokens_str']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_tokens.to_sql('MessageTokens', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories = messages_tokens.merge(categories_df, on='message_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#messages_categories.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 - Find n-grams and save to DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_ngrams_freqs(messages_array, n=1):\n",
    "#     vec = CountVectorizer(ngram_range=(n, n)).fit(messages_array)\n",
    "#     bag_of_words = vec.transform(messages_array)\n",
    "#     word_count = bag_of_words.sum(axis=0)\n",
    "#     words_freq = [(word, n, word_count[0, idx]) for word, idx in vec.vocabulary_.items()]\n",
    "#     words_freq = sorted(words_freq, key = lambda x: x[2], reverse=True)\n",
    "#     words_freq_df = pd.DataFrame(data = words_freq, columns = ['ngram','n','count'])\n",
    "#     return words_freq_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str)\n",
    "# bigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=2)\n",
    "# trigrams_freqs = get_ngrams_freqs(messages_tokens.tokens_str, n=3)\n",
    "# ngrams_freqs = pd.concat([unigrams_freqs, bigrams_freqs, trigrams_freqs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ngrams_freqs.to_sql('NGramsFreqs', engine, index=False, if_exists='replace')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3 - Prepare Data for Train/Test Pipeline\n",
    "- Extract X and Y from datasets\n",
    "- Split data into train and test sets\n",
    "- Train pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_tokens = pd.read_sql_table(con=engine, table_name='MessageTokens')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>message_id</th>\n",
       "      <th>tokens_str</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>weather update cold front cuba could pas haiti</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>hurricane</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>looking someone name</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>un report leogane 80 90 destroyed hospital st ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>say west side haiti rest country today tonight</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26139</th>\n",
       "      <td>30261</td>\n",
       "      <td>training demonstrated enhance micronutrient pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26140</th>\n",
       "      <td>30262</td>\n",
       "      <td>suitable candidate selected ocha jakarta curre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26141</th>\n",
       "      <td>30263</td>\n",
       "      <td>proshika operating cox bazar municipality 5 un...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26142</th>\n",
       "      <td>30264</td>\n",
       "      <td>2 000 woman protesting conduct election tearga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26143</th>\n",
       "      <td>30265</td>\n",
       "      <td>radical shift thinking came result meeting rec...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26144 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       message_id                                         tokens_str\n",
       "0               2     weather update cold front cuba could pas haiti\n",
       "1               7                                          hurricane\n",
       "2               8                               looking someone name\n",
       "3               9  un report leogane 80 90 destroyed hospital st ...\n",
       "4              12     say west side haiti rest country today tonight\n",
       "...           ...                                                ...\n",
       "26139       30261  training demonstrated enhance micronutrient pr...\n",
       "26140       30262  suitable candidate selected ocha jakarta curre...\n",
       "26141       30263  proshika operating cox bazar municipality 5 un...\n",
       "26142       30264  2 000 woman protesting conduct election tearga...\n",
       "26143       30265  radical shift thinking came result meeting rec...\n",
       "\n",
       "[26144 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare data for ML Pipeline\n",
    "X = messages_df.message.values\n",
    "X_tokenized = messages_tokens.tokens_str.values\n",
    "Y_df = categories_df.drop('message_id', axis=1)\n",
    "Y = Y_df.values\n",
    "category_columns = Y_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Weather update - a cold front from Cuba that could pass over Haiti',\n",
       "       'Is the Hurricane over or is it not over',\n",
       "       'Looking for someone but no name', ...,\n",
       "       \"Proshika, operating in Cox's Bazar municipality and 5 other unions, Ramu and Chokoria, assessment, 5 kg rice, 1,5 kg lentils to 700 families.\",\n",
       "       'Some 2,000 women protesting against the conduct of the elections were teargassed as they tried to converge on the local electoral commission offices in the southern oil city of Port Harcourt.',\n",
       "       'A radical shift in thinking came about as a result of this meeting, recognizing that HIV/AIDS is at the core of the humanitarian crisis and identifying the crisis itself as a function of the HIV/AIDS pandemic.'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['weather update cold front cuba could pas haiti', 'hurricane',\n",
       "       'looking someone name', ...,\n",
       "       'proshika operating cox bazar municipality 5 union ramu chokoria assessment 5 kg rice 1 5 kg lentil 700 family',\n",
       "       '2 000 woman protesting conduct election teargassed tried converge local electoral commission office southern oil city port harcourt',\n",
       "       'radical shift thinking came result meeting recognizing hiv aid core humanitarian crisis identifying crisis function hiv aid pandemic'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.25, random_state=199)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['crudess you are the most charm and charismatic DT member .. thanks for all your videos in Santiago.. and to talk with me twice',\n",
       "       'Tidal waves triggered by an earthquake off Indonesia on Sunday swept over a vast swath of coastlines, including those in India, Indonesia, Malaysia, the Maldives, Somalia, Sri Lanka and Thailand.',\n",
       "       'The earthquake happened at 4h34 in the evening and for the night. ',\n",
       "       ...,\n",
       "       'Earthquake of the day #Haiti or Google possibly leaving China ?',\n",
       "       'When the matter had been debated previously, the Non-Aligned Movement foreign ministers firmly stated that there was no right to humanitarian intervention.',\n",
       "       '@DumboNYC : Oh no RT @endtwist : ConEd power station in #DUMBO is flooding ! #nopower #sandy'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(19608,) (19608, 36)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4 - Design Transformer to Tokenize sentences before passing on to other transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tokenizer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, output_str=True):\n",
    "        self.output_str = output_str\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([tokenize_to_str(tokens_str, lemmatizer) if self.output_str else \n",
    "                         tokenize_to_list(tokens_str, lemmatizer) for tokens_str in X])\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# msgTokenizer = Tokenizer().fit(X, None)\n",
    "# msgTokenizer.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 - Create a Mean/TF-IDF word vector from a locally trained Word2Vec model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 - Train a Word2Vec model using messages text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Message(object):\n",
    "#     \"\"\"An interator that yields messages tokens (lists of str).\"\"\"\n",
    "    \n",
    "#     def __init__(self, messages_df, sample_size=-1):\n",
    "#         self.messages_df = messages_df\n",
    "#         self.sample_size = sample_size\n",
    "\n",
    "#     def __iter__(self):\n",
    "#         messages = messages_df\n",
    "        \n",
    "#         #Sample dataset if specified\n",
    "#         if self.sample_size > 0:\n",
    "#             messages = messages.sample(self.sample_size)\n",
    "        \n",
    "#         #Yeld tokenized message joined by whitespaces\n",
    "#         for token_str in messages.tokens_str:\n",
    "#             yield token_str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# messages_df.tokens_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# sentences = Message(messages_df=messages_df)\n",
    "# local_w2v_model_full = gensim.models.Word2Vec(sentences=sentences)\n",
    "# local_w2v_model_full.save(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_model_full = gensim.models.Word2Vec.load(\"messages-word2vec-full.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look up generated word vocab\n",
    "# for i, word in enumerate(w2v_model_full.wv.vocab):\n",
    "#     if i == 10:\n",
    "#         break\n",
    "#     print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # look up generated word vectors\n",
    "# w2v_model_full.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 - Implement Vector Aggregator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X.shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial implementation of aggregators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "class MeanEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Testing defaultdict functionality\n",
    "# test_dict = defaultdict(lambda: 1, [(\"Hello\" , 7), (\"hi\" , 10), (\"there\" , 45),(\"at\" , 23),(\"this\" , 77)])\n",
    "# test_dict['Hi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingVectorizer(object):\n",
    "    def __init__(self, word2vec_model):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "        self.word_weights = None\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "\n",
    "# w2v_mean_pip = Pipeline([\n",
    "#     ('features',FeatureUnion([\n",
    "#         ('w2v_mean', MeanEmbeddingVectorizer(w2v_model_full)),\n",
    "#         ('w2v_tfidf', TfidfEmbeddingVectorizer(w2v_model_full))\n",
    "#     ])),    \n",
    "#     ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "# ])\n",
    "\n",
    "# w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "# for category_idx in range(y_pred.shape[1]):\n",
    "#     print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# category_idx=1\n",
    "# c_report = classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], \n",
    "#                       labels=[0,1], \n",
    "#                       target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1'],\n",
    "#                       output_dict=True)\n",
    "# c_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(c_report.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# w2v_mean_pip.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sum(y_pred[0] == y_test[0])/y_pred[0].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_test = y_test[0]\n",
    "sample_pred = y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_test == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sample_pred == 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(sample_test == 1) == (sample_pred == 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing TPR manually\n",
    "tp_array = (sample_test == 1) & (sample_pred == 1)\n",
    "tp = tp_array.sum()\n",
    "tpr = tp/sample_test.shape[0]\n",
    "print(tp)\n",
    "print(tpr)\n",
    "tp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3/36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing FPR manually\n",
    "fp_array = (sample_test == 0) & (sample_pred == 1)\n",
    "fp = fp_array.sum()\n",
    "fpr = fp/sample_test.shape[0]\n",
    "print(fp)\n",
    "print(fpr)\n",
    "fp_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing FNR manually\n",
    "fn_array = (sample_test == 1) & (sample_pred == 0)\n",
    "fn = fn_array.sum()\n",
    "fnr = fn/sample_test.shape[0]\n",
    "print(fn)\n",
    "print(fnr)\n",
    "fn_array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing Precision manually\n",
    "print(tp/(tp+fp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Computing Recall manually\n",
    "print(tp/(tp+fn))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example_test = np.append(np.repeat(1, 4),np.repeat(0,32))\n",
    "example_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "example_pred = np.repeat(0, 36)\n",
    "example_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(example_test,example_pred))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(example_test,example_pred))\n",
    "print(\"Jaccard Score = \", jaccard_score(example_test,example_pred))\n",
    "print(\"Accuracy = \", accuracy_score(example_test,example_pred))\n",
    "print(\"Recall = \", recall_score(example_test,example_pred))\n",
    "print(\"Precision = \", precision_score(example_test,example_pred))\n",
    "print(\"F1-Score = \", f1_score(example_test,example_pred))\n",
    "print(\"Jaccard Score Micro = \", jaccard_score(example_test,example_pred, average='micro'))\n",
    "print(\"Recall Micro = \", recall_score(example_test,example_pred, average='micro'))\n",
    "print(\"Precision Micro = \", precision_score(example_test,example_pred, average='micro'))\n",
    "print(\"F1-Score Micro = \", f1_score(example_test,example_pred, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import mean_absolute_error, r2_score, explained_variance_score\n",
    "print(mean_absolute_error(y_test, y_pred, multioutput='raw_values'))\n",
    "print(mean_absolute_error(y_test, y_pred))\n",
    "(y_test == y_pred).mean()\n",
    "print(r2_score(y_test, y_pred))\n",
    "print(r2_score(y_test, y_pred, multioutput='variance_weighted'))\n",
    "print(explained_variance_score(y_test, y_pred))\n",
    "print(explained_variance_score(y_test, y_pred, multioutput='variance_weighted'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 - Create Estimator to both train and average w2v on corpus data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trying W2VTransformer - Gensim's sklearn Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from gensim.sklearn_api import W2VTransformer\n",
    "from gensim.test.utils import common_texts\n",
    "\n",
    "model = W2VTransformer(size=10, min_count=1, seed=1)\n",
    "\n",
    "model.fit(common_texts)\n",
    "          \n",
    "model.transform(['graph', 'system'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "common_texts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages_df.head(5).tokens_str.str.split().values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "messages_df.head(5).tokens_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w2v_model = W2VTransformer()\n",
    "w2v_model.fit(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "word = messages_df.tokens_str[0].split()[1]\n",
    "print(word)\n",
    "w2v_model.transform(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model = gensim.models.Word2Vec(messages_df.tokens_str.str.split().values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for i, word in enumerate(test_model.wv.vocab):\n",
    "    if i == 10:\n",
    "        break\n",
    "    print(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_model.wv['earthquake']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Re-implement aggregators now with training capability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MeanEmbeddingTrainVectorizer(object):\n",
    "\n",
    "    def __init__(self, word2vec_model=None):\n",
    "        if word2vec_model is None:\n",
    "            self.word2vec_model = None\n",
    "            self.num_dims = -1\n",
    "        else:\n",
    "            self.word2vec_model = word2vec_model\n",
    "            self.num_dims = word2vec_model.vector_size\n",
    "            \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X)\n",
    "            self.num_dims = self.word2vec_model.vector_size\n",
    "            \n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TfidfEmbeddingTrainVectorizer(object):\n",
    "    \n",
    "    def __init__(self, word2vec_model=None):\n",
    "        if word2vec_model is None:\n",
    "            self.word2vec_model = None\n",
    "            self.num_dims = -1\n",
    "        else:\n",
    "            self.word2vec_model = word2vec_model\n",
    "            self.num_dims = word2vec_model.vector_size\n",
    "        self.word_weights = None\n",
    "        \n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if self.word2vec_model is None:\n",
    "            self.word2vec_model = gensim.models.Word2Vec(X)\n",
    "            self.num_dims = self.word2vec_model.vector_size\n",
    "            \n",
    "        tfidf = TfidfVectorizer(analyzer=lambda x: x)\n",
    "        tfidf.fit(X)\n",
    "        max_idf = max(tfidf.idf_)\n",
    "        \n",
    "        tfidf_weights = [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()]\n",
    "        self.word_weights = defaultdict(lambda: max_idf, tfidf_weights)\n",
    "    \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        mean_embeddings = np.empty([X.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(X.shape[0]):\n",
    "            doc_tokens = X[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w]*self.word_weights[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test aggregators using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('features',FeatureUnion([\n",
    "        ('w2v_mean', MeanEmbeddingTrainVectorizer(w2v_model_full)),\n",
    "        ('w2v_mean_train', MeanEmbeddingTrainVectorizer()),\n",
    "        ('w2v_tfidf', TfidfEmbeddingTrainVectorizer(w2v_model_full)),\n",
    "        ('w2v_tfidf_train', TfidfEmbeddingTrainVectorizer()),\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Use Glove pre-trained model to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Glove pre-trained vectors can be downloaded here: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "# Code which can be used to convert from Glove default format to Gensim W2V format\n",
    "\n",
    "glove_file = 'glove-pretrained/glove.6B.50d.txt'\n",
    "tmp_file = 'glove-pretrained/glove.6B.50d_word2vec.txt'\n",
    "\n",
    "#_ = glove2word2vec(glove_file, tmp_file)\n",
    "\n",
    "glove_model = KeyedVectors.load_word2vec_format(tmp_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2min 32s, sys: 1.39 s, total: 2min 33s\n",
      "Wall time: 2min 33s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "glove_300d_w2v = KeyedVectors.load_word2vec_format('glove-pretrained/glove.6B.300d_word2vec.txt', binary=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Use a Pipeline to train the Word2Vec estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Aggregators with Glove using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('features',FeatureUnion([\n",
    "        ('glove_mean', MeanEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ])),    \n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from sklearn.metrics import precision_recall_curve\n",
    "#from sklearn.metrics import average_precision_score\n",
    "\n",
    "# For each class\n",
    "# precision = dict()\n",
    "# recall = dict()\n",
    "# average_precision = dict()\n",
    "# for i in range(len(category_columns)):\n",
    "#     precision[i], recall[i], _ = precision_recall_curve(y_test[:, i], y_pred[:, i])\n",
    "    #average_precision[i] = average_precision_score(y_test[:, i], y_pred[:, i])\n",
    "\n",
    "# A \"micro-average\": quantifying score on all classes jointly\n",
    "# precision[\"micro\"], recall[\"micro\"], _ = precision_recall_curve(Y_test.ravel(),\n",
    "#     y_score.ravel())\n",
    "# average_precision[\"micro\"] = average_precision_score(Y_test, y_score,\n",
    "#                                                      average=\"micro\")\n",
    "# print('Average precision score, micro-averaged over all classes: {0:0.2f}'\n",
    "#       .format(average_precision[\"micro\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics import hamming_loss, zero_one_loss, jaccard_score, accuracy_score, auc, recall_score, precision_score\n",
    "\n",
    "print(\"Hamming Loss = \", hamming_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Zero-One Loss = \", zero_one_loss(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Jaccard Score = \", jaccard_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Accuracy = \", accuracy_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1)))\n",
    "print(\"Recall = \", recall_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))\n",
    "print(\"Precision = \", precision_score(np.argmax(y_test, axis=1),np.argmax(y_pred, axis=1), average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0] == y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6536*36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(np.argmax(y_pred, axis=1).shape)\n",
    "print(y_pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "train_foo = [['sci-fi', 'thriller'],['comedy'],['sci-fi', 'thriller'],['comedy']]\n",
    "mlb = MultiLabelBinarizer()\n",
    "mlb_label_train = mlb.fit_transform(train_foo)\n",
    "mlb_label_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Y_df[Y_df['related'] == 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Use Doc2Vec to generate feature vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking the number of words in the corpus\n",
    "messages_df['message'].apply(lambda x: len(x.split(' '))).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try Doc2Vec step by step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "def read_messages(messages, tokens_only=False):\n",
    "    for index, tokens_str in np.ndenumerate(messages):\n",
    "        tokens = tokens_str.split()\n",
    "        if tokens_only:\n",
    "            yield tokens\n",
    "        else:\n",
    "            # For training data, add tags\n",
    "            tags = index\n",
    "            yield gensim.models.doc2vec.TaggedDocument(tokens, tags)\n",
    "\n",
    "train_corpus = list(read_messages(X_train))\n",
    "test_corpus = list(read_messages(X_test, tokens_only=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_corpus[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2v_model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=5, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d2v_model.build_vocab(train_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time \n",
    "\n",
    "d2v_model.train(train_corpus, total_examples=d2v_model.corpus_count, epochs=d2v_model.epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a Doc2Vec Estimator/Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Doc2VecTransformer(BaseEstimator):\n",
    "\n",
    "    def __init__(self, vector_size=100, epochs=20):\n",
    "        self.epochs = epochs\n",
    "        self.vector_size = vector_size\n",
    "        self.workers = multiprocessing.cpu_count() - 1\n",
    "        self.d2v_model = None\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        tagged_x = [TaggedDocument(tokens_str.split(), [index]) for index, tokens_str in np.ndenumerate(X)]\n",
    "        self.d2v_model = Doc2Vec(vector_size=self.vector_size, workers=self.workers, epochs=self.epochs)\n",
    "        self.d2v_model.build_vocab(tagged_x)\n",
    "        self.d2v_model.train(tagged_x, total_examples=self.d2v_model.corpus_count, \n",
    "                             epochs=self.epochs)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.asmatrix(np.array([self.d2v_model.infer_vector(tokens_str.split())\n",
    "                                     for tokens_str in X]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "doc2vec_trf = Doc2VecTransformer(epochs=1)\n",
    "doc2vec_trf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test[0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "inferred_vector = doc2vec_trf.d2v_model.infer_vector(X_test[0].split())\n",
    "print(inferred_vector.shape)\n",
    "print(type(inferred_vector))\n",
    "print(inferred_vector)\n",
    "\n",
    "inferred_vector2 = np.asmatrix(np.array([doc2vec_trf.d2v_model.infer_vector(X_test[0].split())]))\n",
    "print(inferred_vector2.shape)\n",
    "print(type(inferred_vector2))\n",
    "print(inferred_vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "doc2vec_features = doc2vec_trf.transform(X_test)\n",
    "print(doc2vec_features.shape)\n",
    "doc2vec_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test Doc2Vec using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "w2v_mean_pip = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "w2v_mean_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = w2v_mean_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"y_pred sum = \",y_pred.sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Extra Feature - Distance Between Message Word Vector to Categories Word Vectors "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use Glove as our space vectorizer as it is trained on a broad corpus and thus able to better describe texts / words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's build the embeddings for the categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['related', 'request', 'offer', 'aid_related', 'medical_help',\n",
       "       'medical_products', 'search_and_rescue', 'security', 'military',\n",
       "       'child_alone', 'water', 'food', 'shelter', 'clothing', 'money',\n",
       "       'missing_people', 'refugees', 'death', 'other_aid',\n",
       "       'infrastructure_related', 'transport', 'buildings', 'electricity',\n",
       "       'tools', 'hospitals', 'shops', 'aid_centers', 'other_infrastructure',\n",
       "       'weather_related', 'floods', 'storm', 'fire', 'earthquake', 'cold',\n",
       "       'other_weather', 'direct_report'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "category_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories_tokens = np.array([np.array(cat.split('_')) for cat in category_columns])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([array(['related'], dtype='<U7'), array(['request'], dtype='<U7'),\n",
       "       array(['offer'], dtype='<U5'),\n",
       "       array(['aid', 'related'], dtype='<U7'),\n",
       "       array(['medical', 'help'], dtype='<U7'),\n",
       "       array(['medical', 'products'], dtype='<U8'),\n",
       "       array(['search', 'and', 'rescue'], dtype='<U6'),\n",
       "       array(['security'], dtype='<U8'), array(['military'], dtype='<U8'),\n",
       "       array(['child', 'alone'], dtype='<U5'),\n",
       "       array(['water'], dtype='<U5'), array(['food'], dtype='<U4'),\n",
       "       array(['shelter'], dtype='<U7'), array(['clothing'], dtype='<U8'),\n",
       "       array(['money'], dtype='<U5'),\n",
       "       array(['missing', 'people'], dtype='<U7'),\n",
       "       array(['refugees'], dtype='<U8'), array(['death'], dtype='<U5'),\n",
       "       array(['other', 'aid'], dtype='<U5'),\n",
       "       array(['infrastructure', 'related'], dtype='<U14'),\n",
       "       array(['transport'], dtype='<U9'),\n",
       "       array(['buildings'], dtype='<U9'),\n",
       "       array(['electricity'], dtype='<U11'),\n",
       "       array(['tools'], dtype='<U5'), array(['hospitals'], dtype='<U9'),\n",
       "       array(['shops'], dtype='<U5'),\n",
       "       array(['aid', 'centers'], dtype='<U7'),\n",
       "       array(['other', 'infrastructure'], dtype='<U14'),\n",
       "       array(['weather', 'related'], dtype='<U7'),\n",
       "       array(['floods'], dtype='<U6'), array(['storm'], dtype='<U5'),\n",
       "       array(['fire'], dtype='<U4'), array(['earthquake'], dtype='<U10'),\n",
       "       array(['cold'], dtype='<U4'),\n",
       "       array(['other', 'weather'], dtype='<U7'),\n",
       "       array(['direct', 'report'], dtype='<U6')], dtype=object)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_300d_w2v_mean = MeanEmbeddingTrainVectorizer(glove_300d_w2v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "glove_300d_w2v_mean.fit(categories_tokens, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cat_tokens_embeddings = glove_300d_w2v_mean.transform(categories_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(cat_tokens_embeddings.shape)\n",
    "print(cat_tokens_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "cat_tokens_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_msgs_embeddings = glove_300d_w2v_mean.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(test_msgs_embeddings.shape)\n",
    "print(test_msgs_embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "test_msgs_embeddings[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "print(test_msgs_embeddings[:10].shape)\n",
    "print(cat_tokens_embeddings.shape)\n",
    "print(cosine_similarity(test_msgs_embeddings[:10],cat_tokens_embeddings).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a CategoriesSimilarity Transformer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "class CategoriesSimilarity(object):\n",
    "    \n",
    "    def __init__(self, word2vec_model, categories_tokens):\n",
    "        self.word2vec_model = word2vec_model\n",
    "        self.categories_tokens = categories_tokens\n",
    "        self.categories_vectors = None\n",
    "        self.num_dims = word2vec_model.vector_size\n",
    "        \n",
    "    def compute_mean_embeddings(self, tokens_array):    \n",
    "        mean_embeddings = np.empty([tokens_array.shape[0],self.num_dims])\n",
    "        \n",
    "        for i in range(tokens_array.shape[0]):\n",
    "            doc_tokens = tokens_array[i]\n",
    "            \n",
    "            words_vectors_concat = [self.word2vec_model[w] for w in doc_tokens if w in self.word2vec_model]\n",
    "\n",
    "            if (len(words_vectors_concat) == 0):\n",
    "                words_vectors_concat = [np.zeros(self.num_dims)]\n",
    "                \n",
    "            #print(np.mean(words_vectors_concat, axis=0))\n",
    "                \n",
    "            mean_embeddings[i] = np.mean(words_vectors_concat, axis=0)\n",
    "            \n",
    "        return mean_embeddings\n",
    "                    \n",
    "    def fit(self, X, y):\n",
    "        self.categories_vectors = self.compute_mean_embeddings(self.categories_tokens)\n",
    "        return self \n",
    "\n",
    "    def transform(self, X):\n",
    "        mean_embeddings = self.compute_mean_embeddings(X)\n",
    "        cats_similarities = cosine_similarity(mean_embeddings, self.categories_vectors)\n",
    "            \n",
    "        return cats_similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test the transformer manually using test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cats_sim_transf = CategoriesSimilarity(glove_300d_w2v, categories_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cats_sim_transf.fit(X_test, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cats_sim_transf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test CategoriesSimilarity feature using a Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "cats_sim_pip = Pipeline([\n",
    "    ('cats_sim', CategoriesSimilarity(glove_300d_w2v, cat_tokens_embeddings)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "cats_sim_pip.fit(X_train, y_train)\n",
    "\n",
    "y_pred = cats_sim_pip.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))\n",
    "    \n",
    "print(\"Score = \", cats_sim_pip.score(X_test, y_test))\n",
    "\n",
    "# accuracy\n",
    "print(\"Accuracy = \",accuracy_score(y_test,y_pred))\n",
    "print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. NLP Feature Selection using Sklearn Pipelines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8.1 - All features together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%time\n",
    "\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "\n",
    "all_feats = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "all_feats.fit(X_train, y_train)\n",
    "\n",
    "y_pred = all_feats.predict(X_test)\n",
    "\n",
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[category_columns[category_idx] + '-0',category_columns[category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score = all_feats.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build a machine learning pipeline\n",
    "This machine pipeline should take in the `message` column as input and output classification results on the other 36 categories in the dataset. You may find the [MultiOutputClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.multioutput.MultiOutputClassifier.html) helpful for predicting multiple target variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline = Pipeline([\n",
    "    ('vect',CountVectorizer(tokenizer=tokenize_to_list(lemmatizer=lemmatizer), ngram_range=(1,3))),\n",
    "    ('tfidf', TfidfTransformer()),\n",
    "    ('multi_clf', MultiOutputClassifier(RandomForestClassifier(random_state=199), n_jobs=-1))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pipeline.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Build pipelines for different combinations of features and classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from sklearn import svm\n",
    "\n",
    "local_w2v_tfidf_svc = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "local_w2v_tfidf_rf = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_svc = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_svc = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "doc2vec_rf = Pipeline([\n",
    "    ('doc2vec', Doc2VecTransformer()),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_svc = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "])\n",
    "\n",
    "all_feats_rf = Pipeline([\n",
    "    ('features', FeatureUnion([\n",
    "        ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "        ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "        ('doc2vec', Doc2VecTransformer()),\n",
    "        ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "    ])),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "pipelines = [local_w2v_tfidf_svc, local_w2v_tfidf_rf, glove_tfidf_svc, glove_tfidf_rf, \n",
    "             doc2vec_svc, doc2vec_rf, all_feats_svc, all_feats_rf]\n",
    "\n",
    "grid_dict = {0 : 'Local Word2Vec - TF-IDF - Linear SVC',\n",
    "             1 : 'Local Word2Vec - TF-IDF - Random Forest',\n",
    "             2 : 'Glove 300d - TF-IDF - Linear SVC',\n",
    "             3 : 'Glove 300d - TF-IDF - Random Forest',\n",
    "             4 : 'Doc2Vec - Linear SVC',\n",
    "             5 : 'Doc2Vec - Random Forest',\n",
    "             6 : 'All Features - Linear SVC',\n",
    "             7 : 'All Features - Random Forest',}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error\n",
    "\n",
    "pip_local_w2v_tfidf_naive = Pipeline([\n",
    "    ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_naive = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(GaussianNB()))\n",
    "])\n",
    "\n",
    "pip_glove_tfidf_rf = Pipeline([\n",
    "    ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "    ('clf', MultiOutputClassifier(RandomForestClassifier(random_state=199)))\n",
    "])\n",
    "\n",
    "# doc2vec_svc = Pipeline([\n",
    "#     ('doc2vec', Doc2VecTransformer()),\n",
    "#     ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "# ])\n",
    "\n",
    "# all_feats_svc = Pipeline([\n",
    "#     ('features', FeatureUnion([\n",
    "#         ('local_w2v_tfidf', TfidfEmbeddingTrainVectorizer()),\n",
    "#         ('glove_tfidf', TfidfEmbeddingTrainVectorizer(glove_300d_w2v)),\n",
    "#         ('doc2vec', Doc2VecTransformer()),\n",
    "#         ('cats_sim', CategoriesSimilarity(glove_300d_w2v, categories_tokens))\n",
    "#     ])),\n",
    "#     ('clf', MultiOutputClassifier(svm.LinearSVC(random_state=199)))\n",
    "# ])\n",
    "\n",
    "grid_params_empty = [{}]\n",
    "\n",
    "# Construct grid searches\n",
    "jobs = -1\n",
    "\n",
    "score = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "gs_local_w2v_tfidf_naive = GridSearchCV(estimator=pip_local_w2v_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=2) \n",
    "\n",
    "gs_glove_tfidf_rf = GridSearchCV(estimator=pip_glove_tfidf_rf,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=2,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "gs_glove_tfidf_naive = GridSearchCV(estimator=pip_glove_tfidf_naive,\n",
    "            param_grid=grid_params_empty,\n",
    "            scoring=score,\n",
    "            cv=2,\n",
    "            n_jobs=jobs) \n",
    "\n",
    "grids = [gs_glove_tfidf_naive]\n",
    "\n",
    "grid_dict = {\n",
    "    0 : 'Glove 300d - TF-IDF - Naive Bayes',\n",
    "    1 : 'Local Word2Vec - TF-IDF - Naive Bayes'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing model optimizations...\n",
      "\n",
      "Estimator: Glove 300d - TF-IDF - Naive Bayes\n",
      "Best params: {}\n",
      "Best training accuracy: -0.609\n",
      "Test set accuracy score for best params: -0.623 \n",
      "\n",
      "Classifier with best test set accuracy: Glove 300d - TF-IDF - Naive Bayes\n",
      "CPU times: user 1min 26s, sys: 2.27 s, total: 1min 28s\n",
      "Wall time: 2min 55s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "print('Performing model optimizations...')\n",
    "best_score = 0.0\n",
    "best_clf = 0\n",
    "best_gs = ''\n",
    "overall_results_df = pd.DataFrame()\n",
    "for idx, gs in enumerate(grids):\n",
    "    print('\\nEstimator: %s' % grid_dict[idx])\n",
    "    # Fit grid search\n",
    "    gs.fit(X_train, y_train)\n",
    "    # Best params\n",
    "    print('Best params: %s' % gs.best_params_)\n",
    "    # Best training data accuracy\n",
    "    print('Best training accuracy: %.3f' % gs.best_score_)\n",
    "    # Predict on test data with best params\n",
    "    test_score = gs.score(X_test, y_test)\n",
    "    # Test data accuracy of model with best params\n",
    "    print('Test set accuracy score for best params: %.3f ' % test_score)\n",
    "    \n",
    "    # Track Grid Search results\n",
    "    gs_results_df = pd.DataFrame(gs.cv_results_)\n",
    "    gs_results_df['grid_id'] = grid_dict[idx]\n",
    "    gs_results_df['best_model_test_score'] = test_score\n",
    "    \n",
    "    overall_results_df = pd.concat([overall_results_df, gs_results_df])\n",
    "    \n",
    "    # Track best (highest test accuracy) model\n",
    "    if test_score > best_score:\n",
    "        best_score = test_score\n",
    "        best_gs = gs\n",
    "        best_clf = idx\n",
    "print('\\nClassifier with best test set accuracy: %s' % grid_dict[best_clf])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean_fit_time</th>\n",
       "      <th>std_fit_time</th>\n",
       "      <th>mean_score_time</th>\n",
       "      <th>std_score_time</th>\n",
       "      <th>params</th>\n",
       "      <th>split0_test_score</th>\n",
       "      <th>split1_test_score</th>\n",
       "      <th>mean_test_score</th>\n",
       "      <th>std_test_score</th>\n",
       "      <th>rank_test_score</th>\n",
       "      <th>grid_id</th>\n",
       "      <th>best_model_test_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>39.863238</td>\n",
       "      <td>2.186177</td>\n",
       "      <td>47.271844</td>\n",
       "      <td>3.999239</td>\n",
       "      <td>{}</td>\n",
       "      <td>-0.59746</td>\n",
       "      <td>-0.620781</td>\n",
       "      <td>-0.609121</td>\n",
       "      <td>0.01166</td>\n",
       "      <td>1</td>\n",
       "      <td>Glove 300d - TF-IDF - Naive Bayes</td>\n",
       "      <td>-0.623321</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean_fit_time  std_fit_time  mean_score_time  std_score_time params  \\\n",
       "0      39.863238      2.186177        47.271844        3.999239     {}   \n",
       "\n",
       "   split0_test_score  split1_test_score  mean_test_score  std_test_score  \\\n",
       "0           -0.59746          -0.620781        -0.609121         0.01166   \n",
       "\n",
       "   rank_test_score                            grid_id  best_model_test_score  \n",
       "0                1  Glove 300d - TF-IDF - Naive Bayes              -0.623321  "
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overall_results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save best grid search pipeline to file\n",
    "dump_file = 'best_gs_pipeline.pkl'\n",
    "joblib.dump(best_gs, dump_file, compress=1)\n",
    "print('\\nSaved %s grid search pipeline to file: %s' % (grid_dict[best_clf], dump_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Test your model\n",
    "Report the f1 score, precision and recall for each output category of the dataset. You can do this by iterating through the columns and calling sklearn's `classification_report` on each."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "print(y_pred.shape)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_pred[:,0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Improve your model\n",
    "Use grid search to find better parameters. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pipeline.get_params().keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "parameters = {\n",
    "        'multi_clf__estimator__n_estimators': [20,50],\n",
    "        'multi_clf__estimator__max_depth': [50, 100]\n",
    "    }\n",
    "\n",
    "cv = GridSearchCV(pipeline, param_grid=parameters, verbose=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Test your model\n",
    "Show the accuracy, precision, and recall of the tuned model.  \n",
    "\n",
    "Since this project focuses on code quality, process, and  pipelines, there is no minimum performance metric needed to pass. However, make sure to fine tune your models for accuracy, precision and recall to make your project stand out - especially for your portfolio!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train classifier\n",
    "cv.fit(X_train, y_train)\n",
    "\n",
    "# predict on test data\n",
    "y_pred = cv.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cv.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for category_idx in range(y_pred.shape[1]):\n",
    "    print(classification_report(y_pred=y_pred[:,category_idx],y_true=y_test[:,category_idx], labels=[0,1], target_names=[df.columns[4+category_idx] + '-0',df.columns[4+category_idx] + '-1']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Try improving your model further. Here are a few ideas:\n",
    "* try other machine learning algorithms\n",
    "* add other features besides the TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Export your model as a pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Output a pickle file for the model\n",
    "joblib.dump(cv, 'classifier.pkl') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Use this notebook to complete `train.py`\n",
    "Use the template file attached in the Resources folder to write a script that runs the steps above to create a database and export a model based on a new dataset specified by the user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
